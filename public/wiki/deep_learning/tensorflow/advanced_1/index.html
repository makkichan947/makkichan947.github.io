<!doctype html>
<html lang="zh">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <title>TensorFlowé«˜çº§ç‰¹æ€§ // Yaku Makki</title>
    <link rel="shortcut icon" href="./avatar.jpg" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.151.2">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Yaku Makki" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="/css/main.min.422840b4454cb211f5b3dab61f082cdff508e88825173c31806835cb75529734.css" />
    

    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="TensorFlowé«˜çº§ç‰¹æ€§">
  <meta name="twitter:description" content="TensorFlowé«˜çº§ç‰¹æ€§ æœ¬ç« æ·±å…¥ä»‹ç»TensorFlowçš„é«˜çº§ç‰¹æ€§ï¼ŒåŒ…æ‹¬è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€åˆ†å¸ƒå¼è®­ç»ƒã€æ¨¡å‹ä¼˜åŒ–ã€æ€§èƒ½è°ƒä¼˜ç­‰å†…å®¹ï¼Œå¸®åŠ©ä½ æ„å»ºæ›´å¤æ‚å’Œé«˜æ•ˆçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚
ğŸ¯ è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ åŸºç¡€è‡ªå®šä¹‰è®­ç»ƒ import tensorflow as tf import numpy as np # å‡†å¤‡æ•°æ® (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() x_train = x_train.reshape(-1, 784) / 255.0 x_test = x_test.reshape(-1, 784) / 255.0 y_train = tf.keras.utils.to_categorical(y_train, 10) y_test = tf.keras.utils.to_categorical(y_test, 10) # åˆ›å»ºæ•°æ®é›† train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) train_dataset = train_dataset.shuffle(1000).batch(32) # æ„å»ºæ¨¡å‹ class CustomModel(tf.keras.Model): def __init__(self): super(CustomModel, self).__init__() self.dense1 = tf.keras.layers.Dense(128, activation=&#39;relu&#39;) self.dense2 = tf.keras.layers.Dense(64, activation=&#39;relu&#39;) self.dense3 = tf.keras.layers.Dense(10, activation=&#39;softmax&#39;) def call(self, inputs): x = self.dense1(inputs) x = self.dense2(x) return self.dense3(x) model = CustomModel() # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ loss_fn = tf.keras.losses.CategoricalCrossentropy() optimizer = tf.keras.optimizers.Adam() # è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ @tf.function # ç¼–è¯‘åŠ é€Ÿ def train_step(x, y): with tf.GradientTape() as tape: predictions = model(x) loss = loss_fn(y, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) return loss # è®­ç»ƒ epochs = 5 for epoch in range(epochs): total_loss = 0 num_batches = 0 for x_batch, y_batch in train_dataset: loss = train_step(x_batch, y_batch) total_loss &#43;= loss num_batches &#43;= 1 avg_loss = total_loss / num_batches print(f&#34;Epoch {epoch &#43; 1}, Loss: {avg_loss:.4f}&#34;) é«˜çº§è‡ªå®šä¹‰è®­ç»ƒ class AdvancedTrainer: def __init__(self, model, optimizer, loss_fn, metrics=None): self.model = model self.optimizer = optimizer self.loss_fn = loss_fn self.metrics = metrics or [] @tf.function def train_step(self, x, y): with tf.GradientTape() as tape: predictions = self.model(x, training=True) loss = self.loss_fn(y, predictions) # æ·»åŠ æ­£åˆ™åŒ–æŸå¤± for var in self.model.trainable_variables: loss &#43;= tf.nn.l2_loss(var) * 1e-4 gradients = tape.gradient(loss, self.model.trainable_variables) self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables)) # è®¡ç®—æŒ‡æ ‡ for metric in self.metrics: metric.update_state(y, predictions) return loss def train_epoch(self, dataset): total_loss = 0 num_batches = 0 for x_batch, y_batch in dataset: loss = self.train_step(x_batch, y_batch) total_loss &#43;= loss num_batches &#43;= 1 # é‡ç½®æŒ‡æ ‡çŠ¶æ€ for metric in self.metrics: metric.reset_states() return total_loss / num_batches # ä½¿ç”¨é«˜çº§è®­ç»ƒå™¨ model = CustomModel() optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) loss_fn = tf.keras.losses.CategoricalCrossentropy() # å®šä¹‰æŒ‡æ ‡ train_acc = tf.keras.metrics.CategoricalAccuracy() val_acc = tf.keras.metrics.CategoricalAccuracy() trainer = AdvancedTrainer(model, optimizer, loss_fn, [train_acc]) # è®­ç»ƒå¤šä¸ªepoch for epoch in range(10): loss = trainer.train_epoch(train_dataset) print(f&#34;Epoch {epoch &#43; 1}, Loss: {loss:.4f}, Accuracy: {train_acc.result():.4f}&#34;) ğŸ“Š åˆ†å¸ƒå¼è®­ç»ƒ å¤šGPUè®­ç»ƒ # ç­–ç•¥1: MirroredStrategy strategy = tf.distribute.MirroredStrategy() print(f&#34;GPUæ•°é‡: {strategy.num_replicas_in_sync}&#34;) with strategy.scope(): # åœ¨ç­–ç•¥èŒƒå›´å†…æ„å»ºæ¨¡å‹ model = tf.keras.Sequential([ tf.keras.layers.Dense(128, activation=&#39;relu&#39;, input_shape=(784,)), tf.keras.layers.Dense(64, activation=&#39;relu&#39;), tf.keras.layers.Dense(10, activation=&#39;softmax&#39;) ]) model.compile( optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;] ) # è®­ç»ƒ model.fit(x_train, y_train, epochs=5, batch_size=64) TPUè®­ç»ƒ # TPUè®­ç»ƒ resolver = tf.distribute.cluster_resolver.TPUClusterResolver() tf.config.experimental_connect_to_cluster(resolver) tf.tpu.experimental.initialize_tpu_system(resolver) # åˆ›å»ºTPUç­–ç•¥ strategy = tf.distribute.experimental.TPUStrategy(resolver) with strategy.scope(): model = create_model() # åœ¨TPUç­–ç•¥èŒƒå›´å†…åˆ›å»ºæ¨¡å‹ model.compile(...) # è®­ç»ƒ model.fit(train_dataset, epochs=10) è‡ªå®šä¹‰åˆ†å¸ƒå¼è®­ç»ƒ # è‡ªå®šä¹‰åˆ†å¸ƒå¼è®­ç»ƒå¾ªç¯ @tf.function def distributed_train_step(dataset_inputs): def train_step_fn(inputs): x, y = inputs with tf.GradientTape() as tape: predictions = model(x, training=True) loss = loss_fn(y, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) return loss # åœ¨æ‰€æœ‰å‰¯æœ¬ä¸Šè¿è¡Œ per_replica_losses = strategy.run(train_step_fn, args=(dataset_inputs,)) return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None) # åˆ†å¸ƒå¼è®­ç»ƒå¾ªç¯ for epoch in range(num_epochs): total_loss = 0 num_batches = 0 for x_batch, y_batch in distributed_dataset: loss = distributed_train_step((x_batch, y_batch)) total_loss &#43;= loss num_batches &#43;= 1 avg_loss = total_loss / num_batches print(f&#34;Epoch {epoch}, Loss: {avg_loss}&#34;) ğŸš€ æ¨¡å‹ä¼˜åŒ– æ··åˆç²¾åº¦è®­ç»ƒ from tensorflow.keras.mixed_precision import experimental as mixed_precision # è®¾ç½®æ··åˆç²¾åº¦ç­–ç•¥ policy = mixed_precision.Policy(&#39;mixed_float16&#39;) mixed_precision.set_policy(policy) # æ„å»ºæ¨¡å‹ï¼ˆè‡ªåŠ¨ä½¿ç”¨æ··åˆç²¾åº¦ï¼‰ with strategy.scope(): model = tf.keras.Sequential([ tf.keras.layers.Dense(128, activation=&#39;relu&#39;, input_shape=(784,)), tf.keras.layers.Dense(10, activation=&#39;softmax&#39;) ]) # ä½¿ç”¨æ··åˆç²¾åº¦ä¼˜åŒ–å™¨ optimizer = mixed_precision.LossScaleOptimizer( tf.keras.optimizers.Adam(), loss_scale=&#39;dynamic&#39; ) model.compile(optimizer=optimizer, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # è®­ç»ƒ model.fit(x_train, y_train, epochs=5) æ¨¡å‹é‡åŒ– import tensorflow_model_optimization as tfmot # åº”ç”¨é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ quantize_model = tfmot.quantization.keras.quantize_model q_aware_model = quantize_model(model) # ç¼–è¯‘é‡åŒ–æ¨¡å‹ q_aware_model.compile( optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;] ) # é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ q_aware_model.fit(x_train, y_train, epochs=5) # è½¬æ¢ä¸ºå®Œå…¨é‡åŒ–æ¨¡å‹ converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model) converter.optimizations = [tf.lite.Optimize.DEFAULT] quantized_tflite_model = converter.convert() æ¨¡å‹å‰ªæ # åº”ç”¨æ¨¡å‹å‰ªæ prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude # å®šä¹‰å‰ªæå‚æ•° pruning_params = { &#39;pruning_schedule&#39;: tfmot.sparsity.keras.PolynomialDecay( initial_sparsity=0.0, final_sparsity=0.5, begin_step=0, end_step=1000 ) } # åˆ›å»ºå‰ªææ¨¡å‹ pruned_model = prune_low_magnitude(model, **pruning_params) # ç¼–è¯‘å‰ªææ¨¡å‹ pruned_model.compile( optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;] ) # å‰ªæè®­ç»ƒ pruned_model.fit(x_train, y_train, epochs=10) # å‰¥ç¦»å‰ªæç»“æ„ stripped_pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model) ğŸ¨ é«˜çº§å±‚å’Œæ“ä½œ è‡ªå®šä¹‰å±‚ class AttentionLayer(tf.keras.layers.Layer): def __init__(self, units, **kwargs): super(AttentionLayer, self).__init__(**kwargs) self.units = units self.W = None self.b = None self.V = None def build(self, input_shape): self.W = self.add_weight( name=&#39;W&#39;, shape=(input_shape[-1], self.units), initializer=&#39;glorot_uniform&#39;, trainable=True ) self.b = self.add_weight( name=&#39;b&#39;, shape=(self.units,), initializer=&#39;zeros&#39;, trainable=True ) self.V = self.add_weight( name=&#39;V&#39;, shape=(self.units, 1), initializer=&#39;glorot_uniform&#39;, trainable=True ) def call(self, inputs): # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° score = tf.nn.tanh(tf.matmul(inputs, self.W) &#43; self.b) attention_weights = tf.nn.softmax(tf.matmul(score, self.V), axis=1) # åº”ç”¨æ³¨æ„åŠ›æƒé‡ context_vector = attention_weights * inputs context_vector = tf.reduce_sum(context_vector, axis=1) return context_vector, attention_weights # ä½¿ç”¨æ³¨æ„åŠ›å±‚ inputs = tf.keras.Input(shape=(10, 64)) # (batch_size, seq_len, features) context, attention = AttentionLayer(32)(inputs) outputs = tf.keras.layers.Dense(10, activation=&#39;softmax&#39;)(context) model = tf.keras.Model(inputs=inputs, outputs=outputs) è‡ªå®šä¹‰æŸå¤±å‡½æ•° class FocalLoss(tf.keras.losses.Loss): def __init__(self, alpha=1.0, gamma=2.0, **kwargs): super(FocalLoss, self).__init__(**kwargs) self.alpha = alpha self.gamma = gamma def call(self, y_true, y_pred): # å°†æ ‡ç­¾è½¬æ¢ä¸ºone-hotç¼–ç  y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=tf.shape(y_pred)[-1]) # è®¡ç®—äº¤å‰ç†µ ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred) # è®¡ç®—è°ƒåˆ¶å› å­ pt = tf.exp(-ce) focal_modulation = self.alpha * tf.pow((1 - pt), self.gamma) return focal_modulation * ce # ä½¿ç”¨ç„¦ç‚¹æŸå¤± model.compile( optimizer=&#39;adam&#39;, loss=FocalLoss(alpha=1.0, gamma=2.0), metrics=[&#39;accuracy&#39;] ) è‡ªå®šä¹‰æŒ‡æ ‡ class F1Score(tf.keras.metrics.Metric): def __init__(self, name=&#39;f1_score&#39;, **kwargs): super(F1Score, self).__init__(name=name, **kwargs) self.true_positives = self.add_weight(name=&#39;tp&#39;, initializer=&#39;zeros&#39;) self.false_positives = self.add_weight(name=&#39;fp&#39;, initializer=&#39;zeros&#39;) self.false_negatives = self.add_weight(name=&#39;fn&#39;, initializer=&#39;zeros&#39;) def update_state(self, y_true, y_pred, sample_weight=None): # è½¬æ¢ä¸ºäºŒåˆ†ç±» y_pred = tf.cast(tf.greater(y_pred, 0.5), tf.float32) y_true = tf.cast(y_true, tf.float32) # è®¡ç®—TP, FP, FN tp = tf.reduce_sum(y_true * y_pred) fp = tf.reduce_sum((1 - y_true) * y_pred) fn = tf.reduce_sum(y_true * (1 - y_pred)) self.true_positives.assign_add(tp) self.false_positives.assign_add(fp) self.false_negatives.assign_add(fn) def result(self): precision = self.true_positives / (self.true_positives &#43; self.false_positives &#43; 1e-15) recall = self.true_positives / (self.true_positives &#43; self.false_negatives &#43; 1e-15) return 2 * precision * recall / (precision &#43; recall &#43; 1e-15) def reset_states(self): self.true_positives.assign(0.0) self.false_positives.assign(0.0) self.false_negatives.assign(0.0) # ä½¿ç”¨F1åˆ†æ•°æŒ‡æ ‡ model.compile( optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;, F1Score()] ) ğŸ”§ æ€§èƒ½ä¼˜åŒ– TensorFlow Profiler import tensorflow as tf # åˆ›å»ºåˆ†æå™¨ profiler = tf.profiler.experimental.Profiler(&#39;/tmp/tf_profile&#39;) # å¯åŠ¨åˆ†æ tf.profiler.experimental.start(&#39;/tmp/tf_profile&#39;) # è¿è¡Œè®­ç»ƒä»£ç  model.fit(x_train, y_train, epochs=1) # åœæ­¢åˆ†æ tf.profiler.experimental.stop() # æŸ¥çœ‹åˆ†æç»“æœ # åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ http://localhost:6006 æŸ¥çœ‹TensorBoard å†…å­˜ä¼˜åŒ– # é™åˆ¶GPUå†…å­˜å¢é•¿ gpus = tf.config.experimental.list_physical_devices(&#39;GPU&#39;) if gpus: try: for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) except RuntimeError as e: print(e) # ä½¿ç”¨è™šæ‹ŸGPU tf.config.experimental.set_virtual_device_configuration( gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)] ) è®¡ç®—å›¾ä¼˜åŒ– # ä½¿ç”¨tf.functionç¼–è¯‘å‡½æ•° @tf.function def train_step(x, y): with tf.GradientTape() as tape: predictions = model(x) loss = loss_fn(y, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) return loss # XLAç¼–è¯‘ @tf.function(jit_compile=True) def optimized_train_step(x, y): return train_step(x, y) ğŸ“ˆ æ¨¡å‹è§£é‡Šæ€§ Grad-CAMå¯è§†åŒ– def grad_cam(model, image, layer_name, class_idx): &#34;&#34;&#34;Grad-CAMå¯è§†åŒ–&#34;&#34;&#34; # åˆ›å»ºæ¢¯åº¦æ¨¡å‹ grad_model = tf.keras.models.Model( inputs=model.input, outputs=[model.get_layer(layer_name).output, model.output] ) with tf.GradientTape() as tape: conv_outputs, predictions = grad_model(image) loss = predictions[:, class_idx] # è®¡ç®—æ¢¯åº¦ grads = tape.gradient(loss, conv_outputs) pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)) # åº”ç”¨Grad-CAM conv_outputs = conv_outputs[0] heatmap = tf.reduce_sum(pooled_grads * conv_outputs, axis=-1) heatmap = tf.maximum(heatmap, 0) # ReLU heatmap /= tf.reduce_max(heatmap) # å½’ä¸€åŒ– return heatmap.numpy() # ä½¿ç”¨Grad-CAM image = tf.expand_dims(x_test[0], 0) heatmap = grad_cam(model, image, &#39;dense1&#39;, class_idx=5) # å¯è§†åŒ–çƒ­åŠ›å›¾ plt.figure(figsize=(10, 4)) plt.subplot(1, 2, 1) plt.imshow(x_test[0].reshape(28, 28), cmap=&#39;gray&#39;) plt.title(&#39;åŸå§‹å›¾åƒ&#39;) plt.subplot(1, 2, 2) plt.imshow(heatmap, cmap=&#39;jet&#39;) plt.title(&#39;Grad-CAMçƒ­åŠ›å›¾&#39;) plt.show() ç‰¹å¾é‡è¦æ€§åˆ†æ def permutation_importance(model, x, y, feature_names): &#34;&#34;&#34;æ’åˆ—é‡è¦æ€§åˆ†æ&#34;&#34;&#34; baseline_score = model.evaluate(x, y, verbose=0)[1] importances = [] for i in range(x.shape[1]): # æ‰“ä¹±ç¬¬iä¸ªç‰¹å¾ x_permuted = x.copy() np.random.shuffle(x_permuted[:, i]) # è®¡ç®—æ‰“ä¹±åçš„åˆ†æ•° permuted_score = model.evaluate(x_permuted, y, verbose=0)[1] importance = baseline_score - permuted_score importances.append(importance) return dict(zip(feature_names, importances)) # åˆ†æç‰¹å¾é‡è¦æ€§ feature_names = [&#39;feature1&#39;, &#39;feature2&#39;, &#39;feature3&#39;, ...] importances = permutation_importance(model, x_test, y_test, feature_names) # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§ plt.figure(figsize=(10, 6)) plt.barh(list(importances.keys()), list(importances.values())) plt.xlabel(&#39;é‡è¦æ€§&#39;) plt.title(&#39;ç‰¹å¾é‡è¦æ€§åˆ†æ&#39;) plt.show() ğŸ¯ å®é™…é¡¹ç›®ï¼šå›¾åƒåˆ†ç±»å™¨ å®Œæ•´é¡¹ç›®ä»£ç  import tensorflow as tf import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.preprocessing.image import ImageDataGenerator # æ•°æ®å‡†å¤‡ (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data() # æ•°æ®å¢å¼º datagen = ImageDataGenerator( rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True, zoom_range=0.1 ) datagen.fit(x_train) # æ„å»ºé«˜çº§æ¨¡å‹ def create_advanced_model(): base_model = tf.keras.applications.ResNet50( weights=&#39;imagenet&#39;, include_top=False, input_shape=(32, 32, 3) ) # å†»ç»“åŸºç¡€æ¨¡å‹å±‚ base_model.trainable = False model = tf.keras.Sequential([ base_model, tf.keras.layers.GlobalAveragePooling2D(), tf.keras.layers.Dense(256, activation=&#39;relu&#39;), tf.keras.layers.BatchNormalization(), tf.keras.layers.Dropout(0.5), tf.keras.layers.Dense(10, activation=&#39;softmax&#39;) ]) return model # åˆ›å»ºå’Œç¼–è¯‘æ¨¡å‹ model = create_advanced_model() model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;] ) # è‡ªå®šä¹‰å›è°ƒ class AdvancedCallback(tf.keras.callbacks.Callback): def __init__(self): self.best_accuracy = 0 def on_epoch_end(self, epoch, logs=None): if logs[&#39;val_accuracy&#39;] &gt; self.best_accuracy: self.best_accuracy = logs[&#39;val_accuracy&#39;] self.model.save(&#39;best_model.h5&#39;) print(f&#34;\nä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œå‡†ç¡®ç‡: {self.best_accuracy:.4f}&#34;) # è®­ç»ƒæ¨¡å‹ history = model.fit( datagen.flow(x_train, y_train, batch_size=64), epochs=20, validation_data=(x_test, y_test), callbacks=[ AdvancedCallback(), tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True), tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3) ] ) # è¯„ä¼°æ¨¡å‹ test_loss, test_acc = model.evaluate(x_test, y_test) print(f&#34;æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}&#34;) # é¢„æµ‹å’Œå¯è§†åŒ– predictions = model.predict(x_test[:9]) predicted_classes = tf.argmax(predictions, axis=1).numpy() plt.figure(figsize=(12, 6)) for i in range(9): plt.subplot(3, 3, i&#43;1) plt.imshow(x_test[i]) plt.title(f&#34;é¢„æµ‹: {predicted_classes[i]}, çœŸå®: {y_test[i][0]}&#34;) plt.axis(&#39;off&#39;) plt.tight_layout() plt.show() ğŸ“š å­¦ä¹ èµ„æº å®˜æ–¹æ–‡æ¡£ TensorFlowé«˜çº§æ•™ç¨‹ TensorFlowæŒ‡å— TensorFlowæ€§èƒ½æŒ‡å— å´æ©è¾¾è¯¾ç¨‹ æ·±åº¦å­¦ä¹ è¯¾ç¨‹ä¸­å…³äºTensorFlowé«˜çº§ç‰¹æ€§çš„éƒ¨åˆ† ç»å…¸è®ºæ–‡ EfficientNet: Rethinking Model Scaling Batch Normalization Dropout: A Simple Way to Prevent Neural Networks from Overfitting ğŸ”§ æœ€ä½³å®è·µ ä»£ç ç»„ç»‡ # é«˜çº§é¡¹ç›®ç»“æ„ advanced_project/ â”œâ”€â”€ config/ â”‚ â”œâ”€â”€ __init__.py â”‚ â”œâ”€â”€ config.yaml â”‚ â””â”€â”€ hyperparameters.py â”œâ”€â”€ src/ â”‚ â”œâ”€â”€ __init__.py â”‚ â”œâ”€â”€ data/ â”‚ â”‚ â”œâ”€â”€ __init__.py â”‚ â”‚ â”œâ”€â”€ preprocessing.py â”‚ â”‚ â””â”€â”€ augmentation.py â”‚ â”œâ”€â”€ models/ â”‚ â”‚ â”œâ”€â”€ __init__.py â”‚ â”‚ â”œâ”€â”€ custom_layers.py â”‚ â”‚ â”œâ”€â”€ custom_losses.py â”‚ â”‚ â””â”€â”€ model_builder.py â”‚ â”œâ”€â”€ training/ â”‚ â”‚ â”œâ”€â”€ __init__.py â”‚ â”‚ â”œâ”€â”€ trainer.py â”‚ â”‚ â””â”€â”€ callbacks.py â”‚ â””â”€â”€ utils/ â”‚ â”œâ”€â”€ __init__.py â”‚ â””â”€â”€ visualization.py â”œâ”€â”€ notebooks/ â”‚ â””â”€â”€ experiments.ipynb â””â”€â”€ scripts/ â”œâ”€â”€ train.py â””â”€â”€ evaluate.py è°ƒè¯•æŠ€å·§ # å¯ç”¨è¯¦ç»†æ—¥å¿— tf.debugging.set_log_device_placement(True) # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§ with tf.GradientTape() as tape: predictions = model(x) loss = loss_fn(y, predictions) gradients = tape.gradient(loss, model.trainable_variables) # æ£€æŸ¥æ¢¯åº¦ for i, grad in enumerate(gradients): if grad is not None: print(f&#34;æ¢¯åº¦{i}çš„èŒƒæ•°: {tf.norm(grad).numpy()}&#34;) # ä½¿ç”¨tf.printè°ƒè¯• x = tf.print(x, [x], &#34;è°ƒè¯•ä¿¡æ¯:&#34;) æœ€è¿‘æ›´æ–°: {{ .Lastmod.Format â€œ2006-01-02â€ }}">

    <meta property="og:url" content="http://localhost:1313/wiki/deep_learning/tensorflow/advanced_1/">
  <meta property="og:site_name" content="Yaku Makki">
  <meta property="og:title" content="TensorFlowé«˜çº§ç‰¹æ€§">
  <meta property="og:description" content="TensorFlowé«˜çº§ç‰¹æ€§ æœ¬ç« æ·±å…¥ä»‹ç»TensorFlowçš„é«˜çº§ç‰¹æ€§ï¼ŒåŒ…æ‹¬è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€åˆ†å¸ƒå¼è®­ç»ƒã€æ¨¡å‹ä¼˜åŒ–ã€æ€§èƒ½è°ƒä¼˜ç­‰å†…å®¹ï¼Œå¸®åŠ©ä½ æ„å»ºæ›´å¤æ‚å’Œé«˜æ•ˆçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚
ğŸ¯ è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ åŸºç¡€è‡ªå®šä¹‰è®­ç»ƒ import tensorflow as tf import numpy as np # å‡†å¤‡æ•°æ® (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() x_train = x_train.reshape(-1, 784) / 255.0 x_test = x_test.reshape(-1, 784) / 255.0 y_train = tf.keras.utils.to_categorical(y_train, 10) y_test = tf.keras.utils.to_categorical(y_test, 10) # åˆ›å»ºæ•°æ®é›† train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) train_dataset = train_dataset.shuffle(1000).batch(32) # æ„å»ºæ¨¡å‹ class CustomModel(tf.keras.Model): def __init__(self): super(CustomModel, self).__init__() self.dense1 = tf.keras.layers.Dense(128, activation=&#39;relu&#39;) self.dense2 = tf.keras.layers.Dense(64, activation=&#39;relu&#39;) self.dense3 = tf.keras.layers.Dense(10, activation=&#39;softmax&#39;) def call(self, inputs): x = self.dense1(inputs) x = self.dense2(x) return self.dense3(x) model = CustomModel() # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ loss_fn = tf.keras.losses.CategoricalCrossentropy() optimizer = tf.keras.optimizers.Adam() # è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ @tf.function # ç¼–è¯‘åŠ é€Ÿ def train_step(x, y): with tf.GradientTape() as tape: predictions = model(x) loss = loss_fn(y, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) return loss # è®­ç»ƒ epochs = 5 for epoch in range(epochs): total_loss = 0 num_batches = 0 for x_batch, y_batch in train_dataset: loss = train_step(x_batch, y_batch) total_loss &#43;= loss num_batches &#43;= 1 avg_loss = total_loss / num_batches print(f&#34;Epoch {epoch &#43; 1}, Loss: {avg_loss:.4f}&#34;) é«˜çº§è‡ªå®šä¹‰è®­ç»ƒ class AdvancedTrainer: def __init__(self, model, optimizer, loss_fn, metrics=None): self.model = model self.optimizer = optimizer self.loss_fn = loss_fn self.metrics = metrics or [] @tf.function def train_step(self, x, y): with tf.GradientTape() as tape: predictions = self.model(x, training=True) loss = self.loss_fn(y, predictions) # æ·»åŠ æ­£åˆ™åŒ–æŸå¤± for var in self.model.trainable_variables: loss &#43;= tf.nn.l2_loss(var) * 1e-4 gradients = tape.gradient(loss, self.model.trainable_variables) self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables)) # è®¡ç®—æŒ‡æ ‡ for metric in self.metrics: metric.update_state(y, predictions) return loss def train_epoch(self, dataset): total_loss = 0 num_batches = 0 for x_batch, y_batch in dataset: loss = self.train_step(x_batch, y_batch) total_loss &#43;= loss num_batches &#43;= 1 # é‡ç½®æŒ‡æ ‡çŠ¶æ€ for metric in self.metrics: metric.reset_states() return total_loss / num_batches # ä½¿ç”¨é«˜çº§è®­ç»ƒå™¨ model = CustomModel() optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) loss_fn = tf.keras.losses.CategoricalCrossentropy() # å®šä¹‰æŒ‡æ ‡ train_acc = tf.keras.metrics.CategoricalAccuracy() val_acc = tf.keras.metrics.CategoricalAccuracy() trainer = AdvancedTrainer(model, optimizer, loss_fn, [train_acc]) # è®­ç»ƒå¤šä¸ªepoch for epoch in range(10): loss = trainer.train_epoch(train_dataset) print(f&#34;Epoch {epoch &#43; 1}, Loss: {loss:.4f}, Accuracy: {train_acc.result():.4f}&#34;) ğŸ“Š åˆ†å¸ƒå¼è®­ç»ƒ å¤šGPUè®­ç»ƒ # ç­–ç•¥1: MirroredStrategy strategy = tf.distribute.MirroredStrategy() print(f&#34;GPUæ•°é‡: {strategy.num_replicas_in_sync}&#34;) with strategy.scope(): # åœ¨ç­–ç•¥èŒƒå›´å†…æ„å»ºæ¨¡å‹ model = tf.keras.Sequential([ tf.keras.layers.Dense(128, activation=&#39;relu&#39;, input_shape=(784,)), tf.keras.layers.Dense(64, activation=&#39;relu&#39;), tf.keras.layers.Dense(10, activation=&#39;softmax&#39;) ]) model.compile( optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;] ) # è®­ç»ƒ model.fit(x_train, y_train, epochs=5, batch_size=64) TPUè®­ç»ƒ # TPUè®­ç»ƒ resolver = tf.distribute.cluster_resolver.TPUClusterResolver() tf.config.experimental_connect_to_cluster(resolver) tf.tpu.experimental.initialize_tpu_system(resolver) # åˆ›å»ºTPUç­–ç•¥ strategy = tf.distribute.experimental.TPUStrategy(resolver) with strategy.scope(): model = create_model() # åœ¨TPUç­–ç•¥èŒƒå›´å†…åˆ›å»ºæ¨¡å‹ model.compile(...) # è®­ç»ƒ model.fit(train_dataset, epochs=10) è‡ªå®šä¹‰åˆ†å¸ƒå¼è®­ç»ƒ # è‡ªå®šä¹‰åˆ†å¸ƒå¼è®­ç»ƒå¾ªç¯ @tf.function def distributed_train_step(dataset_inputs): def train_step_fn(inputs): x, y = inputs with tf.GradientTape() as tape: predictions = model(x, training=True) loss = loss_fn(y, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) return loss # åœ¨æ‰€æœ‰å‰¯æœ¬ä¸Šè¿è¡Œ per_replica_losses = strategy.run(train_step_fn, args=(dataset_inputs,)) return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None) # åˆ†å¸ƒå¼è®­ç»ƒå¾ªç¯ for epoch in range(num_epochs): total_loss = 0 num_batches = 0 for x_batch, y_batch in distributed_dataset: loss = distributed_train_step((x_batch, y_batch)) total_loss &#43;= loss num_batches &#43;= 1 avg_loss = total_loss / num_batches print(f&#34;Epoch {epoch}, Loss: {avg_loss}&#34;) ğŸš€ æ¨¡å‹ä¼˜åŒ– æ··åˆç²¾åº¦è®­ç»ƒ from tensorflow.keras.mixed_precision import experimental as mixed_precision # è®¾ç½®æ··åˆç²¾åº¦ç­–ç•¥ policy = mixed_precision.Policy(&#39;mixed_float16&#39;) mixed_precision.set_policy(policy) # æ„å»ºæ¨¡å‹ï¼ˆè‡ªåŠ¨ä½¿ç”¨æ··åˆç²¾åº¦ï¼‰ with strategy.scope(): model = tf.keras.Sequential([ tf.keras.layers.Dense(128, activation=&#39;relu&#39;, input_shape=(784,)), tf.keras.layers.Dense(10, activation=&#39;softmax&#39;) ]) # ä½¿ç”¨æ··åˆç²¾åº¦ä¼˜åŒ–å™¨ optimizer = mixed_precision.LossScaleOptimizer( tf.keras.optimizers.Adam(), loss_scale=&#39;dynamic&#39; ) model.compile(optimizer=optimizer, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # è®­ç»ƒ model.fit(x_train, y_train, epochs=5) æ¨¡å‹é‡åŒ– import tensorflow_model_optimization as tfmot # åº”ç”¨é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ quantize_model = tfmot.quantization.keras.quantize_model q_aware_model = quantize_model(model) # ç¼–è¯‘é‡åŒ–æ¨¡å‹ q_aware_model.compile( optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;] ) # é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ q_aware_model.fit(x_train, y_train, epochs=5) # è½¬æ¢ä¸ºå®Œå…¨é‡åŒ–æ¨¡å‹ converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model) converter.optimizations = [tf.lite.Optimize.DEFAULT] quantized_tflite_model = converter.convert() æ¨¡å‹å‰ªæ # åº”ç”¨æ¨¡å‹å‰ªæ prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude # å®šä¹‰å‰ªæå‚æ•° pruning_params = { &#39;pruning_schedule&#39;: tfmot.sparsity.keras.PolynomialDecay( initial_sparsity=0.0, final_sparsity=0.5, begin_step=0, end_step=1000 ) } # åˆ›å»ºå‰ªææ¨¡å‹ pruned_model = prune_low_magnitude(model, **pruning_params) # ç¼–è¯‘å‰ªææ¨¡å‹ pruned_model.compile( optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;] ) # å‰ªæè®­ç»ƒ pruned_model.fit(x_train, y_train, epochs=10) # å‰¥ç¦»å‰ªæç»“æ„ stripped_pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model) ğŸ¨ é«˜çº§å±‚å’Œæ“ä½œ è‡ªå®šä¹‰å±‚ class AttentionLayer(tf.keras.layers.Layer): def __init__(self, units, **kwargs): super(AttentionLayer, self).__init__(**kwargs) self.units = units self.W = None self.b = None self.V = None def build(self, input_shape): self.W = self.add_weight( name=&#39;W&#39;, shape=(input_shape[-1], self.units), initializer=&#39;glorot_uniform&#39;, trainable=True ) self.b = self.add_weight( name=&#39;b&#39;, shape=(self.units,), initializer=&#39;zeros&#39;, trainable=True ) self.V = self.add_weight( name=&#39;V&#39;, shape=(self.units, 1), initializer=&#39;glorot_uniform&#39;, trainable=True ) def call(self, inputs): # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° score = tf.nn.tanh(tf.matmul(inputs, self.W) &#43; self.b) attention_weights = tf.nn.softmax(tf.matmul(score, self.V), axis=1) # åº”ç”¨æ³¨æ„åŠ›æƒé‡ context_vector = attention_weights * inputs context_vector = tf.reduce_sum(context_vector, axis=1) return context_vector, attention_weights # ä½¿ç”¨æ³¨æ„åŠ›å±‚ inputs = tf.keras.Input(shape=(10, 64)) # (batch_size, seq_len, features) context, attention = AttentionLayer(32)(inputs) outputs = tf.keras.layers.Dense(10, activation=&#39;softmax&#39;)(context) model = tf.keras.Model(inputs=inputs, outputs=outputs) è‡ªå®šä¹‰æŸå¤±å‡½æ•° class FocalLoss(tf.keras.losses.Loss): def __init__(self, alpha=1.0, gamma=2.0, **kwargs): super(FocalLoss, self).__init__(**kwargs) self.alpha = alpha self.gamma = gamma def call(self, y_true, y_pred): # å°†æ ‡ç­¾è½¬æ¢ä¸ºone-hotç¼–ç  y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=tf.shape(y_pred)[-1]) # è®¡ç®—äº¤å‰ç†µ ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred) # è®¡ç®—è°ƒåˆ¶å› å­ pt = tf.exp(-ce) focal_modulation = self.alpha * tf.pow((1 - pt), self.gamma) return focal_modulation * ce # ä½¿ç”¨ç„¦ç‚¹æŸå¤± model.compile( optimizer=&#39;adam&#39;, loss=FocalLoss(alpha=1.0, gamma=2.0), metrics=[&#39;accuracy&#39;] ) è‡ªå®šä¹‰æŒ‡æ ‡ class F1Score(tf.keras.metrics.Metric): def __init__(self, name=&#39;f1_score&#39;, **kwargs): super(F1Score, self).__init__(name=name, **kwargs) self.true_positives = self.add_weight(name=&#39;tp&#39;, initializer=&#39;zeros&#39;) self.false_positives = self.add_weight(name=&#39;fp&#39;, initializer=&#39;zeros&#39;) self.false_negatives = self.add_weight(name=&#39;fn&#39;, initializer=&#39;zeros&#39;) def update_state(self, y_true, y_pred, sample_weight=None): # è½¬æ¢ä¸ºäºŒåˆ†ç±» y_pred = tf.cast(tf.greater(y_pred, 0.5), tf.float32) y_true = tf.cast(y_true, tf.float32) # è®¡ç®—TP, FP, FN tp = tf.reduce_sum(y_true * y_pred) fp = tf.reduce_sum((1 - y_true) * y_pred) fn = tf.reduce_sum(y_true * (1 - y_pred)) self.true_positives.assign_add(tp) self.false_positives.assign_add(fp) self.false_negatives.assign_add(fn) def result(self): precision = self.true_positives / (self.true_positives &#43; self.false_positives &#43; 1e-15) recall = self.true_positives / (self.true_positives &#43; self.false_negatives &#43; 1e-15) return 2 * precision * recall / (precision &#43; recall &#43; 1e-15) def reset_states(self): self.true_positives.assign(0.0) self.false_positives.assign(0.0) self.false_negatives.assign(0.0) # ä½¿ç”¨F1åˆ†æ•°æŒ‡æ ‡ model.compile( optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;, F1Score()] ) ğŸ”§ æ€§èƒ½ä¼˜åŒ– TensorFlow Profiler import tensorflow as tf # åˆ›å»ºåˆ†æå™¨ profiler = tf.profiler.experimental.Profiler(&#39;/tmp/tf_profile&#39;) # å¯åŠ¨åˆ†æ tf.profiler.experimental.start(&#39;/tmp/tf_profile&#39;) # è¿è¡Œè®­ç»ƒä»£ç  model.fit(x_train, y_train, epochs=1) # åœæ­¢åˆ†æ tf.profiler.experimental.stop() # æŸ¥çœ‹åˆ†æç»“æœ # åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ http://localhost:6006 æŸ¥çœ‹TensorBoard å†…å­˜ä¼˜åŒ– # é™åˆ¶GPUå†…å­˜å¢é•¿ gpus = tf.config.experimental.list_physical_devices(&#39;GPU&#39;) if gpus: try: for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) except RuntimeError as e: print(e) # ä½¿ç”¨è™šæ‹ŸGPU tf.config.experimental.set_virtual_device_configuration( gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)] ) è®¡ç®—å›¾ä¼˜åŒ– # ä½¿ç”¨tf.functionç¼–è¯‘å‡½æ•° @tf.function def train_step(x, y): with tf.GradientTape() as tape: predictions = model(x) loss = loss_fn(y, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) return loss # XLAç¼–è¯‘ @tf.function(jit_compile=True) def optimized_train_step(x, y): return train_step(x, y) ğŸ“ˆ æ¨¡å‹è§£é‡Šæ€§ Grad-CAMå¯è§†åŒ– def grad_cam(model, image, layer_name, class_idx): &#34;&#34;&#34;Grad-CAMå¯è§†åŒ–&#34;&#34;&#34; # åˆ›å»ºæ¢¯åº¦æ¨¡å‹ grad_model = tf.keras.models.Model( inputs=model.input, outputs=[model.get_layer(layer_name).output, model.output] ) with tf.GradientTape() as tape: conv_outputs, predictions = grad_model(image) loss = predictions[:, class_idx] # è®¡ç®—æ¢¯åº¦ grads = tape.gradient(loss, conv_outputs) pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)) # åº”ç”¨Grad-CAM conv_outputs = conv_outputs[0] heatmap = tf.reduce_sum(pooled_grads * conv_outputs, axis=-1) heatmap = tf.maximum(heatmap, 0) # ReLU heatmap /= tf.reduce_max(heatmap) # å½’ä¸€åŒ– return heatmap.numpy() # ä½¿ç”¨Grad-CAM image = tf.expand_dims(x_test[0], 0) heatmap = grad_cam(model, image, &#39;dense1&#39;, class_idx=5) # å¯è§†åŒ–çƒ­åŠ›å›¾ plt.figure(figsize=(10, 4)) plt.subplot(1, 2, 1) plt.imshow(x_test[0].reshape(28, 28), cmap=&#39;gray&#39;) plt.title(&#39;åŸå§‹å›¾åƒ&#39;) plt.subplot(1, 2, 2) plt.imshow(heatmap, cmap=&#39;jet&#39;) plt.title(&#39;Grad-CAMçƒ­åŠ›å›¾&#39;) plt.show() ç‰¹å¾é‡è¦æ€§åˆ†æ def permutation_importance(model, x, y, feature_names): &#34;&#34;&#34;æ’åˆ—é‡è¦æ€§åˆ†æ&#34;&#34;&#34; baseline_score = model.evaluate(x, y, verbose=0)[1] importances = [] for i in range(x.shape[1]): # æ‰“ä¹±ç¬¬iä¸ªç‰¹å¾ x_permuted = x.copy() np.random.shuffle(x_permuted[:, i]) # è®¡ç®—æ‰“ä¹±åçš„åˆ†æ•° permuted_score = model.evaluate(x_permuted, y, verbose=0)[1] importance = baseline_score - permuted_score importances.append(importance) return dict(zip(feature_names, importances)) # åˆ†æç‰¹å¾é‡è¦æ€§ feature_names = [&#39;feature1&#39;, &#39;feature2&#39;, &#39;feature3&#39;, ...] importances = permutation_importance(model, x_test, y_test, feature_names) # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§ plt.figure(figsize=(10, 6)) plt.barh(list(importances.keys()), list(importances.values())) plt.xlabel(&#39;é‡è¦æ€§&#39;) plt.title(&#39;ç‰¹å¾é‡è¦æ€§åˆ†æ&#39;) plt.show() ğŸ¯ å®é™…é¡¹ç›®ï¼šå›¾åƒåˆ†ç±»å™¨ å®Œæ•´é¡¹ç›®ä»£ç  import tensorflow as tf import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.preprocessing.image import ImageDataGenerator # æ•°æ®å‡†å¤‡ (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data() # æ•°æ®å¢å¼º datagen = ImageDataGenerator( rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True, zoom_range=0.1 ) datagen.fit(x_train) # æ„å»ºé«˜çº§æ¨¡å‹ def create_advanced_model(): base_model = tf.keras.applications.ResNet50( weights=&#39;imagenet&#39;, include_top=False, input_shape=(32, 32, 3) ) # å†»ç»“åŸºç¡€æ¨¡å‹å±‚ base_model.trainable = False model = tf.keras.Sequential([ base_model, tf.keras.layers.GlobalAveragePooling2D(), tf.keras.layers.Dense(256, activation=&#39;relu&#39;), tf.keras.layers.BatchNormalization(), tf.keras.layers.Dropout(0.5), tf.keras.layers.Dense(10, activation=&#39;softmax&#39;) ]) return model # åˆ›å»ºå’Œç¼–è¯‘æ¨¡å‹ model = create_advanced_model() model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;] ) # è‡ªå®šä¹‰å›è°ƒ class AdvancedCallback(tf.keras.callbacks.Callback): def __init__(self): self.best_accuracy = 0 def on_epoch_end(self, epoch, logs=None): if logs[&#39;val_accuracy&#39;] &gt; self.best_accuracy: self.best_accuracy = logs[&#39;val_accuracy&#39;] self.model.save(&#39;best_model.h5&#39;) print(f&#34;\nä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œå‡†ç¡®ç‡: {self.best_accuracy:.4f}&#34;) # è®­ç»ƒæ¨¡å‹ history = model.fit( datagen.flow(x_train, y_train, batch_size=64), epochs=20, validation_data=(x_test, y_test), callbacks=[ AdvancedCallback(), tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True), tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3) ] ) # è¯„ä¼°æ¨¡å‹ test_loss, test_acc = model.evaluate(x_test, y_test) print(f&#34;æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}&#34;) # é¢„æµ‹å’Œå¯è§†åŒ– predictions = model.predict(x_test[:9]) predicted_classes = tf.argmax(predictions, axis=1).numpy() plt.figure(figsize=(12, 6)) for i in range(9): plt.subplot(3, 3, i&#43;1) plt.imshow(x_test[i]) plt.title(f&#34;é¢„æµ‹: {predicted_classes[i]}, çœŸå®: {y_test[i][0]}&#34;) plt.axis(&#39;off&#39;) plt.tight_layout() plt.show() ğŸ“š å­¦ä¹ èµ„æº å®˜æ–¹æ–‡æ¡£ TensorFlowé«˜çº§æ•™ç¨‹ TensorFlowæŒ‡å— TensorFlowæ€§èƒ½æŒ‡å— å´æ©è¾¾è¯¾ç¨‹ æ·±åº¦å­¦ä¹ è¯¾ç¨‹ä¸­å…³äºTensorFlowé«˜çº§ç‰¹æ€§çš„éƒ¨åˆ† ç»å…¸è®ºæ–‡ EfficientNet: Rethinking Model Scaling Batch Normalization Dropout: A Simple Way to Prevent Neural Networks from Overfitting ğŸ”§ æœ€ä½³å®è·µ ä»£ç ç»„ç»‡ # é«˜çº§é¡¹ç›®ç»“æ„ advanced_project/ â”œâ”€â”€ config/ â”‚ â”œâ”€â”€ __init__.py â”‚ â”œâ”€â”€ config.yaml â”‚ â””â”€â”€ hyperparameters.py â”œâ”€â”€ src/ â”‚ â”œâ”€â”€ __init__.py â”‚ â”œâ”€â”€ data/ â”‚ â”‚ â”œâ”€â”€ __init__.py â”‚ â”‚ â”œâ”€â”€ preprocessing.py â”‚ â”‚ â””â”€â”€ augmentation.py â”‚ â”œâ”€â”€ models/ â”‚ â”‚ â”œâ”€â”€ __init__.py â”‚ â”‚ â”œâ”€â”€ custom_layers.py â”‚ â”‚ â”œâ”€â”€ custom_losses.py â”‚ â”‚ â””â”€â”€ model_builder.py â”‚ â”œâ”€â”€ training/ â”‚ â”‚ â”œâ”€â”€ __init__.py â”‚ â”‚ â”œâ”€â”€ trainer.py â”‚ â”‚ â””â”€â”€ callbacks.py â”‚ â””â”€â”€ utils/ â”‚ â”œâ”€â”€ __init__.py â”‚ â””â”€â”€ visualization.py â”œâ”€â”€ notebooks/ â”‚ â””â”€â”€ experiments.ipynb â””â”€â”€ scripts/ â”œâ”€â”€ train.py â””â”€â”€ evaluate.py è°ƒè¯•æŠ€å·§ # å¯ç”¨è¯¦ç»†æ—¥å¿— tf.debugging.set_log_device_placement(True) # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§ with tf.GradientTape() as tape: predictions = model(x) loss = loss_fn(y, predictions) gradients = tape.gradient(loss, model.trainable_variables) # æ£€æŸ¥æ¢¯åº¦ for i, grad in enumerate(gradients): if grad is not None: print(f&#34;æ¢¯åº¦{i}çš„èŒƒæ•°: {tf.norm(grad).numpy()}&#34;) # ä½¿ç”¨tf.printè°ƒè¯• x = tf.print(x, [x], &#34;è°ƒè¯•ä¿¡æ¯:&#34;) æœ€è¿‘æ›´æ–°: {{ .Lastmod.Format â€œ2006-01-02â€ }}">
  <meta property="og:locale" content="zh">
  <meta property="og:type" content="article">
    <meta property="article:section" content="wiki">
    <meta property="article:published_time" content="2025-10-24T21:58:49+08:00">
    <meta property="article:modified_time" content="2025-10-24T21:58:49+08:00">


  </head>
  <body class="synthwave-bg">
    <header class="app-header glass-morphism neon-border">
      <a href="/"><img class="app-header-avatar" src="/avatar.jpg" alt="Yaku Makki" /></a>
      <span class="app-header-title gradient-text">Yaku Makki</span>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">é¦–é¡µ</a>
             | 
          
          <a class="app-header-menu-item" href="/post/">åšå®¢</a>
             | 
          
          <a class="app-header-menu-item" href="/wiki/">ç»´åŸº</a>
             | 
          
          <a class="app-header-menu-item" href="/tags/">æ ‡ç­¾</a>
             | 
          
          <a class="app-header-menu-item" href="/about/">å…³äº</a>
      </nav>
      <p>ä¸ªäººå°å·¢</p>
      <div class="app-header-social">
        
          <a href="https://github.com/makkichan947" target="_blank" rel="noreferrer noopener me" class="pulse-glow">
            <svg class="icon icon-brand-github" viewBox="0 0 24 24" fill="currentColor"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
          </a>
        
          <a href="mailto:yakumakki947@hotmail.com" target="_blank" rel="noreferrer noopener me" class="pulse-glow">
            <svg class="icon icon-mail" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>mail</title><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline></svg>
          </a>
        
          <a href="https://x.com/Makki_Yaku947" target="_blank" rel="noreferrer noopener me" class="pulse-glow">
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-link">
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
</svg>
          </a>
        
          <a href="https://www.twitch.tv/u/makkichan947" target="_blank" rel="noreferrer noopener me" class="pulse-glow">
            <svg class="icon icon-brand-twitch" viewBox="0 0 24 24" fill="currentColor"><title>Twitch</title><path d="M11.571 4.714h1.715v5.143H11.57zm4.715 0H18v5.143h-1.714zM6 0L1.714 4.286v15.428h5.143V24l4.286-4.286h3.428L22.286 12V0zm14.571 11.143l-3.428 3.428h-3.429l-3 3v-3H6.857V1.714h13.714Z"/></svg>
          </a>
        
      </div>
    </header>
    <main class="app-container glass-morphism glow-effect">
      <div class="floating-particles">
        <div class="particle" style="left: 10%; animation-delay: 0s;"></div>
        <div class="particle" style="left: 20%; animation-delay: 1s;"></div>
        <div class="particle" style="left: 30%; animation-delay: 2s;"></div>
        <div class="particle" style="left: 40%; animation-delay: 3s;"></div>
        <div class="particle" style="left: 50%; animation-delay: 4s;"></div>
        <div class="particle" style="left: 60%; animation-delay: 5s;"></div>
        <div class="particle" style="left: 70%; animation-delay: 6s;"></div>
        <div class="particle" style="left: 80%; animation-delay: 7s;"></div>
        <div class="particle" style="left: 90%; animation-delay: 8s;"></div>
      </div>
      
  <article class="post enhanced-card glow-effect">
    <header class="post-header">
      <h1 class ="post-title gradient-text">TensorFlowé«˜çº§ç‰¹æ€§</h1>
      <div class="post-meta">
        <div>
          <svg class="icon icon-calendar" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
          Oct 24, 2025
        </div>
        <div>
          <svg class="icon icon-clock" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>clock</title><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>
          7 min read
        </div>
      </div>
    </header>
    <div class="post-content">
      <h1 id="tensorflowé«˜çº§ç‰¹æ€§">TensorFlowé«˜çº§ç‰¹æ€§</h1>
<p>æœ¬ç« æ·±å…¥ä»‹ç»TensorFlowçš„é«˜çº§ç‰¹æ€§ï¼ŒåŒ…æ‹¬è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€åˆ†å¸ƒå¼è®­ç»ƒã€æ¨¡å‹ä¼˜åŒ–ã€æ€§èƒ½è°ƒä¼˜ç­‰å†…å®¹ï¼Œå¸®åŠ©ä½ æ„å»ºæ›´å¤æ‚å’Œé«˜æ•ˆçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</p>
<h2 id="-è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯">ğŸ¯ è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯</h2>
<h3 id="åŸºç¡€è‡ªå®šä¹‰è®­ç»ƒ">åŸºç¡€è‡ªå®šä¹‰è®­ç»ƒ</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># å‡†å¤‡æ•°æ®</span>
</span></span><span style="display:flex;"><span>(x_train, y_train), (x_test, y_test) <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>datasets<span style="color:#f92672">.</span>mnist<span style="color:#f92672">.</span>load_data()
</span></span><span style="display:flex;"><span>x_train <span style="color:#f92672">=</span> x_train<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">784</span>) <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>
</span></span><span style="display:flex;"><span>x_test <span style="color:#f92672">=</span> x_test<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">784</span>) <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>
</span></span><span style="display:flex;"><span>y_train <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>to_categorical(y_train, <span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>y_test <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>to_categorical(y_test, <span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># åˆ›å»ºæ•°æ®é›†</span>
</span></span><span style="display:flex;"><span>train_dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_train, y_train))
</span></span><span style="display:flex;"><span>train_dataset <span style="color:#f92672">=</span> train_dataset<span style="color:#f92672">.</span>shuffle(<span style="color:#ae81ff">1000</span>)<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">32</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># æ„å»ºæ¨¡å‹</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CustomModel</span>(tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self):
</span></span><span style="display:flex;"><span>        super(CustomModel, self)<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dense1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dense2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">64</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dense3 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">10</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">call</span>(self, inputs):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dense1(inputs)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dense2(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>dense3(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> CustomModel()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨</span>
</span></span><span style="display:flex;"><span>loss_fn <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>losses<span style="color:#f92672">.</span>CategoricalCrossentropy()
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@tf.function</span>  <span style="color:#75715e"># ç¼–è¯‘åŠ é€Ÿ</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_step</span>(x, y):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
</span></span><span style="display:flex;"><span>        predictions <span style="color:#f92672">=</span> model(x)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> loss_fn(y, predictions)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    gradients <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(loss, model<span style="color:#f92672">.</span>trainable_variables)
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>apply_gradients(zip(gradients, model<span style="color:#f92672">.</span>trainable_variables))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># è®­ç»ƒ</span>
</span></span><span style="display:flex;"><span>epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(epochs):
</span></span><span style="display:flex;"><span>    total_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    num_batches <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> x_batch, y_batch <span style="color:#f92672">in</span> train_dataset:
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> train_step(x_batch, y_batch)
</span></span><span style="display:flex;"><span>        total_loss <span style="color:#f92672">+=</span> loss
</span></span><span style="display:flex;"><span>        num_batches <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    avg_loss <span style="color:#f92672">=</span> total_loss <span style="color:#f92672">/</span> num_batches
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Epoch </span><span style="color:#e6db74">{</span>epoch <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, Loss: </span><span style="color:#e6db74">{</span>avg_loss<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h3 id="é«˜çº§è‡ªå®šä¹‰è®­ç»ƒ">é«˜çº§è‡ªå®šä¹‰è®­ç»ƒ</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AdvancedTrainer</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, model, optimizer, loss_fn, metrics<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> model
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>optimizer <span style="color:#f92672">=</span> optimizer
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_fn <span style="color:#f92672">=</span> loss_fn
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>metrics <span style="color:#f92672">=</span> metrics <span style="color:#f92672">or</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@tf.function</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_step</span>(self, x, y):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
</span></span><span style="display:flex;"><span>            predictions <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model(x, training<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>loss_fn(y, predictions)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># æ·»åŠ æ­£åˆ™åŒ–æŸå¤±</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> var <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>trainable_variables:
</span></span><span style="display:flex;"><span>                loss <span style="color:#f92672">+=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>l2_loss(var) <span style="color:#f92672">*</span> <span style="color:#ae81ff">1e-4</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        gradients <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(loss, self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>trainable_variables)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>optimizer<span style="color:#f92672">.</span>apply_gradients(zip(gradients, self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>trainable_variables))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># è®¡ç®—æŒ‡æ ‡</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> metric <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>metrics:
</span></span><span style="display:flex;"><span>            metric<span style="color:#f92672">.</span>update_state(y, predictions)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_epoch</span>(self, dataset):
</span></span><span style="display:flex;"><span>        total_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        num_batches <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> x_batch, y_batch <span style="color:#f92672">in</span> dataset:
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>train_step(x_batch, y_batch)
</span></span><span style="display:flex;"><span>            total_loss <span style="color:#f92672">+=</span> loss
</span></span><span style="display:flex;"><span>            num_batches <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># é‡ç½®æŒ‡æ ‡çŠ¶æ€</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> metric <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>metrics:
</span></span><span style="display:flex;"><span>            metric<span style="color:#f92672">.</span>reset_states()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> total_loss <span style="color:#f92672">/</span> num_batches
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ä½¿ç”¨é«˜çº§è®­ç»ƒå™¨</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> CustomModel()
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.001</span>)
</span></span><span style="display:flex;"><span>loss_fn <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>losses<span style="color:#f92672">.</span>CategoricalCrossentropy()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># å®šä¹‰æŒ‡æ ‡</span>
</span></span><span style="display:flex;"><span>train_acc <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>metrics<span style="color:#f92672">.</span>CategoricalAccuracy()
</span></span><span style="display:flex;"><span>val_acc <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>metrics<span style="color:#f92672">.</span>CategoricalAccuracy()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>trainer <span style="color:#f92672">=</span> AdvancedTrainer(model, optimizer, loss_fn, [train_acc])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># è®­ç»ƒå¤šä¸ªepoch</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">10</span>):
</span></span><span style="display:flex;"><span>    loss <span style="color:#f92672">=</span> trainer<span style="color:#f92672">.</span>train_epoch(train_dataset)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Epoch </span><span style="color:#e6db74">{</span>epoch <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, Loss: </span><span style="color:#e6db74">{</span>loss<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, Accuracy: </span><span style="color:#e6db74">{</span>train_acc<span style="color:#f92672">.</span>result()<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h2 id="-åˆ†å¸ƒå¼è®­ç»ƒ">ğŸ“Š åˆ†å¸ƒå¼è®­ç»ƒ</h2>
<h3 id="å¤šgpuè®­ç»ƒ">å¤šGPUè®­ç»ƒ</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># ç­–ç•¥1: MirroredStrategy</span>
</span></span><span style="display:flex;"><span>strategy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>distribute<span style="color:#f92672">.</span>MirroredStrategy()
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;GPUæ•°é‡: </span><span style="color:#e6db74">{</span>strategy<span style="color:#f92672">.</span>num_replicas_in_sync<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> strategy<span style="color:#f92672">.</span>scope():
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># åœ¨ç­–ç•¥èŒƒå›´å†…æ„å»ºæ¨¡å‹</span>
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Sequential([
</span></span><span style="display:flex;"><span>        tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>, input_shape<span style="color:#f92672">=</span>(<span style="color:#ae81ff">784</span>,)),
</span></span><span style="display:flex;"><span>        tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">64</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>),
</span></span><span style="display:flex;"><span>        tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">10</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)
</span></span><span style="display:flex;"><span>    ])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>compile(
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>,
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;categorical_crossentropy&#39;</span>,
</span></span><span style="display:flex;"><span>        metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>]
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># è®­ç»ƒ</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(x_train, y_train, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>)
</span></span></code></pre></div><h3 id="tpuè®­ç»ƒ">TPUè®­ç»ƒ</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># TPUè®­ç»ƒ</span>
</span></span><span style="display:flex;"><span>resolver <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>distribute<span style="color:#f92672">.</span>cluster_resolver<span style="color:#f92672">.</span>TPUClusterResolver()
</span></span><span style="display:flex;"><span>tf<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>experimental_connect_to_cluster(resolver)
</span></span><span style="display:flex;"><span>tf<span style="color:#f92672">.</span>tpu<span style="color:#f92672">.</span>experimental<span style="color:#f92672">.</span>initialize_tpu_system(resolver)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># åˆ›å»ºTPUç­–ç•¥</span>
</span></span><span style="display:flex;"><span>strategy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>distribute<span style="color:#f92672">.</span>experimental<span style="color:#f92672">.</span>TPUStrategy(resolver)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> strategy<span style="color:#f92672">.</span>scope():
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> create_model()  <span style="color:#75715e"># åœ¨TPUç­–ç•¥èŒƒå›´å†…åˆ›å»ºæ¨¡å‹</span>
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>compile(<span style="color:#f92672">...</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># è®­ç»ƒ</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(train_dataset, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span></code></pre></div><h3 id="è‡ªå®šä¹‰åˆ†å¸ƒå¼è®­ç»ƒ">è‡ªå®šä¹‰åˆ†å¸ƒå¼è®­ç»ƒ</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># è‡ªå®šä¹‰åˆ†å¸ƒå¼è®­ç»ƒå¾ªç¯</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@tf.function</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">distributed_train_step</span>(dataset_inputs):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_step_fn</span>(inputs):
</span></span><span style="display:flex;"><span>        x, y <span style="color:#f92672">=</span> inputs
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
</span></span><span style="display:flex;"><span>            predictions <span style="color:#f92672">=</span> model(x, training<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">=</span> loss_fn(y, predictions)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        gradients <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(loss, model<span style="color:#f92672">.</span>trainable_variables)
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>apply_gradients(zip(gradients, model<span style="color:#f92672">.</span>trainable_variables))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># åœ¨æ‰€æœ‰å‰¯æœ¬ä¸Šè¿è¡Œ</span>
</span></span><span style="display:flex;"><span>    per_replica_losses <span style="color:#f92672">=</span> strategy<span style="color:#f92672">.</span>run(train_step_fn, args<span style="color:#f92672">=</span>(dataset_inputs,))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> strategy<span style="color:#f92672">.</span>reduce(tf<span style="color:#f92672">.</span>distribute<span style="color:#f92672">.</span>ReduceOp<span style="color:#f92672">.</span>SUM, per_replica_losses, axis<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># åˆ†å¸ƒå¼è®­ç»ƒå¾ªç¯</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(num_epochs):
</span></span><span style="display:flex;"><span>    total_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    num_batches <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> x_batch, y_batch <span style="color:#f92672">in</span> distributed_dataset:
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> distributed_train_step((x_batch, y_batch))
</span></span><span style="display:flex;"><span>        total_loss <span style="color:#f92672">+=</span> loss
</span></span><span style="display:flex;"><span>        num_batches <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    avg_loss <span style="color:#f92672">=</span> total_loss <span style="color:#f92672">/</span> num_batches
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Epoch </span><span style="color:#e6db74">{</span>epoch<span style="color:#e6db74">}</span><span style="color:#e6db74">, Loss: </span><span style="color:#e6db74">{</span>avg_loss<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h2 id="-æ¨¡å‹ä¼˜åŒ–">ğŸš€ æ¨¡å‹ä¼˜åŒ–</h2>
<h3 id="æ··åˆç²¾åº¦è®­ç»ƒ">æ··åˆç²¾åº¦è®­ç»ƒ</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.mixed_precision <span style="color:#f92672">import</span> experimental <span style="color:#66d9ef">as</span> mixed_precision
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># è®¾ç½®æ··åˆç²¾åº¦ç­–ç•¥</span>
</span></span><span style="display:flex;"><span>policy <span style="color:#f92672">=</span> mixed_precision<span style="color:#f92672">.</span>Policy(<span style="color:#e6db74">&#39;mixed_float16&#39;</span>)
</span></span><span style="display:flex;"><span>mixed_precision<span style="color:#f92672">.</span>set_policy(policy)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># æ„å»ºæ¨¡å‹ï¼ˆè‡ªåŠ¨ä½¿ç”¨æ··åˆç²¾åº¦ï¼‰</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> strategy<span style="color:#f92672">.</span>scope():
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Sequential([
</span></span><span style="display:flex;"><span>        tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>, input_shape<span style="color:#f92672">=</span>(<span style="color:#ae81ff">784</span>,)),
</span></span><span style="display:flex;"><span>        tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">10</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)
</span></span><span style="display:flex;"><span>    ])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># ä½¿ç”¨æ··åˆç²¾åº¦ä¼˜åŒ–å™¨</span>
</span></span><span style="display:flex;"><span>    optimizer <span style="color:#f92672">=</span> mixed_precision<span style="color:#f92672">.</span>LossScaleOptimizer(
</span></span><span style="display:flex;"><span>        tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(), loss_scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;dynamic&#39;</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span>optimizer, loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;categorical_crossentropy&#39;</span>, metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># è®­ç»ƒ</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(x_train, y_train, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
</span></span></code></pre></div><h3 id="æ¨¡å‹é‡åŒ–">æ¨¡å‹é‡åŒ–</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow_model_optimization <span style="color:#66d9ef">as</span> tfmot
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># åº”ç”¨é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ</span>
</span></span><span style="display:flex;"><span>quantize_model <span style="color:#f92672">=</span> tfmot<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>quantize_model
</span></span><span style="display:flex;"><span>q_aware_model <span style="color:#f92672">=</span> quantize_model(model)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ç¼–è¯‘é‡åŒ–æ¨¡å‹</span>
</span></span><span style="display:flex;"><span>q_aware_model<span style="color:#f92672">.</span>compile(
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>,
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;categorical_crossentropy&#39;</span>,
</span></span><span style="display:flex;"><span>    metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ</span>
</span></span><span style="display:flex;"><span>q_aware_model<span style="color:#f92672">.</span>fit(x_train, y_train, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># è½¬æ¢ä¸ºå®Œå…¨é‡åŒ–æ¨¡å‹</span>
</span></span><span style="display:flex;"><span>converter <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>lite<span style="color:#f92672">.</span>TFLiteConverter<span style="color:#f92672">.</span>from_keras_model(q_aware_model)
</span></span><span style="display:flex;"><span>converter<span style="color:#f92672">.</span>optimizations <span style="color:#f92672">=</span> [tf<span style="color:#f92672">.</span>lite<span style="color:#f92672">.</span>Optimize<span style="color:#f92672">.</span>DEFAULT]
</span></span><span style="display:flex;"><span>quantized_tflite_model <span style="color:#f92672">=</span> converter<span style="color:#f92672">.</span>convert()
</span></span></code></pre></div><h3 id="æ¨¡å‹å‰ªæ">æ¨¡å‹å‰ªæ</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># åº”ç”¨æ¨¡å‹å‰ªæ</span>
</span></span><span style="display:flex;"><span>prune_low_magnitude <span style="color:#f92672">=</span> tfmot<span style="color:#f92672">.</span>sparsity<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>prune_low_magnitude
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># å®šä¹‰å‰ªæå‚æ•°</span>
</span></span><span style="display:flex;"><span>pruning_params <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;pruning_schedule&#39;</span>: tfmot<span style="color:#f92672">.</span>sparsity<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>PolynomialDecay(
</span></span><span style="display:flex;"><span>        initial_sparsity<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>,
</span></span><span style="display:flex;"><span>        final_sparsity<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>,
</span></span><span style="display:flex;"><span>        begin_step<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>        end_step<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># åˆ›å»ºå‰ªææ¨¡å‹</span>
</span></span><span style="display:flex;"><span>pruned_model <span style="color:#f92672">=</span> prune_low_magnitude(model, <span style="color:#f92672">**</span>pruning_params)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ç¼–è¯‘å‰ªææ¨¡å‹</span>
</span></span><span style="display:flex;"><span>pruned_model<span style="color:#f92672">.</span>compile(
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>,
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;categorical_crossentropy&#39;</span>,
</span></span><span style="display:flex;"><span>    metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># å‰ªæè®­ç»ƒ</span>
</span></span><span style="display:flex;"><span>pruned_model<span style="color:#f92672">.</span>fit(x_train, y_train, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># å‰¥ç¦»å‰ªæç»“æ„</span>
</span></span><span style="display:flex;"><span>stripped_pruned_model <span style="color:#f92672">=</span> tfmot<span style="color:#f92672">.</span>sparsity<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>strip_pruning(pruned_model)
</span></span></code></pre></div><h2 id="-é«˜çº§å±‚å’Œæ“ä½œ">ğŸ¨ é«˜çº§å±‚å’Œæ“ä½œ</h2>
<h3 id="è‡ªå®šä¹‰å±‚">è‡ªå®šä¹‰å±‚</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AttentionLayer</span>(tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Layer):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, units, <span style="color:#f92672">**</span>kwargs):
</span></span><span style="display:flex;"><span>        super(AttentionLayer, self)<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>(<span style="color:#f92672">**</span>kwargs)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>units <span style="color:#f92672">=</span> units
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>b <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>V <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build</span>(self, input_shape):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>add_weight(
</span></span><span style="display:flex;"><span>            name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;W&#39;</span>,
</span></span><span style="display:flex;"><span>            shape<span style="color:#f92672">=</span>(input_shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], self<span style="color:#f92672">.</span>units),
</span></span><span style="display:flex;"><span>            initializer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;glorot_uniform&#39;</span>,
</span></span><span style="display:flex;"><span>            trainable<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>b <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>add_weight(
</span></span><span style="display:flex;"><span>            name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;b&#39;</span>,
</span></span><span style="display:flex;"><span>            shape<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>units,),
</span></span><span style="display:flex;"><span>            initializer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;zeros&#39;</span>,
</span></span><span style="display:flex;"><span>            trainable<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>V <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>add_weight(
</span></span><span style="display:flex;"><span>            name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;V&#39;</span>,
</span></span><span style="display:flex;"><span>            shape<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>units, <span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            initializer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;glorot_uniform&#39;</span>,
</span></span><span style="display:flex;"><span>            trainable<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">call</span>(self, inputs):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°</span>
</span></span><span style="display:flex;"><span>        score <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>tanh(tf<span style="color:#f92672">.</span>matmul(inputs, self<span style="color:#f92672">.</span>W) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>b)
</span></span><span style="display:flex;"><span>        attention_weights <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>softmax(tf<span style="color:#f92672">.</span>matmul(score, self<span style="color:#f92672">.</span>V), axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># åº”ç”¨æ³¨æ„åŠ›æƒé‡</span>
</span></span><span style="display:flex;"><span>        context_vector <span style="color:#f92672">=</span> attention_weights <span style="color:#f92672">*</span> inputs
</span></span><span style="display:flex;"><span>        context_vector <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_sum(context_vector, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> context_vector, attention_weights
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ä½¿ç”¨æ³¨æ„åŠ›å±‚</span>
</span></span><span style="display:flex;"><span>inputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Input(shape<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">64</span>))  <span style="color:#75715e"># (batch_size, seq_len, features)</span>
</span></span><span style="display:flex;"><span>context, attention <span style="color:#f92672">=</span> AttentionLayer(<span style="color:#ae81ff">32</span>)(inputs)
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">10</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)(context)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model(inputs<span style="color:#f92672">=</span>inputs, outputs<span style="color:#f92672">=</span>outputs)
</span></span></code></pre></div><h3 id="è‡ªå®šä¹‰æŸå¤±å‡½æ•°">è‡ªå®šä¹‰æŸå¤±å‡½æ•°</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">FocalLoss</span>(tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>losses<span style="color:#f92672">.</span>Loss):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">2.0</span>, <span style="color:#f92672">**</span>kwargs):
</span></span><span style="display:flex;"><span>        super(FocalLoss, self)<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>(<span style="color:#f92672">**</span>kwargs)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">=</span> alpha
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">=</span> gamma
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">call</span>(self, y_true, y_pred):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># å°†æ ‡ç­¾è½¬æ¢ä¸ºone-hotç¼–ç </span>
</span></span><span style="display:flex;"><span>        y_true <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>one_hot(tf<span style="color:#f92672">.</span>cast(y_true, tf<span style="color:#f92672">.</span>int32), depth<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>shape(y_pred)[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># è®¡ç®—äº¤å‰ç†µ</span>
</span></span><span style="display:flex;"><span>        ce <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>losses<span style="color:#f92672">.</span>categorical_crossentropy(y_true, y_pred)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># è®¡ç®—è°ƒåˆ¶å› å­</span>
</span></span><span style="display:flex;"><span>        pt <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>ce)
</span></span><span style="display:flex;"><span>        focal_modulation <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>pow((<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> pt), self<span style="color:#f92672">.</span>gamma)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> focal_modulation <span style="color:#f92672">*</span> ce
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ä½¿ç”¨ç„¦ç‚¹æŸå¤±</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>compile(
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>,
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">=</span>FocalLoss(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">2.0</span>),
</span></span><span style="display:flex;"><span>    metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>]
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h3 id="è‡ªå®šä¹‰æŒ‡æ ‡">è‡ªå®šä¹‰æŒ‡æ ‡</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">F1Score</span>(tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>metrics<span style="color:#f92672">.</span>Metric):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;f1_score&#39;</span>, <span style="color:#f92672">**</span>kwargs):
</span></span><span style="display:flex;"><span>        super(F1Score, self)<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>(name<span style="color:#f92672">=</span>name, <span style="color:#f92672">**</span>kwargs)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>true_positives <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>add_weight(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;tp&#39;</span>, initializer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;zeros&#39;</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>false_positives <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>add_weight(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;fp&#39;</span>, initializer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;zeros&#39;</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>false_negatives <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>add_weight(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;fn&#39;</span>, initializer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;zeros&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">update_state</span>(self, y_true, y_pred, sample_weight<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># è½¬æ¢ä¸ºäºŒåˆ†ç±»</span>
</span></span><span style="display:flex;"><span>        y_pred <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(tf<span style="color:#f92672">.</span>greater(y_pred, <span style="color:#ae81ff">0.5</span>), tf<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>        y_true <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cast(y_true, tf<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># è®¡ç®—TP, FP, FN</span>
</span></span><span style="display:flex;"><span>        tp <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_sum(y_true <span style="color:#f92672">*</span> y_pred)
</span></span><span style="display:flex;"><span>        fp <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_sum((<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> y_true) <span style="color:#f92672">*</span> y_pred)
</span></span><span style="display:flex;"><span>        fn <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_sum(y_true <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> y_pred))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>true_positives<span style="color:#f92672">.</span>assign_add(tp)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>false_positives<span style="color:#f92672">.</span>assign_add(fp)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>false_negatives<span style="color:#f92672">.</span>assign_add(fn)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">result</span>(self):
</span></span><span style="display:flex;"><span>        precision <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>true_positives <span style="color:#f92672">/</span> (self<span style="color:#f92672">.</span>true_positives <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>false_positives <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-15</span>)
</span></span><span style="display:flex;"><span>        recall <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>true_positives <span style="color:#f92672">/</span> (self<span style="color:#f92672">.</span>true_positives <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>false_negatives <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-15</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> precision <span style="color:#f92672">*</span> recall <span style="color:#f92672">/</span> (precision <span style="color:#f92672">+</span> recall <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-15</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">reset_states</span>(self):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>true_positives<span style="color:#f92672">.</span>assign(<span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>false_positives<span style="color:#f92672">.</span>assign(<span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>false_negatives<span style="color:#f92672">.</span>assign(<span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ä½¿ç”¨F1åˆ†æ•°æŒ‡æ ‡</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>compile(
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>,
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;binary_crossentropy&#39;</span>,
</span></span><span style="display:flex;"><span>    metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>, F1Score()]
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h2 id="-æ€§èƒ½ä¼˜åŒ–">ğŸ”§ æ€§èƒ½ä¼˜åŒ–</h2>
<h3 id="tensorflow-profiler">TensorFlow Profiler</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># åˆ›å»ºåˆ†æå™¨</span>
</span></span><span style="display:flex;"><span>profiler <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>profiler<span style="color:#f92672">.</span>experimental<span style="color:#f92672">.</span>Profiler(<span style="color:#e6db74">&#39;/tmp/tf_profile&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># å¯åŠ¨åˆ†æ</span>
</span></span><span style="display:flex;"><span>tf<span style="color:#f92672">.</span>profiler<span style="color:#f92672">.</span>experimental<span style="color:#f92672">.</span>start(<span style="color:#e6db74">&#39;/tmp/tf_profile&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># è¿è¡Œè®­ç»ƒä»£ç </span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(x_train, y_train, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># åœæ­¢åˆ†æ</span>
</span></span><span style="display:flex;"><span>tf<span style="color:#f92672">.</span>profiler<span style="color:#f92672">.</span>experimental<span style="color:#f92672">.</span>stop()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># æŸ¥çœ‹åˆ†æç»“æœ</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ http://localhost:6006 æŸ¥çœ‹TensorBoard</span>
</span></span></code></pre></div><h3 id="å†…å­˜ä¼˜åŒ–">å†…å­˜ä¼˜åŒ–</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># é™åˆ¶GPUå†…å­˜å¢é•¿</span>
</span></span><span style="display:flex;"><span>gpus <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>experimental<span style="color:#f92672">.</span>list_physical_devices(<span style="color:#e6db74">&#39;GPU&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> gpus:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> gpu <span style="color:#f92672">in</span> gpus:
</span></span><span style="display:flex;"><span>            tf<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>experimental<span style="color:#f92672">.</span>set_memory_growth(gpu, <span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">except</span> <span style="color:#a6e22e">RuntimeError</span> <span style="color:#66d9ef">as</span> e:
</span></span><span style="display:flex;"><span>        print(e)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ä½¿ç”¨è™šæ‹ŸGPU</span>
</span></span><span style="display:flex;"><span>tf<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>experimental<span style="color:#f92672">.</span>set_virtual_device_configuration(
</span></span><span style="display:flex;"><span>    gpus[<span style="color:#ae81ff">0</span>],
</span></span><span style="display:flex;"><span>    [tf<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>experimental<span style="color:#f92672">.</span>VirtualDeviceConfiguration(memory_limit<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>)]
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h3 id="è®¡ç®—å›¾ä¼˜åŒ–">è®¡ç®—å›¾ä¼˜åŒ–</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># ä½¿ç”¨tf.functionç¼–è¯‘å‡½æ•°</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@tf.function</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_step</span>(x, y):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
</span></span><span style="display:flex;"><span>        predictions <span style="color:#f92672">=</span> model(x)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> loss_fn(y, predictions)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    gradients <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(loss, model<span style="color:#f92672">.</span>trainable_variables)
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>apply_gradients(zip(gradients, model<span style="color:#f92672">.</span>trainable_variables))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># XLAç¼–è¯‘</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@tf.function</span>(jit_compile<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">optimized_train_step</span>(x, y):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> train_step(x, y)
</span></span></code></pre></div><h2 id="-æ¨¡å‹è§£é‡Šæ€§">ğŸ“ˆ æ¨¡å‹è§£é‡Šæ€§</h2>
<h3 id="grad-camå¯è§†åŒ–">Grad-CAMå¯è§†åŒ–</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad_cam</span>(model, image, layer_name, class_idx):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Grad-CAMå¯è§†åŒ–&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># åˆ›å»ºæ¢¯åº¦æ¨¡å‹</span>
</span></span><span style="display:flex;"><span>    grad_model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>Model(
</span></span><span style="display:flex;"><span>        inputs<span style="color:#f92672">=</span>model<span style="color:#f92672">.</span>input,
</span></span><span style="display:flex;"><span>        outputs<span style="color:#f92672">=</span>[model<span style="color:#f92672">.</span>get_layer(layer_name)<span style="color:#f92672">.</span>output, model<span style="color:#f92672">.</span>output]
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
</span></span><span style="display:flex;"><span>        conv_outputs, predictions <span style="color:#f92672">=</span> grad_model(image)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> predictions[:, class_idx]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># è®¡ç®—æ¢¯åº¦</span>
</span></span><span style="display:flex;"><span>    grads <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(loss, conv_outputs)
</span></span><span style="display:flex;"><span>    pooled_grads <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(grads, axis<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># åº”ç”¨Grad-CAM</span>
</span></span><span style="display:flex;"><span>    conv_outputs <span style="color:#f92672">=</span> conv_outputs[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    heatmap <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_sum(pooled_grads <span style="color:#f92672">*</span> conv_outputs, axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    heatmap <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>maximum(heatmap, <span style="color:#ae81ff">0</span>)  <span style="color:#75715e"># ReLU</span>
</span></span><span style="display:flex;"><span>    heatmap <span style="color:#f92672">/=</span> tf<span style="color:#f92672">.</span>reduce_max(heatmap)  <span style="color:#75715e"># å½’ä¸€åŒ–</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> heatmap<span style="color:#f92672">.</span>numpy()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ä½¿ç”¨Grad-CAM</span>
</span></span><span style="display:flex;"><span>image <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>expand_dims(x_test[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>heatmap <span style="color:#f92672">=</span> grad_cam(model, image, <span style="color:#e6db74">&#39;dense1&#39;</span>, class_idx<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># å¯è§†åŒ–çƒ­åŠ›å›¾</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>imshow(x_test[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">28</span>), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;åŸå§‹å›¾åƒ&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>imshow(heatmap, cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;jet&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Grad-CAMçƒ­åŠ›å›¾&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h3 id="ç‰¹å¾é‡è¦æ€§åˆ†æ">ç‰¹å¾é‡è¦æ€§åˆ†æ</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">permutation_importance</span>(model, x, y, feature_names):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;æ’åˆ—é‡è¦æ€§åˆ†æ&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    baseline_score <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>evaluate(x, y, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>    importances <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># æ‰“ä¹±ç¬¬iä¸ªç‰¹å¾</span>
</span></span><span style="display:flex;"><span>        x_permuted <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>copy()
</span></span><span style="display:flex;"><span>        np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>shuffle(x_permuted[:, i])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># è®¡ç®—æ‰“ä¹±åçš„åˆ†æ•°</span>
</span></span><span style="display:flex;"><span>        permuted_score <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>evaluate(x_permuted, y, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        importance <span style="color:#f92672">=</span> baseline_score <span style="color:#f92672">-</span> permuted_score
</span></span><span style="display:flex;"><span>        importances<span style="color:#f92672">.</span>append(importance)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> dict(zip(feature_names, importances))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># åˆ†æç‰¹å¾é‡è¦æ€§</span>
</span></span><span style="display:flex;"><span>feature_names <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;feature1&#39;</span>, <span style="color:#e6db74">&#39;feature2&#39;</span>, <span style="color:#e6db74">&#39;feature3&#39;</span>, <span style="color:#f92672">...</span>]
</span></span><span style="display:flex;"><span>importances <span style="color:#f92672">=</span> permutation_importance(model, x_test, y_test, feature_names)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>barh(list(importances<span style="color:#f92672">.</span>keys()), list(importances<span style="color:#f92672">.</span>values()))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;é‡è¦æ€§&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;ç‰¹å¾é‡è¦æ€§åˆ†æ&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h2 id="-å®é™…é¡¹ç›®å›¾åƒåˆ†ç±»å™¨">ğŸ¯ å®é™…é¡¹ç›®ï¼šå›¾åƒåˆ†ç±»å™¨</h2>
<h3 id="å®Œæ•´é¡¹ç›®ä»£ç ">å®Œæ•´é¡¹ç›®ä»£ç </h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.preprocessing.image <span style="color:#f92672">import</span> ImageDataGenerator
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># æ•°æ®å‡†å¤‡</span>
</span></span><span style="display:flex;"><span>(x_train, y_train), (x_test, y_test) <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>datasets<span style="color:#f92672">.</span>cifar10<span style="color:#f92672">.</span>load_data()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># æ•°æ®å¢å¼º</span>
</span></span><span style="display:flex;"><span>datagen <span style="color:#f92672">=</span> ImageDataGenerator(
</span></span><span style="display:flex;"><span>    rotation_range<span style="color:#f92672">=</span><span style="color:#ae81ff">15</span>,
</span></span><span style="display:flex;"><span>    width_shift_range<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>,
</span></span><span style="display:flex;"><span>    height_shift_range<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>,
</span></span><span style="display:flex;"><span>    horizontal_flip<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    zoom_range<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>datagen<span style="color:#f92672">.</span>fit(x_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># æ„å»ºé«˜çº§æ¨¡å‹</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_advanced_model</span>():
</span></span><span style="display:flex;"><span>    base_model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>applications<span style="color:#f92672">.</span>ResNet50(
</span></span><span style="display:flex;"><span>        weights<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;imagenet&#39;</span>,
</span></span><span style="display:flex;"><span>        include_top<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>        input_shape<span style="color:#f92672">=</span>(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># å†»ç»“åŸºç¡€æ¨¡å‹å±‚</span>
</span></span><span style="display:flex;"><span>    base_model<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Sequential([
</span></span><span style="display:flex;"><span>        base_model,
</span></span><span style="display:flex;"><span>        tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>GlobalAveragePooling2D(),
</span></span><span style="display:flex;"><span>        tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">256</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>),
</span></span><span style="display:flex;"><span>        tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>BatchNormalization(),
</span></span><span style="display:flex;"><span>        tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dropout(<span style="color:#ae81ff">0.5</span>),
</span></span><span style="display:flex;"><span>        tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">10</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)
</span></span><span style="display:flex;"><span>    ])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># åˆ›å»ºå’Œç¼–è¯‘æ¨¡å‹</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> create_advanced_model()
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>compile(
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.001</span>),
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sparse_categorical_crossentropy&#39;</span>,
</span></span><span style="display:flex;"><span>    metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># è‡ªå®šä¹‰å›è°ƒ</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AdvancedCallback</span>(tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>callbacks<span style="color:#f92672">.</span>Callback):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>best_accuracy <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">on_epoch_end</span>(self, epoch, logs<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> logs[<span style="color:#e6db74">&#39;val_accuracy&#39;</span>] <span style="color:#f92672">&gt;</span> self<span style="color:#f92672">.</span>best_accuracy:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>best_accuracy <span style="color:#f92672">=</span> logs[<span style="color:#e6db74">&#39;val_accuracy&#39;</span>]
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>save(<span style="color:#e6db74">&#39;best_model.h5&#39;</span>)
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œå‡†ç¡®ç‡: </span><span style="color:#e6db74">{</span>self<span style="color:#f92672">.</span>best_accuracy<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># è®­ç»ƒæ¨¡å‹</span>
</span></span><span style="display:flex;"><span>history <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>fit(
</span></span><span style="display:flex;"><span>    datagen<span style="color:#f92672">.</span>flow(x_train, y_train, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>),
</span></span><span style="display:flex;"><span>    epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>,
</span></span><span style="display:flex;"><span>    validation_data<span style="color:#f92672">=</span>(x_test, y_test),
</span></span><span style="display:flex;"><span>    callbacks<span style="color:#f92672">=</span>[
</span></span><span style="display:flex;"><span>        AdvancedCallback(),
</span></span><span style="display:flex;"><span>        tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>callbacks<span style="color:#f92672">.</span>EarlyStopping(patience<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, restore_best_weights<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>        tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>callbacks<span style="color:#f92672">.</span>ReduceLROnPlateau(factor<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, patience<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># è¯„ä¼°æ¨¡å‹</span>
</span></span><span style="display:flex;"><span>test_loss, test_acc <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>evaluate(x_test, y_test)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;æµ‹è¯•å‡†ç¡®ç‡: </span><span style="color:#e6db74">{</span>test_acc<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># é¢„æµ‹å’Œå¯è§†åŒ–</span>
</span></span><span style="display:flex;"><span>predictions <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(x_test[:<span style="color:#ae81ff">9</span>])
</span></span><span style="display:flex;"><span>predicted_classes <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>argmax(predictions, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>numpy()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">9</span>):
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>, i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>imshow(x_test[i])
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;é¢„æµ‹: </span><span style="color:#e6db74">{</span>predicted_classes[i]<span style="color:#e6db74">}</span><span style="color:#e6db74">, çœŸå®: </span><span style="color:#e6db74">{</span>y_test[i][<span style="color:#ae81ff">0</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#39;off&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h2 id="-å­¦ä¹ èµ„æº">ğŸ“š å­¦ä¹ èµ„æº</h2>
<h3 id="å®˜æ–¹æ–‡æ¡£">å®˜æ–¹æ–‡æ¡£</h3>
<ul>
<li><a href="https://www.tensorflow.org/tutorials">TensorFlowé«˜çº§æ•™ç¨‹</a></li>
<li><a href="https://www.tensorflow.org/guide">TensorFlowæŒ‡å—</a></li>
<li><a href="https://www.tensorflow.org/guide/performance">TensorFlowæ€§èƒ½æŒ‡å—</a></li>
</ul>
<h3 id="å´æ©è¾¾è¯¾ç¨‹">å´æ©è¾¾è¯¾ç¨‹</h3>
<ul>
<li>æ·±åº¦å­¦ä¹ è¯¾ç¨‹ä¸­å…³äºTensorFlowé«˜çº§ç‰¹æ€§çš„éƒ¨åˆ†</li>
</ul>
<h3 id="ç»å…¸è®ºæ–‡">ç»å…¸è®ºæ–‡</h3>
<ul>
<li><a href="https://arxiv.org/abs/1905.11946">EfficientNet: Rethinking Model Scaling</a></li>
<li><a href="https://arxiv.org/abs/1502.03167">Batch Normalization</a></li>
<li><a href="https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
</ul>
<h2 id="-æœ€ä½³å®è·µ">ğŸ”§ æœ€ä½³å®è·µ</h2>
<h3 id="ä»£ç ç»„ç»‡">ä»£ç ç»„ç»‡</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># é«˜çº§é¡¹ç›®ç»“æ„</span>
</span></span><span style="display:flex;"><span>advanced_project<span style="color:#f92672">/</span>
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> config<span style="color:#f92672">/</span>
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> <span style="color:#a6e22e">__init__</span><span style="color:#f92672">.</span>py
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> config<span style="color:#f92672">.</span>yaml
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â””â”€â”€</span> hyperparameters<span style="color:#f92672">.</span>py
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> src<span style="color:#f92672">/</span>
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> <span style="color:#a6e22e">__init__</span><span style="color:#f92672">.</span>py
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> data<span style="color:#f92672">/</span>
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> <span style="color:#a6e22e">__init__</span><span style="color:#f92672">.</span>py
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> preprocessing<span style="color:#f92672">.</span>py
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â””â”€â”€</span> augmentation<span style="color:#f92672">.</span>py
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> models<span style="color:#f92672">/</span>
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> <span style="color:#a6e22e">__init__</span><span style="color:#f92672">.</span>py
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> custom_layers<span style="color:#f92672">.</span>py
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> custom_losses<span style="color:#f92672">.</span>py
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â””â”€â”€</span> model_builder<span style="color:#f92672">.</span>py
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> training<span style="color:#f92672">/</span>
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> <span style="color:#a6e22e">__init__</span><span style="color:#f92672">.</span>py
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> trainer<span style="color:#f92672">.</span>py
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â””â”€â”€</span> callbacks<span style="color:#f92672">.</span>py
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â””â”€â”€</span> utils<span style="color:#f92672">/</span>
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>       <span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> <span style="color:#a6e22e">__init__</span><span style="color:#f92672">.</span>py
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>       <span style="color:#960050;background-color:#1e0010">â””â”€â”€</span> visualization<span style="color:#f92672">.</span>py
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> notebooks<span style="color:#f92672">/</span>
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â”‚</span>   <span style="color:#960050;background-color:#1e0010">â””â”€â”€</span> experiments<span style="color:#f92672">.</span>ipynb
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">â””â”€â”€</span> scripts<span style="color:#f92672">/</span>
</span></span><span style="display:flex;"><span>    <span style="color:#960050;background-color:#1e0010">â”œâ”€â”€</span> train<span style="color:#f92672">.</span>py
</span></span><span style="display:flex;"><span>    <span style="color:#960050;background-color:#1e0010">â””â”€â”€</span> evaluate<span style="color:#f92672">.</span>py
</span></span></code></pre></div><h3 id="è°ƒè¯•æŠ€å·§">è°ƒè¯•æŠ€å·§</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># å¯ç”¨è¯¦ç»†æ—¥å¿—</span>
</span></span><span style="display:flex;"><span>tf<span style="color:#f92672">.</span>debugging<span style="color:#f92672">.</span>set_log_device_placement(<span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
</span></span><span style="display:flex;"><span>    predictions <span style="color:#f92672">=</span> model(x)
</span></span><span style="display:flex;"><span>    loss <span style="color:#f92672">=</span> loss_fn(y, predictions)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>gradients <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(loss, model<span style="color:#f92672">.</span>trainable_variables)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># æ£€æŸ¥æ¢¯åº¦</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i, grad <span style="color:#f92672">in</span> enumerate(gradients):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> grad <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;æ¢¯åº¦</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">çš„èŒƒæ•°: </span><span style="color:#e6db74">{</span>tf<span style="color:#f92672">.</span>norm(grad)<span style="color:#f92672">.</span>numpy()<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ä½¿ç”¨tf.printè°ƒè¯•</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>print(x, [x], <span style="color:#e6db74">&#34;è°ƒè¯•ä¿¡æ¯:&#34;</span>)
</span></span></code></pre></div><hr>
<p><em>æœ€è¿‘æ›´æ–°: {{ .Lastmod.Format &ldquo;2006-01-02&rdquo; }}</em></p>

      
      <div class="post-separator">------------------------------------------------------------------------</div>
      
    </div>
    <div class="post-footer">
      
      <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
      <div class="busuanzi-container">
        <span id="busuanzi_container_page_pv">--æœ¬æ–‡æ€»é˜…è¯»é‡<span id="busuanzi_value_page_pv"></span>æ¬¡--</span>
      </div>
      
      
      <script src="https://utteranc.es/client.js"
        repo="makkichan947/makkichan947.github.io"
        issue-term="pathname"
        label="Utterances"
        theme="github-dark"
        crossorigin="anonymous"
        async>
      </script>
    </div>
  </article>

    </main>
  </body>
</html>
