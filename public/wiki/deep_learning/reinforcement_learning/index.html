<!doctype html>
<html lang="zh">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <title>å¼ºåŒ–å­¦ä¹  // Yaku Makki</title>
    <link rel="shortcut icon" href="./assets/avatar/avatar.jpg" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.151.2">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Yaku Makki" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="/css/main.min.422840b4454cb211f5b3dab61f082cdff508e88825173c31806835cb75529734.css" />
    

    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="å¼ºåŒ–å­¦ä¹ ">
  <meta name="twitter:description" content="å¼ºåŒ–å­¦ä¹  å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œé€šè¿‡æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œåœ¨æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆå°±ã€‚
ğŸ¯ å¼ºåŒ–å­¦ä¹ åŸºç¡€ é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP) å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ¡†æ¶æ˜¯é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼š
MDPåŒ…å«çš„å…ƒç´ ï¼š
çŠ¶æ€ç©ºé—´ Sï¼šç¯å¢ƒå¯èƒ½çš„æ‰€æœ‰çŠ¶æ€ åŠ¨ä½œç©ºé—´ Aï¼šæ™ºèƒ½ä½“å¯ä»¥æ‰§è¡Œçš„æ‰€æœ‰åŠ¨ä½œ å¥–åŠ±å‡½æ•° R(s,a)ï¼šä»çŠ¶æ€sæ‰§è¡ŒåŠ¨ä½œaåè·å¾—çš„å¥–åŠ± çŠ¶æ€è½¬ç§»æ¦‚ç‡ P(sâ€™|s,a)ï¼šä»çŠ¶æ€sæ‰§è¡ŒåŠ¨ä½œaåè½¬ç§»åˆ°çŠ¶æ€sâ€™çš„æ¦‚ç‡ æŠ˜æ‰£å› å­ Î³ï¼šæœªæ¥å¥–åŠ±çš„è¡°å‡å› å­ MDPçš„ç›®æ ‡ï¼š æ‰¾åˆ°ä¸€ä¸ªç­–ç•¥Ï€(a|s)ï¼Œä½¿å¾—ä»åˆå§‹çŠ¶æ€å¼€å§‹çš„ç´¯ç§¯æŠ˜æ‰£å¥–åŠ±æœ€å¤§åŒ–ï¼š $$\pi^* = \arg\max_\pi \mathbb{E}[\sum_{t=0}^\infty \gamma^t R(s_t, a_t)]$$
å¼ºåŒ–å­¦ä¹ ç»„æˆ æ™ºèƒ½ä½“ (Agent)ï¼š
è§‚å¯Ÿç¯å¢ƒçŠ¶æ€ é€‰æ‹©å’Œæ‰§è¡ŒåŠ¨ä½œ æ¥æ”¶å¥–åŠ±ä¿¡å· å­¦ä¹ æœ€ä¼˜ç­–ç•¥ ç¯å¢ƒ (Environment)ï¼š
æ¥æ”¶æ™ºèƒ½ä½“çš„åŠ¨ä½œ è½¬ç§»åˆ°æ–°çŠ¶æ€ æä¾›å¥–åŠ±ä¿¡å· æä¾›çŠ¶æ€è§‚å¯Ÿ ğŸ—ï¸ å¼ºåŒ–å­¦ä¹ ç®—æ³• Q-Learningç®—æ³• Q-Learningæ˜¯æœ€ç»å…¸çš„åŸºäºå€¼å‡½æ•°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼š
import numpy as np import random class QLearningAgent: def __init__(self, state_size, action_size, learning_rate=0.1, gamma=0.95, epsilon=1.0): self.state_size = state_size self.action_size = action_size self.learning_rate = learning_rate self.gamma = gamma # æŠ˜æ‰£å› å­ self.epsilon = epsilon # æ¢ç´¢ç‡ self.epsilon_min = 0.01 self.epsilon_decay = 0.995 # Qè¡¨ï¼šçŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•° self.q_table = np.zeros((state_size, action_size)) def choose_action(self, state): &#34;&#34;&#34;Îµ-è´ªå¿ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œ&#34;&#34;&#34; if random.uniform(0, 1) &lt; self.epsilon: return random.randint(0, self.action_size - 1) # æ¢ç´¢ else: return np.argmax(self.q_table[state]) # åˆ©ç”¨ def learn(self, state, action, reward, next_state, done): &#34;&#34;&#34;Q-Learningæ›´æ–°&#34;&#34;&#34; # å½“å‰Qå€¼ current_q = self.q_table[state, action] # ç›®æ ‡Qå€¼ if done: target_q = reward else: target_q = reward &#43; self.gamma * np.max(self.q_table[next_state]) # Qå€¼æ›´æ–° self.q_table[state, action] &#43;= self.learning_rate * (target_q - current_q) # è¡°å‡æ¢ç´¢ç‡ if self.epsilon &gt; self.epsilon_min: self.epsilon *= self.epsilon_decay # ä½¿ç”¨Q-Learning env = gym.make(&#39;FrozenLake-v1&#39;) state_size = env.observation_space.n action_size = env.action_space.n agent = QLearningAgent(state_size, action_size) # è®­ç»ƒ episodes = 1000 for episode in range(episodes): state = env.reset() done = False total_reward = 0 while not done: action = agent.choose_action(state) next_state, reward, done, _ = env.step(action) agent.learn(state, action, reward, next_state, done) state = next_state total_reward &#43;= reward if episode % 100 == 0: print(f&#34;Episode: {episode}, Total Reward: {total_reward}&#34;) æ·±åº¦Qç½‘ç»œ (DQN) DQNä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼Qå‡½æ•°ï¼š">

    <meta property="og:url" content="http://localhost:1313/wiki/deep_learning/reinforcement_learning/">
  <meta property="og:site_name" content="Yaku Makki">
  <meta property="og:title" content="å¼ºåŒ–å­¦ä¹ ">
  <meta property="og:description" content="å¼ºåŒ–å­¦ä¹  å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œé€šè¿‡æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œåœ¨æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆå°±ã€‚
ğŸ¯ å¼ºåŒ–å­¦ä¹ åŸºç¡€ é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP) å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ¡†æ¶æ˜¯é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼š
MDPåŒ…å«çš„å…ƒç´ ï¼š
çŠ¶æ€ç©ºé—´ Sï¼šç¯å¢ƒå¯èƒ½çš„æ‰€æœ‰çŠ¶æ€ åŠ¨ä½œç©ºé—´ Aï¼šæ™ºèƒ½ä½“å¯ä»¥æ‰§è¡Œçš„æ‰€æœ‰åŠ¨ä½œ å¥–åŠ±å‡½æ•° R(s,a)ï¼šä»çŠ¶æ€sæ‰§è¡ŒåŠ¨ä½œaåè·å¾—çš„å¥–åŠ± çŠ¶æ€è½¬ç§»æ¦‚ç‡ P(sâ€™|s,a)ï¼šä»çŠ¶æ€sæ‰§è¡ŒåŠ¨ä½œaåè½¬ç§»åˆ°çŠ¶æ€sâ€™çš„æ¦‚ç‡ æŠ˜æ‰£å› å­ Î³ï¼šæœªæ¥å¥–åŠ±çš„è¡°å‡å› å­ MDPçš„ç›®æ ‡ï¼š æ‰¾åˆ°ä¸€ä¸ªç­–ç•¥Ï€(a|s)ï¼Œä½¿å¾—ä»åˆå§‹çŠ¶æ€å¼€å§‹çš„ç´¯ç§¯æŠ˜æ‰£å¥–åŠ±æœ€å¤§åŒ–ï¼š $$\pi^* = \arg\max_\pi \mathbb{E}[\sum_{t=0}^\infty \gamma^t R(s_t, a_t)]$$
å¼ºåŒ–å­¦ä¹ ç»„æˆ æ™ºèƒ½ä½“ (Agent)ï¼š
è§‚å¯Ÿç¯å¢ƒçŠ¶æ€ é€‰æ‹©å’Œæ‰§è¡ŒåŠ¨ä½œ æ¥æ”¶å¥–åŠ±ä¿¡å· å­¦ä¹ æœ€ä¼˜ç­–ç•¥ ç¯å¢ƒ (Environment)ï¼š
æ¥æ”¶æ™ºèƒ½ä½“çš„åŠ¨ä½œ è½¬ç§»åˆ°æ–°çŠ¶æ€ æä¾›å¥–åŠ±ä¿¡å· æä¾›çŠ¶æ€è§‚å¯Ÿ ğŸ—ï¸ å¼ºåŒ–å­¦ä¹ ç®—æ³• Q-Learningç®—æ³• Q-Learningæ˜¯æœ€ç»å…¸çš„åŸºäºå€¼å‡½æ•°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼š
import numpy as np import random class QLearningAgent: def __init__(self, state_size, action_size, learning_rate=0.1, gamma=0.95, epsilon=1.0): self.state_size = state_size self.action_size = action_size self.learning_rate = learning_rate self.gamma = gamma # æŠ˜æ‰£å› å­ self.epsilon = epsilon # æ¢ç´¢ç‡ self.epsilon_min = 0.01 self.epsilon_decay = 0.995 # Qè¡¨ï¼šçŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•° self.q_table = np.zeros((state_size, action_size)) def choose_action(self, state): &#34;&#34;&#34;Îµ-è´ªå¿ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œ&#34;&#34;&#34; if random.uniform(0, 1) &lt; self.epsilon: return random.randint(0, self.action_size - 1) # æ¢ç´¢ else: return np.argmax(self.q_table[state]) # åˆ©ç”¨ def learn(self, state, action, reward, next_state, done): &#34;&#34;&#34;Q-Learningæ›´æ–°&#34;&#34;&#34; # å½“å‰Qå€¼ current_q = self.q_table[state, action] # ç›®æ ‡Qå€¼ if done: target_q = reward else: target_q = reward &#43; self.gamma * np.max(self.q_table[next_state]) # Qå€¼æ›´æ–° self.q_table[state, action] &#43;= self.learning_rate * (target_q - current_q) # è¡°å‡æ¢ç´¢ç‡ if self.epsilon &gt; self.epsilon_min: self.epsilon *= self.epsilon_decay # ä½¿ç”¨Q-Learning env = gym.make(&#39;FrozenLake-v1&#39;) state_size = env.observation_space.n action_size = env.action_space.n agent = QLearningAgent(state_size, action_size) # è®­ç»ƒ episodes = 1000 for episode in range(episodes): state = env.reset() done = False total_reward = 0 while not done: action = agent.choose_action(state) next_state, reward, done, _ = env.step(action) agent.learn(state, action, reward, next_state, done) state = next_state total_reward &#43;= reward if episode % 100 == 0: print(f&#34;Episode: {episode}, Total Reward: {total_reward}&#34;) æ·±åº¦Qç½‘ç»œ (DQN) DQNä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼Qå‡½æ•°ï¼š">
  <meta property="og:locale" content="zh">
  <meta property="og:type" content="article">
    <meta property="article:section" content="wiki">
    <meta property="article:published_time" content="2025-10-25T10:00:00+08:00">
    <meta property="article:modified_time" content="2025-10-25T10:00:00+08:00">


  </head>
  <body class="synthwave-bg">
    <header class="app-header glass-morphism neon-border">
      <a href="/"><img class="app-header-avatar" src="/assets/avatar/avatar.jpg" alt="Yaku Makki" /></a>
      <span class="app-header-title gradient-text">Yaku Makki</span>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">é¦–é¡µ</a>
             | 
          
          <a class="app-header-menu-item" href="/post/">åšå®¢</a>
             | 
          
          <a class="app-header-menu-item" href="/wiki/">ç»´åŸº</a>
             | 
          
          <a class="app-header-menu-item" href="/tags/">æ ‡ç­¾</a>
             | 
          
          <a class="app-header-menu-item" href="/about/">å…³äº</a>
      </nav>
      <p>ä¸ªäººå°å·¢</p>
      <div class="app-header-social">
        
          <a href="https://github.com/makkichan947" target="_blank" rel="noreferrer noopener me" class="pulse-glow">
            <svg class="icon icon-brand-github" viewBox="0 0 24 24" fill="currentColor"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
          </a>
        
          <a href="mailto:yakumakki947@hotmail.com" target="_blank" rel="noreferrer noopener me" class="pulse-glow">
            <svg class="icon icon-mail" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>mail</title><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline></svg>
          </a>
        
          <a href="https://x.com/Makki_Yaku947" target="_blank" rel="noreferrer noopener me" class="pulse-glow">
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-link">
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
</svg>
          </a>
        
          <a href="https://www.twitch.tv/u/makkichan947" target="_blank" rel="noreferrer noopener me" class="pulse-glow">
            <svg class="icon icon-brand-twitch" viewBox="0 0 24 24" fill="currentColor"><title>Twitch</title><path d="M11.571 4.714h1.715v5.143H11.57zm4.715 0H18v5.143h-1.714zM6 0L1.714 4.286v15.428h5.143V24l4.286-4.286h3.428L22.286 12V0zm14.571 11.143l-3.428 3.428h-3.429l-3 3v-3H6.857V1.714h13.714Z"/></svg>
          </a>
        
      </div>
    </header>
    <main class="app-container glass-morphism glow-effect">
      <div class="floating-particles">
        <div class="particle" style="left: 10%; animation-delay: 0s;"></div>
        <div class="particle" style="left: 20%; animation-delay: 1s;"></div>
        <div class="particle" style="left: 30%; animation-delay: 2s;"></div>
        <div class="particle" style="left: 40%; animation-delay: 3s;"></div>
        <div class="particle" style="left: 50%; animation-delay: 4s;"></div>
        <div class="particle" style="left: 60%; animation-delay: 5s;"></div>
        <div class="particle" style="left: 70%; animation-delay: 6s;"></div>
        <div class="particle" style="left: 80%; animation-delay: 7s;"></div>
        <div class="particle" style="left: 90%; animation-delay: 8s;"></div>
      </div>
      
  <article class="post enhanced-card glow-effect">
    <header class="post-header">
      <h1 class ="post-title gradient-text">å¼ºåŒ–å­¦ä¹ </h1>
      <div class="post-meta">
        <div>
          <svg class="icon icon-calendar" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
          Oct 25, 2025
        </div>
        <div>
          <svg class="icon icon-clock" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>clock</title><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>
          9 min read
        </div>
      </div>
    </header>
    <div class="post-content">
      <h1 id="å¼ºåŒ–å­¦ä¹ ">å¼ºåŒ–å­¦ä¹ </h1>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œé€šè¿‡æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œåœ¨æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆå°±ã€‚</p>
<h2 id="-å¼ºåŒ–å­¦ä¹ åŸºç¡€">ğŸ¯ å¼ºåŒ–å­¦ä¹ åŸºç¡€</h2>
<h3 id="é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹-mdp">é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)</h3>
<p>å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ¡†æ¶æ˜¯é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼š</p>
<p><strong>MDPåŒ…å«çš„å…ƒç´ </strong>ï¼š</p>
<ul>
<li><strong>çŠ¶æ€ç©ºé—´ S</strong>ï¼šç¯å¢ƒå¯èƒ½çš„æ‰€æœ‰çŠ¶æ€</li>
<li><strong>åŠ¨ä½œç©ºé—´ A</strong>ï¼šæ™ºèƒ½ä½“å¯ä»¥æ‰§è¡Œçš„æ‰€æœ‰åŠ¨ä½œ</li>
<li><strong>å¥–åŠ±å‡½æ•° R(s,a)</strong>ï¼šä»çŠ¶æ€sæ‰§è¡ŒåŠ¨ä½œaåè·å¾—çš„å¥–åŠ±</li>
<li><strong>çŠ¶æ€è½¬ç§»æ¦‚ç‡ P(s&rsquo;|s,a)</strong>ï¼šä»çŠ¶æ€sæ‰§è¡ŒåŠ¨ä½œaåè½¬ç§»åˆ°çŠ¶æ€s&rsquo;çš„æ¦‚ç‡</li>
<li><strong>æŠ˜æ‰£å› å­ Î³</strong>ï¼šæœªæ¥å¥–åŠ±çš„è¡°å‡å› å­</li>
</ul>
<p><strong>MDPçš„ç›®æ ‡</strong>ï¼š
æ‰¾åˆ°ä¸€ä¸ªç­–ç•¥Ï€(a|s)ï¼Œä½¿å¾—ä»åˆå§‹çŠ¶æ€å¼€å§‹çš„ç´¯ç§¯æŠ˜æ‰£å¥–åŠ±æœ€å¤§åŒ–ï¼š
$$\pi^* = \arg\max_\pi \mathbb{E}[\sum_{t=0}^\infty \gamma^t R(s_t, a_t)]$$</p>
<h3 id="å¼ºåŒ–å­¦ä¹ ç»„æˆ">å¼ºåŒ–å­¦ä¹ ç»„æˆ</h3>
<p><strong>æ™ºèƒ½ä½“ (Agent)</strong>ï¼š</p>
<ul>
<li>è§‚å¯Ÿç¯å¢ƒçŠ¶æ€</li>
<li>é€‰æ‹©å’Œæ‰§è¡ŒåŠ¨ä½œ</li>
<li>æ¥æ”¶å¥–åŠ±ä¿¡å·</li>
<li>å­¦ä¹ æœ€ä¼˜ç­–ç•¥</li>
</ul>
<p><strong>ç¯å¢ƒ (Environment)</strong>ï¼š</p>
<ul>
<li>æ¥æ”¶æ™ºèƒ½ä½“çš„åŠ¨ä½œ</li>
<li>è½¬ç§»åˆ°æ–°çŠ¶æ€</li>
<li>æä¾›å¥–åŠ±ä¿¡å·</li>
<li>æä¾›çŠ¶æ€è§‚å¯Ÿ</li>
</ul>
<h2 id="-å¼ºåŒ–å­¦ä¹ ç®—æ³•">ğŸ—ï¸ å¼ºåŒ–å­¦ä¹ ç®—æ³•</h2>
<h3 id="q-learningç®—æ³•">Q-Learningç®—æ³•</h3>
<p><strong>Q-Learning</strong>æ˜¯æœ€ç»å…¸çš„åŸºäºå€¼å‡½æ•°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> random
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">QLearningAgent</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, state_size, action_size, learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.95</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>state_size <span style="color:#f92672">=</span> state_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>action_size <span style="color:#f92672">=</span> action_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>learning_rate <span style="color:#f92672">=</span> learning_rate
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">=</span> gamma  <span style="color:#75715e"># æŠ˜æ‰£å› å­</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>epsilon <span style="color:#f92672">=</span> epsilon  <span style="color:#75715e"># æ¢ç´¢ç‡</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>epsilon_min <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>epsilon_decay <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.995</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Qè¡¨ï¼šçŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•°</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>q_table <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((state_size, action_size))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">choose_action</span>(self, state):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Îµ-è´ªå¿ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œ&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> random<span style="color:#f92672">.</span>uniform(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>) <span style="color:#f92672">&lt;</span> self<span style="color:#f92672">.</span>epsilon:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>, self<span style="color:#f92672">.</span>action_size <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># æ¢ç´¢</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>argmax(self<span style="color:#f92672">.</span>q_table[state])  <span style="color:#75715e"># åˆ©ç”¨</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">learn</span>(self, state, action, reward, next_state, done):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Q-Learningæ›´æ–°&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># å½“å‰Qå€¼</span>
</span></span><span style="display:flex;"><span>        current_q <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>q_table[state, action]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># ç›®æ ‡Qå€¼</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> done:
</span></span><span style="display:flex;"><span>            target_q <span style="color:#f92672">=</span> reward
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            target_q <span style="color:#f92672">=</span> reward <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>max(self<span style="color:#f92672">.</span>q_table[next_state])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Qå€¼æ›´æ–°</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>q_table[state, action] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>learning_rate <span style="color:#f92672">*</span> (target_q <span style="color:#f92672">-</span> current_q)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># è¡°å‡æ¢ç´¢ç‡</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>epsilon <span style="color:#f92672">&gt;</span> self<span style="color:#f92672">.</span>epsilon_min:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>epsilon <span style="color:#f92672">*=</span> self<span style="color:#f92672">.</span>epsilon_decay
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ä½¿ç”¨Q-Learning</span>
</span></span><span style="display:flex;"><span>env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(<span style="color:#e6db74">&#39;FrozenLake-v1&#39;</span>)
</span></span><span style="display:flex;"><span>state_size <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>observation_space<span style="color:#f92672">.</span>n
</span></span><span style="display:flex;"><span>action_size <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>n
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>agent <span style="color:#f92672">=</span> QLearningAgent(state_size, action_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># è®­ç»ƒ</span>
</span></span><span style="display:flex;"><span>episodes <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> episode <span style="color:#f92672">in</span> range(episodes):
</span></span><span style="display:flex;"><span>    state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>    done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>    total_reward <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
</span></span><span style="display:flex;"><span>        action <span style="color:#f92672">=</span> agent<span style="color:#f92672">.</span>choose_action(state)
</span></span><span style="display:flex;"><span>        next_state, reward, done, _ <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        agent<span style="color:#f92672">.</span>learn(state, action, reward, next_state, done)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        state <span style="color:#f92672">=</span> next_state
</span></span><span style="display:flex;"><span>        total_reward <span style="color:#f92672">+=</span> reward
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> episode <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Episode: </span><span style="color:#e6db74">{</span>episode<span style="color:#e6db74">}</span><span style="color:#e6db74">, Total Reward: </span><span style="color:#e6db74">{</span>total_reward<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h3 id="æ·±åº¦qç½‘ç»œ-dqn">æ·±åº¦Qç½‘ç»œ (DQN)</h3>
<p><strong>DQN</strong>ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼Qå‡½æ•°ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> random
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> deque
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DQNAgent</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, state_size, action_size):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>state_size <span style="color:#f92672">=</span> state_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>action_size <span style="color:#f92672">=</span> action_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>memory <span style="color:#f92672">=</span> deque(maxlen<span style="color:#f92672">=</span><span style="color:#ae81ff">2000</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.95</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>epsilon <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>epsilon_min <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>epsilon_decay <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.995</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.001</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">32</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_build_model()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>target_model <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_build_model()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>update_target_model()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_build_model</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;æ„å»ºDQNç½‘ç»œ&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Sequential([
</span></span><span style="display:flex;"><span>            tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">24</span>, input_dim<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>state_size, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>),
</span></span><span style="display:flex;"><span>            tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">24</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>),
</span></span><span style="display:flex;"><span>            tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(self<span style="color:#f92672">.</span>action_size, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>)
</span></span><span style="display:flex;"><span>        ])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        model<span style="color:#f92672">.</span>compile(
</span></span><span style="display:flex;"><span>            optimizer<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(learning_rate<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>learning_rate),
</span></span><span style="display:flex;"><span>            loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;mse&#39;</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">update_target_model</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;æ›´æ–°ç›®æ ‡ç½‘ç»œ&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>target_model<span style="color:#f92672">.</span>set_weights(self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>get_weights())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">remember</span>(self, state, action, reward, next_state, done):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;å­˜å‚¨ç»éªŒ&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>memory<span style="color:#f92672">.</span>append((state, action, reward, next_state, done))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">choose_action</span>(self, state):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Îµ-è´ªå¿ƒç­–ç•¥&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand() <span style="color:#f92672">&lt;=</span> self<span style="color:#f92672">.</span>epsilon:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> random<span style="color:#f92672">.</span>randrange(self<span style="color:#f92672">.</span>action_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        act_values <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>predict(state, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>argmax(act_values[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">replay</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;ç»éªŒå›æ”¾&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> len(self<span style="color:#f92672">.</span>memory) <span style="color:#f92672">&lt;</span> self<span style="color:#f92672">.</span>batch_size:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        minibatch <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>sample(self<span style="color:#f92672">.</span>memory, self<span style="color:#f92672">.</span>batch_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> state, action, reward, next_state, done <span style="color:#f92672">in</span> minibatch:
</span></span><span style="display:flex;"><span>            target <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>predict(state, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> done:
</span></span><span style="display:flex;"><span>                target[<span style="color:#ae81ff">0</span>][action] <span style="color:#f92672">=</span> reward
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                t <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>target_model<span style="color:#f92672">.</span>predict(next_state, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>                target[<span style="color:#ae81ff">0</span>][action] <span style="color:#f92672">=</span> reward <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>amax(t[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>fit(state, target, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>epsilon <span style="color:#f92672">&gt;</span> self<span style="color:#f92672">.</span>epsilon_min:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>epsilon <span style="color:#f92672">*=</span> self<span style="color:#f92672">.</span>epsilon_decay
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ä½¿ç”¨DQN</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> gym
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(<span style="color:#e6db74">&#39;CartPole-v1&#39;</span>)
</span></span><span style="display:flex;"><span>state_size <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>observation_space<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>action_size <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>n
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>agent <span style="color:#f92672">=</span> DQNAgent(state_size, action_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># è®­ç»ƒ</span>
</span></span><span style="display:flex;"><span>episodes <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> e <span style="color:#f92672">in</span> range(episodes):
</span></span><span style="display:flex;"><span>    state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>    state <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>reshape(state, [<span style="color:#ae81ff">1</span>, state_size])
</span></span><span style="display:flex;"><span>    total_reward <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> time <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">500</span>):
</span></span><span style="display:flex;"><span>        action <span style="color:#f92672">=</span> agent<span style="color:#f92672">.</span>choose_action(state)
</span></span><span style="display:flex;"><span>        next_state, reward, done, _ <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)
</span></span><span style="display:flex;"><span>        next_state <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>reshape(next_state, [<span style="color:#ae81ff">1</span>, state_size])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        agent<span style="color:#f92672">.</span>remember(state, action, reward, next_state, done)
</span></span><span style="display:flex;"><span>        agent<span style="color:#f92672">.</span>replay()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        state <span style="color:#f92672">=</span> next_state
</span></span><span style="display:flex;"><span>        total_reward <span style="color:#f92672">+=</span> reward
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> done:
</span></span><span style="display:flex;"><span>            agent<span style="color:#f92672">.</span>update_target_model()
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Episode: </span><span style="color:#e6db74">{</span>e<span style="color:#e6db74">}</span><span style="color:#e6db74">, Score: </span><span style="color:#e6db74">{</span>total_reward<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>
</span></span></code></pre></div><h3 id="ç­–ç•¥æ¢¯åº¦æ–¹æ³•">ç­–ç•¥æ¢¯åº¦æ–¹æ³•</h3>
<p><strong>REINFORCEç®—æ³•</strong>ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">REINFORCEAgent</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, state_size, action_size, learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.99</span>):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>state_size <span style="color:#f92672">=</span> state_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>action_size <span style="color:#f92672">=</span> action_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>learning_rate <span style="color:#f92672">=</span> learning_rate
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">=</span> gamma
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_build_model()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(learning_rate<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>learning_rate)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_build_model</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;æ„å»ºç­–ç•¥ç½‘ç»œ&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Sequential([
</span></span><span style="display:flex;"><span>            tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">24</span>, input_dim<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>state_size, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>),
</span></span><span style="display:flex;"><span>            tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">24</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>),
</span></span><span style="display:flex;"><span>            tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(self<span style="color:#f92672">.</span>action_size, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)
</span></span><span style="display:flex;"><span>        ])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">choose_action</span>(self, state):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;æ ¹æ®ç­–ç•¥é€‰æ‹©åŠ¨ä½œ&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        state <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>reshape(state, [<span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>state_size])
</span></span><span style="display:flex;"><span>        probs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>predict(state, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        action <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(self<span style="color:#f92672">.</span>action_size, p<span style="color:#f92672">=</span>probs)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> action, probs
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">learn</span>(self, states, actions, rewards):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;REINFORCEå­¦ä¹ &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        discounted_rewards <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_discount_rewards(rewards)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># è®¡ç®—ç­–ç•¥æ¦‚ç‡</span>
</span></span><span style="display:flex;"><span>            states <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(states)
</span></span><span style="display:flex;"><span>            probs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model(states)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># é€‰æ‹©å¯¹åº”åŠ¨ä½œçš„æ¦‚ç‡</span>
</span></span><span style="display:flex;"><span>            action_probs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>gather(probs, actions, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, batch_dims<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># è®¡ç®—æŸå¤±ï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰</span>
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(action_probs) <span style="color:#f92672">*</span> discounted_rewards)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°</span>
</span></span><span style="display:flex;"><span>        gradients <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(loss, self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>trainable_variables)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>optimizer<span style="color:#f92672">.</span>apply_gradients(zip(gradients, self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>trainable_variables))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> loss<span style="color:#f92672">.</span>numpy()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_discount_rewards</span>(self, rewards):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;è®¡ç®—æŠ˜æ‰£å¥–åŠ±&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        discounted_rewards <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros_like(rewards, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>        running_sum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> reversed(range(len(rewards))):
</span></span><span style="display:flex;"><span>            running_sum <span style="color:#f92672">=</span> running_sum <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">+</span> rewards[i]
</span></span><span style="display:flex;"><span>            discounted_rewards[i] <span style="color:#f92672">=</span> running_sum
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># æ ‡å‡†åŒ–</span>
</span></span><span style="display:flex;"><span>        discounted_rewards <span style="color:#f92672">-=</span> np<span style="color:#f92672">.</span>mean(discounted_rewards)
</span></span><span style="display:flex;"><span>        discounted_rewards <span style="color:#f92672">/=</span> np<span style="color:#f92672">.</span>std(discounted_rewards) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-8</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> discounted_rewards
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ä½¿ç”¨REINFORCE</span>
</span></span><span style="display:flex;"><span>agent <span style="color:#f92672">=</span> REINFORCEAgent(state_size, action_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># è®­ç»ƒ</span>
</span></span><span style="display:flex;"><span>episodes <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> episode <span style="color:#f92672">in</span> range(episodes):
</span></span><span style="display:flex;"><span>    states <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    actions <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    rewards <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>    done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
</span></span><span style="display:flex;"><span>        action, prob <span style="color:#f92672">=</span> agent<span style="color:#f92672">.</span>choose_action(state)
</span></span><span style="display:flex;"><span>        next_state, reward, done, _ <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        states<span style="color:#f92672">.</span>append(state)
</span></span><span style="display:flex;"><span>        actions<span style="color:#f92672">.</span>append(action)
</span></span><span style="display:flex;"><span>        rewards<span style="color:#f92672">.</span>append(reward)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        state <span style="color:#f92672">=</span> next_state
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    loss <span style="color:#f92672">=</span> agent<span style="color:#f92672">.</span>learn(states, actions, rewards)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> episode <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Episode: </span><span style="color:#e6db74">{</span>episode<span style="color:#e6db74">}</span><span style="color:#e6db74">, Loss: </span><span style="color:#e6db74">{</span>loss<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h2 id="-é«˜çº§å¼ºåŒ–å­¦ä¹ ç®—æ³•">ğŸ® é«˜çº§å¼ºåŒ–å­¦ä¹ ç®—æ³•</h2>
<h3 id="actor-criticæ–¹æ³•">Actor-Criticæ–¹æ³•</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ActorCriticAgent</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, state_size, action_size, learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.001</span>):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>state_size <span style="color:#f92672">=</span> state_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>action_size <span style="color:#f92672">=</span> action_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>learning_rate <span style="color:#f92672">=</span> learning_rate
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.99</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Actorç½‘ç»œï¼ˆç­–ç•¥ï¼‰</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>actor <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_build_actor()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Criticç½‘ç»œï¼ˆä»·å€¼ï¼‰</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>critic <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_build_critic()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>actor_optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(learning_rate<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>learning_rate)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>critic_optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(learning_rate<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>learning_rate)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_build_actor</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;æ„å»ºActorç½‘ç»œ&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        inputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Input(shape<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>state_size,))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">24</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(inputs)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">24</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(self<span style="color:#f92672">.</span>action_size, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model(inputs<span style="color:#f92672">=</span>inputs, outputs<span style="color:#f92672">=</span>outputs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_build_critic</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;æ„å»ºCriticç½‘ç»œ&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        inputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Input(shape<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>state_size,))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">24</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(inputs)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">24</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">1</span>)(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model(inputs<span style="color:#f92672">=</span>inputs, outputs<span style="color:#f92672">=</span>outputs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">choose_action</span>(self, state):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;é€‰æ‹©åŠ¨ä½œ&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        state <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>reshape(state, [<span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>state_size])
</span></span><span style="display:flex;"><span>        probs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>actor<span style="color:#f92672">.</span>predict(state, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        action <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(self<span style="color:#f92672">.</span>action_size, p<span style="color:#f92672">=</span>probs)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> action, probs
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">learn</span>(self, state, action, reward, next_state, done):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Actor-Criticå­¦ä¹ &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        state <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>reshape(state, [<span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>state_size])
</span></span><span style="display:flex;"><span>        next_state <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>reshape(next_state, [<span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>state_size])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Criticæ›´æ–°</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
</span></span><span style="display:flex;"><span>            value <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>critic(state)
</span></span><span style="display:flex;"><span>            next_value <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>critic(next_state) <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> done <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            target <span style="color:#f92672">=</span> reward <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">*</span> next_value
</span></span><span style="display:flex;"><span>            critic_loss <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(target <span style="color:#f92672">-</span> value))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        critic_grads <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(critic_loss, self<span style="color:#f92672">.</span>critic<span style="color:#f92672">.</span>trainable_variables)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>critic_optimizer<span style="color:#f92672">.</span>apply_gradients(zip(critic_grads, self<span style="color:#f92672">.</span>critic<span style="color:#f92672">.</span>trainable_variables))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Actoræ›´æ–°</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
</span></span><span style="display:flex;"><span>            probs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>actor(state)
</span></span><span style="display:flex;"><span>            action_prob <span style="color:#f92672">=</span> probs[<span style="color:#ae81ff">0</span>][action]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            advantage <span style="color:#f92672">=</span> target <span style="color:#f92672">-</span> value
</span></span><span style="display:flex;"><span>            actor_loss <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>log(action_prob) <span style="color:#f92672">*</span> advantage
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        actor_grads <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(actor_loss, self<span style="color:#f92672">.</span>actor<span style="color:#f92672">.</span>trainable_variables)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>actor_optimizer<span style="color:#f92672">.</span>apply_gradients(zip(actor_grads, self<span style="color:#f92672">.</span>actor<span style="color:#f92672">.</span>trainable_variables))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> critic_loss<span style="color:#f92672">.</span>numpy(), actor_loss<span style="color:#f92672">.</span>numpy()
</span></span></code></pre></div><h3 id="è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–-ppo">è¿‘ç«¯ç­–ç•¥ä¼˜åŒ– (PPO)</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PPOAgent</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, state_size, action_size, clip_ratio<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>state_size <span style="color:#f92672">=</span> state_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>action_size <span style="color:#f92672">=</span> action_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>clip_ratio <span style="color:#f92672">=</span> clip_ratio
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.99</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0003</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>actor <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_build_actor()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>critic <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_build_critic()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>actor_optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(learning_rate<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>learning_rate)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>critic_optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(learning_rate<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>learning_rate)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_build_actor</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;æ„å»ºActorç½‘ç»œ&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        inputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Input(shape<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>state_size,))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">64</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(inputs)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">64</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(self<span style="color:#f92672">.</span>action_size, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model(inputs<span style="color:#f92672">=</span>inputs, outputs<span style="color:#f92672">=</span>outputs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_build_critic</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;æ„å»ºCriticç½‘ç»œ&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        inputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Input(shape<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>state_size,))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">64</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(inputs)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">64</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">1</span>)(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model(inputs<span style="color:#f92672">=</span>inputs, outputs<span style="color:#f92672">=</span>outputs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_action</span>(self, state):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;è·å–åŠ¨ä½œå’Œæ¦‚ç‡&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        state <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>reshape(state, [<span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>state_size])
</span></span><span style="display:flex;"><span>        probs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>actor<span style="color:#f92672">.</span>predict(state, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        action <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(self<span style="color:#f92672">.</span>action_size, p<span style="color:#f92672">=</span>probs)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> action, probs[action]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_advantages</span>(self, rewards, values, next_values, dones):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;è®¡ç®—ä¼˜åŠ¿å‡½æ•°&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        advantages <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros_like(rewards)
</span></span><span style="display:flex;"><span>        last_gae_lam <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> reversed(range(len(rewards))):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> t <span style="color:#f92672">==</span> len(rewards) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>                next_non_terminal <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> dones[t]
</span></span><span style="display:flex;"><span>                next_values <span style="color:#f92672">=</span> next_values[t]
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                next_non_terminal <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> dones[t]
</span></span><span style="display:flex;"><span>                next_values <span style="color:#f92672">=</span> values[t <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            delta <span style="color:#f92672">=</span> rewards[t] <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">*</span> next_values <span style="color:#f92672">*</span> next_non_terminal <span style="color:#f92672">-</span> values[t]
</span></span><span style="display:flex;"><span>            advantages[t] <span style="color:#f92672">=</span> last_gae_lam <span style="color:#f92672">=</span> delta <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.95</span> <span style="color:#f92672">*</span> next_non_terminal <span style="color:#f92672">*</span> last_gae_lam
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> advantages
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train</span>(self, states, actions, old_probs, advantages, returns):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;PPOè®­ç»ƒ&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
</span></span><span style="display:flex;"><span>            probs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>actor(states)
</span></span><span style="display:flex;"><span>            values <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>critic(states)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># è®¡ç®—ç­–ç•¥æŸå¤±</span>
</span></span><span style="display:flex;"><span>            new_probs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>gather(probs, actions, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, batch_dims<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            ratio <span style="color:#f92672">=</span> new_probs <span style="color:#f92672">/</span> old_probs
</span></span><span style="display:flex;"><span>            clipped_ratio <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>clip_by_value(ratio, <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>clip_ratio, <span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>clip_ratio)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            policy_loss <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>minimum(ratio <span style="color:#f92672">*</span> advantages, clipped_ratio <span style="color:#f92672">*</span> advantages))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># è®¡ç®—ä»·å€¼æŸå¤±</span>
</span></span><span style="display:flex;"><span>            value_loss <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(returns <span style="color:#f92672">-</span> values))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># æ€»æŸå¤±</span>
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">=</span> policy_loss <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> value_loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># æ›´æ–°Actor</span>
</span></span><span style="display:flex;"><span>        actor_grads <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(policy_loss, self<span style="color:#f92672">.</span>actor<span style="color:#f92672">.</span>trainable_variables)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>actor_optimizer<span style="color:#f92672">.</span>apply_gradients(zip(actor_grads, self<span style="color:#f92672">.</span>actor<span style="color:#f92672">.</span>trainable_variables))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># æ›´æ–°Critic</span>
</span></span><span style="display:flex;"><span>        critic_grads <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(value_loss, self<span style="color:#f92672">.</span>critic<span style="color:#f92672">.</span>trainable_variables)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>critic_optimizer<span style="color:#f92672">.</span>apply_gradients(zip(critic_grads, self<span style="color:#f92672">.</span>critic<span style="color:#f92672">.</span>trainable_variables))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> loss<span style="color:#f92672">.</span>numpy()
</span></span></code></pre></div><h2 id="-åº”ç”¨é¢†åŸŸ">ğŸ¯ åº”ç”¨é¢†åŸŸ</h2>
<h3 id="æ¸¸æˆai">æ¸¸æˆAI</h3>
<p><strong>AlphaGoçš„å¯å‘</strong>ï¼š</p>
<ul>
<li><strong>è’™ç‰¹å¡æ´›æ ‘æœç´¢ (MCTS)</strong>ï¼šç»“åˆæ·±åº¦å­¦ä¹ å’Œæœç´¢</li>
<li><strong>è‡ªæˆ‘å¯¹å¼ˆ</strong>ï¼šé€šè¿‡è‡ªæˆ‘å¯¹å¼ˆæå‡ç­–ç•¥</li>
<li><strong>ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œ</strong>ï¼šåˆ†åˆ«é¢„æµ‹èµ°æ³•å’Œå±€é¢ä»·å€¼</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># ç®€åŒ–ç‰ˆAlphaGoç­–ç•¥</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SimpleAlphaGo</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, board_size<span style="color:#f92672">=</span><span style="color:#ae81ff">9</span>):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>board_size <span style="color:#f92672">=</span> board_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>policy_network <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_build_policy_network()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>value_network <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_build_value_network()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_build_policy_network</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;ç­–ç•¥ç½‘ç»œ&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        inputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Input(shape<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>board_size, self<span style="color:#f92672">.</span>board_size, <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;same&#39;</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(inputs)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;same&#39;</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Flatten()(x)
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(self<span style="color:#f92672">.</span>board_size <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>board_size, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model(inputs<span style="color:#f92672">=</span>inputs, outputs<span style="color:#f92672">=</span>outputs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_build_value_network</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;ä»·å€¼ç½‘ç»œ&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        inputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Input(shape<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>board_size, self<span style="color:#f92672">.</span>board_size, <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;same&#39;</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(inputs)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;same&#39;</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Flatten()(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">1</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;tanh&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model(inputs<span style="color:#f92672">=</span>inputs, outputs<span style="color:#f92672">=</span>outputs)
</span></span></code></pre></div><h3 id="æœºå™¨äººæ§åˆ¶">æœºå™¨äººæ§åˆ¶</h3>
<p><strong>è¿ç»­æ§åˆ¶ä»»åŠ¡</strong>ï¼š</p>
<ul>
<li><strong>DDPG (Deep Deterministic Policy Gradient)</strong>ï¼šå¤„ç†è¿ç»­åŠ¨ä½œç©ºé—´</li>
<li><strong>TD3 (Twin Delayed DDPG)</strong>ï¼šæ”¹è¿›çš„DDPGç®—æ³•</li>
<li><strong>SAC (Soft Actor-Critic)</strong>ï¼šæœ€å¤§åŒ–ç†µçš„å¼ºåŒ–å­¦ä¹ </li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DDPGAgent</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, state_size, action_size, action_low, action_high):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>state_size <span style="color:#f92672">=</span> state_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>action_size <span style="color:#f92672">=</span> action_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>action_low <span style="color:#f92672">=</span> action_low
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>action_high <span style="color:#f92672">=</span> action_high
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Actorç½‘ç»œï¼ˆç¡®å®šæ€§ç­–ç•¥ï¼‰</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>actor <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_build_actor()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Criticç½‘ç»œï¼ˆQå‡½æ•°ï¼‰</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>critic <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_build_critic()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># ç›®æ ‡ç½‘ç»œ</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>target_actor <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_build_actor()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>target_critic <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_build_critic()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># å¤åˆ¶æƒé‡åˆ°ç›®æ ‡ç½‘ç»œ</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>update_target_networks(<span style="color:#ae81ff">1.0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_build_actor</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;æ„å»ºActorç½‘ç»œ&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        inputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Input(shape<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>state_size,))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">400</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(inputs)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">300</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(self<span style="color:#f92672">.</span>action_size, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;tanh&#39;</span>)(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># ç¼©æ”¾åˆ°åŠ¨ä½œèŒƒå›´</span>
</span></span><span style="display:flex;"><span>        model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model(inputs<span style="color:#f92672">=</span>inputs, outputs<span style="color:#f92672">=</span>outputs)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_build_critic</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;æ„å»ºCriticç½‘ç»œ&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        state_inputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Input(shape<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>state_size,))
</span></span><span style="display:flex;"><span>        action_inputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Input(shape<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>action_size,))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># çŠ¶æ€è·¯å¾„</span>
</span></span><span style="display:flex;"><span>        state_out <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">400</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(state_inputs)
</span></span><span style="display:flex;"><span>        state_out <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">300</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(state_out)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># åŠ¨ä½œè·¯å¾„</span>
</span></span><span style="display:flex;"><span>        action_out <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">300</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(action_inputs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># åˆå¹¶</span>
</span></span><span style="display:flex;"><span>        merged <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Add()([state_out, action_out])
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">1</span>)(merged)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model(inputs<span style="color:#f92672">=</span>[state_inputs, action_inputs], outputs<span style="color:#f92672">=</span>outputs)
</span></span></code></pre></div><h3 id="è‡ªåŠ¨é©¾é©¶">è‡ªåŠ¨é©¾é©¶</h3>
<p><strong>è‡ªåŠ¨é©¾é©¶ä¸­çš„å¼ºåŒ–å­¦ä¹ </strong>ï¼š</p>
<ul>
<li><strong>è·¯å¾„è§„åˆ’</strong>ï¼šå­¦ä¹ æœ€ä¼˜é©¾é©¶è·¯å¾„</li>
<li><strong>è¡Œä¸ºå†³ç­–</strong>ï¼šåœ¨å¤æ‚äº¤é€šç¯å¢ƒä¸­å†³ç­–</li>
<li><strong>æ§åˆ¶ä¼˜åŒ–</strong>ï¼šä¼˜åŒ–è½¦è¾†æ§åˆ¶å‚æ•°</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AutonomousDrivingAgent</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, state_size, action_size):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>state_size <span style="color:#f92672">=</span> state_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>action_size <span style="color:#f92672">=</span> action_size
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># æ„ŸçŸ¥æ¨¡å—</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>perception_model <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_build_perception_model()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># å†³ç­–æ¨¡å—</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>decision_model <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_build_decision_model()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># æ§åˆ¶æ¨¡å—</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>control_model <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_build_control_model()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_build_perception_model</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;æ„ŸçŸ¥æ¨¡å—ï¼šå¤„ç†ä¼ æ„Ÿå™¨æ•°æ®&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        inputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Input(shape<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>state_size,))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">256</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(inputs)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># æ£€æµ‹å…¶ä»–è½¦è¾†ã€è¡Œäººã€äº¤é€šæ ‡å¿—ç­‰</span>
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;vehicles&#39;</span>: tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">10</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sigmoid&#39;</span>)(x),
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;pedestrians&#39;</span>: tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">5</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sigmoid&#39;</span>)(x),
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;traffic_lights&#39;</span>: tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">3</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model(inputs<span style="color:#f92672">=</span>inputs, outputs<span style="color:#f92672">=</span>outputs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_build_decision_model</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;å†³ç­–æ¨¡å—ï¼šåšå‡ºé©¾é©¶å†³ç­–&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        inputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Input(shape<span style="color:#f92672">=</span>(<span style="color:#ae81ff">128</span>,))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">64</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(inputs)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">32</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(self<span style="color:#f92672">.</span>action_size, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model(inputs<span style="color:#f92672">=</span>inputs, outputs<span style="color:#f92672">=</span>outputs)
</span></span></code></pre></div><h2 id="-è¯„ä¼°å’Œè°ƒè¯•">ğŸ“Š è¯„ä¼°å’Œè°ƒè¯•</h2>
<h3 id="è¯„ä¼°æŒ‡æ ‡">è¯„ä¼°æŒ‡æ ‡</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">evaluate_agent</span>(agent, env, episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    total_rewards <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    episode_lengths <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> episode <span style="color:#f92672">in</span> range(episodes):
</span></span><span style="display:flex;"><span>        state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>        total_reward <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        steps <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>            action <span style="color:#f92672">=</span> agent<span style="color:#f92672">.</span>choose_action(state)
</span></span><span style="display:flex;"><span>            next_state, reward, done, _ <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            total_reward <span style="color:#f92672">+=</span> reward
</span></span><span style="display:flex;"><span>            steps <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> done:
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            state <span style="color:#f92672">=</span> next_state
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        total_rewards<span style="color:#f92672">.</span>append(total_reward)
</span></span><span style="display:flex;"><span>        episode_lengths<span style="color:#f92672">.</span>append(steps)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;mean_reward&#39;</span>: np<span style="color:#f92672">.</span>mean(total_rewards),
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;std_reward&#39;</span>: np<span style="color:#f92672">.</span>std(total_rewards),
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;mean_length&#39;</span>: np<span style="color:#f92672">.</span>mean(episode_lengths),
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;success_rate&#39;</span>: np<span style="color:#f92672">.</span>mean([r <span style="color:#f92672">&gt;</span> threshold <span style="color:#66d9ef">for</span> r <span style="color:#f92672">in</span> total_rewards])
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ä½¿ç”¨è¯„ä¼°å‡½æ•°</span>
</span></span><span style="display:flex;"><span>results <span style="color:#f92672">=</span> evaluate_agent(agent, env, episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;å¹³å‡å¥–åŠ±: </span><span style="color:#e6db74">{</span>results[<span style="color:#e6db74">&#39;mean_reward&#39;</span>]<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> Â± </span><span style="color:#e6db74">{</span>results[<span style="color:#e6db74">&#39;std_reward&#39;</span>]<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;æˆåŠŸç‡: </span><span style="color:#e6db74">{</span>results[<span style="color:#e6db74">&#39;success_rate&#39;</span>]<span style="color:#e6db74">:</span><span style="color:#e6db74">.2%</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h3 id="è°ƒè¯•æŠ€å·§">è°ƒè¯•æŠ€å·§</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DebuggingAgent</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, agent):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>agent <span style="color:#f92672">=</span> agent
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>episode_rewards <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>q_values <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>gradients <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">debug_episode</span>(self, env):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;è°ƒè¯•å•ä¸ªepisode&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>        episode_reward <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        episode_q_values <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># è®°å½•Qå€¼</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> hasattr(self<span style="color:#f92672">.</span>agent, <span style="color:#e6db74">&#39;model&#39;</span>):
</span></span><span style="display:flex;"><span>                q_values <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>agent<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>predict(np<span style="color:#f92672">.</span>reshape(state, [<span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]), verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>                episode_q_values<span style="color:#f92672">.</span>append(np<span style="color:#f92672">.</span>max(q_values))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            action <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>agent<span style="color:#f92672">.</span>choose_action(state)
</span></span><span style="display:flex;"><span>            next_state, reward, done, _ <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            episode_reward <span style="color:#f92672">+=</span> reward
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> done:
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            state <span style="color:#f92672">=</span> next_state
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>episode_rewards<span style="color:#f92672">.</span>append(episode_reward)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>q_values<span style="color:#f92672">.</span>append(episode_q_values)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> episode_reward
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_debugging_info</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;ç»˜åˆ¶è°ƒè¯•ä¿¡æ¯&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># å¥–åŠ±æ›²çº¿</span>
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>plot(self<span style="color:#f92672">.</span>episode_rewards)
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Episode&#39;</span>)
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Reward&#39;</span>)
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Episode Rewards&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Qå€¼æ›²çº¿</span>
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i, q_values <span style="color:#f92672">in</span> enumerate(self<span style="color:#f92672">.</span>q_values[<span style="color:#f92672">-</span><span style="color:#ae81ff">10</span>:]):  <span style="color:#75715e"># æœ€è¿‘10ä¸ªepisode</span>
</span></span><span style="display:flex;"><span>            plt<span style="color:#f92672">.</span>plot(q_values, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Episode </span><span style="color:#e6db74">{</span>len(self<span style="color:#f92672">.</span>episode_rewards)<span style="color:#f92672">-</span><span style="color:#ae81ff">10</span><span style="color:#f92672">+</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Step&#39;</span>)
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Max Q-value&#39;</span>)
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Q-values&#39;</span>)
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>legend()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># å¥–åŠ±åˆ†å¸ƒ</span>
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>hist(self<span style="color:#f92672">.</span>episode_rewards, bins<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Reward&#39;</span>)
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Frequency&#39;</span>)
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Reward Distribution&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ä½¿ç”¨è°ƒè¯•å·¥å…·</span>
</span></span><span style="display:flex;"><span>debug_agent <span style="color:#f92672">=</span> DebuggingAgent(agent)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> episode <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">100</span>):
</span></span><span style="display:flex;"><span>    reward <span style="color:#f92672">=</span> debug_agent<span style="color:#f92672">.</span>debug_episode(env)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>debug_agent<span style="color:#f92672">.</span>plot_debugging_info()
</span></span></code></pre></div><h2 id="-å­¦ä¹ èµ„æº">ğŸ“š å­¦ä¹ èµ„æº</h2>
<h3 id="ç»å…¸è®ºæ–‡">ç»å…¸è®ºæ–‡</h3>
<ul>
<li><a href="https://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a> - DQN</li>
<li><a href="https://www.nature.com/articles/nature14236">Human-level control through deep reinforcement learning</a> - DQN Nature</li>
<li><a href="https://www.nature.com/articles/nature16961">Mastering the game of Go with deep neural networks</a> - AlphaGo</li>
<li><a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization Algorithms</a> - PPO</li>
</ul>
<h3 id="åœ¨çº¿èµ„æº">åœ¨çº¿èµ„æº</h3>
<ul>
<li><a href="https://gym.openai.com/">OpenAI Gym</a> - å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ</li>
<li><a href="https://stable-baselines.readthedocs.io/">Stable Baselines</a> - å¼ºåŒ–å­¦ä¹ ç®—æ³•åº“</li>
<li><a href="https://ray.readthedocs.io/en/latest/rllib.html">RLlib</a> - Rayä¸­çš„å¼ºåŒ–å­¦ä¹ åº“</li>
</ul>
<h3 id="å´æ©è¾¾è¯¾ç¨‹">å´æ©è¾¾è¯¾ç¨‹</h3>
<ul>
<li>æ·±åº¦å­¦ä¹ è¯¾ç¨‹ä¸­å…³äºå¼ºåŒ–å­¦ä¹ çš„éƒ¨åˆ†</li>
</ul>
<h2 id="-å®é™…é¡¹ç›®">ğŸ¯ å®é™…é¡¹ç›®</h2>
<h3 id="æ™ºèƒ½ä½“è®­ç»ƒæ¡†æ¶">æ™ºèƒ½ä½“è®­ç»ƒæ¡†æ¶</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">RLTrainingFramework</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, env_name, agent_class, config):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>env_name <span style="color:#f92672">=</span> env_name
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>agent_class <span style="color:#f92672">=</span> agent_class
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>config <span style="color:#f92672">=</span> config
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(env_name)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>agent <span style="color:#f92672">=</span> agent_class(self<span style="color:#f92672">.</span>env<span style="color:#f92672">.</span>observation_space<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>],
</span></span><span style="display:flex;"><span>                                self<span style="color:#f92672">.</span>env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>n, <span style="color:#f92672">**</span>config)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train</span>(self, num_episodes, eval_interval<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;è®­ç»ƒæ™ºèƒ½ä½“&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        best_reward <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>np<span style="color:#f92672">.</span>inf
</span></span><span style="display:flex;"><span>        rewards_history <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> episode <span style="color:#f92672">in</span> range(num_episodes):
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># è®­ç»ƒepisode</span>
</span></span><span style="display:flex;"><span>            episode_reward <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_train_episode()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            rewards_history<span style="color:#f92672">.</span>append(episode_reward)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># å®šæœŸè¯„ä¼°</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> episode <span style="color:#f92672">%</span> eval_interval <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>                eval_reward <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_evaluate_agent()
</span></span><span style="display:flex;"><span>                print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Episode </span><span style="color:#e6db74">{</span>episode<span style="color:#e6db74">}</span><span style="color:#e6db74">: Train Reward = </span><span style="color:#e6db74">{</span>episode_reward<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, &#34;</span>
</span></span><span style="display:flex;"><span>                      <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Eval Reward = </span><span style="color:#e6db74">{</span>eval_reward<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># ä¿å­˜æœ€ä½³æ¨¡å‹</span>
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">if</span> eval_reward <span style="color:#f92672">&gt;</span> best_reward:
</span></span><span style="display:flex;"><span>                    best_reward <span style="color:#f92672">=</span> eval_reward
</span></span><span style="display:flex;"><span>                    self<span style="color:#f92672">.</span>_save_model(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;best_model_</span><span style="color:#e6db74">{</span>episode<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> rewards_history
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_train_episode</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;è®­ç»ƒå•ä¸ªepisode&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        state <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>        total_reward <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
</span></span><span style="display:flex;"><span>            action <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>agent<span style="color:#f92672">.</span>choose_action(state)
</span></span><span style="display:flex;"><span>            next_state, reward, done, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>env<span style="color:#f92672">.</span>step(action)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>agent<span style="color:#f92672">.</span>learn(state, action, reward, next_state, done)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            state <span style="color:#f92672">=</span> next_state
</span></span><span style="display:flex;"><span>            total_reward <span style="color:#f92672">+=</span> reward
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> total_reward
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_evaluate_agent</span>(self, episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;è¯„ä¼°æ™ºèƒ½ä½“&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        total_reward <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(episodes):
</span></span><span style="display:flex;"><span>            state <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>            episode_reward <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>            done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
</span></span><span style="display:flex;"><span>                action <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>agent<span style="color:#f92672">.</span>choose_action(state)
</span></span><span style="display:flex;"><span>                next_state, reward, done, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>env<span style="color:#f92672">.</span>step(action)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>                state <span style="color:#f92672">=</span> next_state
</span></span><span style="display:flex;"><span>                episode_reward <span style="color:#f92672">+=</span> reward
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            total_reward <span style="color:#f92672">+=</span> episode_reward
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> total_reward <span style="color:#f92672">/</span> episodes
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_save_model</span>(self, filename):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;ä¿å­˜æ¨¡å‹&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># å®ç°æ¨¡å‹ä¿å­˜é€»è¾‘</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">pass</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ä½¿ç”¨è®­ç»ƒæ¡†æ¶</span>
</span></span><span style="display:flex;"><span>config <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;learning_rate&#39;</span>: <span style="color:#ae81ff">0.001</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;gamma&#39;</span>: <span style="color:#ae81ff">0.99</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;epsilon&#39;</span>: <span style="color:#ae81ff">1.0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;epsilon_decay&#39;</span>: <span style="color:#ae81ff">0.995</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>framework <span style="color:#f92672">=</span> RLTrainingFramework(<span style="color:#e6db74">&#39;CartPole-v1&#39;</span>, DQNAgent, config)
</span></span><span style="display:flex;"><span>rewards <span style="color:#f92672">=</span> framework<span style="color:#f92672">.</span>train(num_episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>)
</span></span></code></pre></div><hr>
<p><em>æœ€è¿‘æ›´æ–°: {{ .Lastmod.Format &ldquo;2006-01-02&rdquo; }}</em></p>

      
      <div class="post-separator">------------------------------------------------------------------------</div>
      
    </div>
    <div class="post-footer">
      
      <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
      <div class="busuanzi-container">
        <span id="busuanzi_container_page_pv">--æœ¬æ–‡æ€»é˜…è¯»é‡<span id="busuanzi_value_page_pv"></span>æ¬¡--</span>
      </div>
      
      
      <script src="https://utteranc.es/client.js"
        repo="makkichan947/makkichan947.github.io"
        issue-term="pathname"
        label="Utterances"
        theme="github-dark"
        crossorigin="anonymous"
        async>
      </script>
    </div>
  </article>

    </main>
  </body>
</html>
