<!doctype html>
<html lang="zh">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <title>Transformeræ¶æ„ // Yaku Makki</title>
    <link rel="shortcut icon" href="./avatar.jpg" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.151.2">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Yaku Makki" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="/css/main.min.422840b4454cb211f5b3dab61f082cdff508e88825173c31806835cb75529734.css" />
    

    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Transformeræ¶æ„">
  <meta name="twitter:description" content="Transformeræ¶æ„ Transformeræ˜¯2017å¹´ç”±Googleæå‡ºçš„é©å‘½æ€§æ¶æ„ï¼Œå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œå½»åº•æ”¹å˜äº†æ·±åº¦å­¦ä¹ é¢†åŸŸã€‚æœ¬ç« è¯¦ç»†ä»‹ç»Transformerçš„æ ¸å¿ƒæ¦‚å¿µå’Œå®ç°ç»†èŠ‚ã€‚
ğŸ¯ æ³¨æ„åŠ›æœºåˆ¶ åŸºæœ¬æ³¨æ„åŠ› æ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹åœ¨å¤„ç†åºåˆ—æ—¶åŠ¨æ€åœ°èšç„¦äºç›¸å…³éƒ¨åˆ†ï¼š
Scaled Dot-Product Attentionï¼š $$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$
å…¶ä¸­ï¼š
$Q$ï¼šæŸ¥è¯¢çŸ©é˜µ $K$ï¼šé”®çŸ©é˜µ $V$ï¼šå€¼çŸ©é˜µ $d_k$ï¼šé”®å‘é‡çš„ç»´åº¦ å¤šå¤´æ³¨æ„åŠ› å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å¹¶è¡Œè®¡ç®—å¤šä¸ªæ³¨æ„åŠ›ï¼š
å¤šå¤´è®¡ç®—ï¼š $$MultiHead(Q, K, V) = Concat(head_1, â€¦, head_h)W^O$$ $$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$$
ğŸ—ï¸ Transformeræ¶æ„ ç¼–ç å™¨ (Encoder) ç¼–ç å™¨å±‚ç»“æ„ï¼š
å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼š$MultiHeadAttention$ å‰é¦ˆç½‘ç»œï¼š$FFN(x) = max(0, xW_1 &#43; b_1)W_2 &#43; b_2$ æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ– è§£ç å™¨ (Decoder) è§£ç å™¨å±‚ç»“æ„ï¼š
æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼šé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ å¤šå¤´æ³¨æ„åŠ›ï¼šå…³æ³¨ç¼–ç å™¨è¾“å‡º å‰é¦ˆç½‘ç»œï¼šä¸ç¼–ç å™¨ç›¸åŒ æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ– ğŸ“ ä½ç½®ç¼–ç  é—®é¢˜ Transformeræ²¡æœ‰å¾ªç¯æˆ–å·ç§¯ç»“æ„ï¼Œæ— æ³•æ„ŸçŸ¥åºåˆ—ä½ç½®ã€‚
è§£å†³æ–¹æ¡ˆ æ­£å¼¦ä½ç½®ç¼–ç ï¼š $$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$$ $$PE_{(pos, 2i&#43;1)} = \cos(pos / 10000^{2i/d_{model}})$$">

    <meta property="og:url" content="http://localhost:1313/wiki/deep_learning/transformer/">
  <meta property="og:site_name" content="Yaku Makki">
  <meta property="og:title" content="Transformeræ¶æ„">
  <meta property="og:description" content="Transformeræ¶æ„ Transformeræ˜¯2017å¹´ç”±Googleæå‡ºçš„é©å‘½æ€§æ¶æ„ï¼Œå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œå½»åº•æ”¹å˜äº†æ·±åº¦å­¦ä¹ é¢†åŸŸã€‚æœ¬ç« è¯¦ç»†ä»‹ç»Transformerçš„æ ¸å¿ƒæ¦‚å¿µå’Œå®ç°ç»†èŠ‚ã€‚
ğŸ¯ æ³¨æ„åŠ›æœºåˆ¶ åŸºæœ¬æ³¨æ„åŠ› æ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹åœ¨å¤„ç†åºåˆ—æ—¶åŠ¨æ€åœ°èšç„¦äºç›¸å…³éƒ¨åˆ†ï¼š
Scaled Dot-Product Attentionï¼š $$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$
å…¶ä¸­ï¼š
$Q$ï¼šæŸ¥è¯¢çŸ©é˜µ $K$ï¼šé”®çŸ©é˜µ $V$ï¼šå€¼çŸ©é˜µ $d_k$ï¼šé”®å‘é‡çš„ç»´åº¦ å¤šå¤´æ³¨æ„åŠ› å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å¹¶è¡Œè®¡ç®—å¤šä¸ªæ³¨æ„åŠ›ï¼š
å¤šå¤´è®¡ç®—ï¼š $$MultiHead(Q, K, V) = Concat(head_1, â€¦, head_h)W^O$$ $$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$$
ğŸ—ï¸ Transformeræ¶æ„ ç¼–ç å™¨ (Encoder) ç¼–ç å™¨å±‚ç»“æ„ï¼š
å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼š$MultiHeadAttention$ å‰é¦ˆç½‘ç»œï¼š$FFN(x) = max(0, xW_1 &#43; b_1)W_2 &#43; b_2$ æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ– è§£ç å™¨ (Decoder) è§£ç å™¨å±‚ç»“æ„ï¼š
æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼šé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ å¤šå¤´æ³¨æ„åŠ›ï¼šå…³æ³¨ç¼–ç å™¨è¾“å‡º å‰é¦ˆç½‘ç»œï¼šä¸ç¼–ç å™¨ç›¸åŒ æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ– ğŸ“ ä½ç½®ç¼–ç  é—®é¢˜ Transformeræ²¡æœ‰å¾ªç¯æˆ–å·ç§¯ç»“æ„ï¼Œæ— æ³•æ„ŸçŸ¥åºåˆ—ä½ç½®ã€‚
è§£å†³æ–¹æ¡ˆ æ­£å¼¦ä½ç½®ç¼–ç ï¼š $$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$$ $$PE_{(pos, 2i&#43;1)} = \cos(pos / 10000^{2i/d_{model}})$$">
  <meta property="og:locale" content="zh">
  <meta property="og:type" content="article">
    <meta property="article:section" content="wiki">
    <meta property="article:published_time" content="2025-10-25T00:10:19+08:00">
    <meta property="article:modified_time" content="2025-10-25T00:10:19+08:00">


  </head>
  <body class="synthwave-bg">
    <header class="app-header glass-morphism neon-border">
      <a href="/"><img class="app-header-avatar" src="/avatar.jpg" alt="Yaku Makki" /></a>
      <span class="app-header-title gradient-text">Yaku Makki</span>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">é¦–é¡µ</a>
             | 
          
          <a class="app-header-menu-item" href="/post/">åšå®¢</a>
             | 
          
          <a class="app-header-menu-item" href="/wiki/">ç»´åŸº</a>
             | 
          
          <a class="app-header-menu-item" href="/tags/">æ ‡ç­¾</a>
             | 
          
          <a class="app-header-menu-item" href="/about/">å…³äº</a>
      </nav>
      <p>ä¸ªäººå°å·¢</p>
      <div class="app-header-social">
        
          <a href="https://github.com/makkichan947" target="_blank" rel="noreferrer noopener me" class="pulse-glow">
            <svg class="icon icon-brand-github" viewBox="0 0 24 24" fill="currentColor"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
          </a>
        
          <a href="mailto:yakumakki947@hotmail.com" target="_blank" rel="noreferrer noopener me" class="pulse-glow">
            <svg class="icon icon-mail" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>mail</title><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline></svg>
          </a>
        
          <a href="https://x.com/Makki_Yaku947" target="_blank" rel="noreferrer noopener me" class="pulse-glow">
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-link">
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
</svg>
          </a>
        
          <a href="https://www.twitch.tv/u/makkichan947" target="_blank" rel="noreferrer noopener me" class="pulse-glow">
            <svg class="icon icon-brand-twitch" viewBox="0 0 24 24" fill="currentColor"><title>Twitch</title><path d="M11.571 4.714h1.715v5.143H11.57zm4.715 0H18v5.143h-1.714zM6 0L1.714 4.286v15.428h5.143V24l4.286-4.286h3.428L22.286 12V0zm14.571 11.143l-3.428 3.428h-3.429l-3 3v-3H6.857V1.714h13.714Z"/></svg>
          </a>
        
      </div>
    </header>
    <main class="app-container glass-morphism glow-effect">
      <div class="floating-particles">
        <div class="particle" style="left: 10%; animation-delay: 0s;"></div>
        <div class="particle" style="left: 20%; animation-delay: 1s;"></div>
        <div class="particle" style="left: 30%; animation-delay: 2s;"></div>
        <div class="particle" style="left: 40%; animation-delay: 3s;"></div>
        <div class="particle" style="left: 50%; animation-delay: 4s;"></div>
        <div class="particle" style="left: 60%; animation-delay: 5s;"></div>
        <div class="particle" style="left: 70%; animation-delay: 6s;"></div>
        <div class="particle" style="left: 80%; animation-delay: 7s;"></div>
        <div class="particle" style="left: 90%; animation-delay: 8s;"></div>
      </div>
      
  <article class="post enhanced-card glow-effect">
    <header class="post-header">
      <h1 class ="post-title gradient-text">Transformeræ¶æ„</h1>
      <div class="post-meta">
        <div>
          <svg class="icon icon-calendar" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
          Oct 25, 2025
        </div>
        <div>
          <svg class="icon icon-clock" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>clock</title><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>
          4 min read
        </div>
      </div>
    </header>
    <div class="post-content">
      <h1 id="transformeræ¶æ„">Transformeræ¶æ„</h1>
<p>Transformeræ˜¯2017å¹´ç”±Googleæå‡ºçš„é©å‘½æ€§æ¶æ„ï¼Œå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œå½»åº•æ”¹å˜äº†æ·±åº¦å­¦ä¹ é¢†åŸŸã€‚æœ¬ç« è¯¦ç»†ä»‹ç»Transformerçš„æ ¸å¿ƒæ¦‚å¿µå’Œå®ç°ç»†èŠ‚ã€‚</p>
<h2 id="-æ³¨æ„åŠ›æœºåˆ¶">ğŸ¯ æ³¨æ„åŠ›æœºåˆ¶</h2>
<h3 id="åŸºæœ¬æ³¨æ„åŠ›">åŸºæœ¬æ³¨æ„åŠ›</h3>
<p>æ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹åœ¨å¤„ç†åºåˆ—æ—¶åŠ¨æ€åœ°èšç„¦äºç›¸å…³éƒ¨åˆ†ï¼š</p>
<p><strong>Scaled Dot-Product Attention</strong>ï¼š
$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$</p>
<p>å…¶ä¸­ï¼š</p>
<ul>
<li>$Q$ï¼šæŸ¥è¯¢çŸ©é˜µ</li>
<li>$K$ï¼šé”®çŸ©é˜µ</li>
<li>$V$ï¼šå€¼çŸ©é˜µ</li>
<li>$d_k$ï¼šé”®å‘é‡çš„ç»´åº¦</li>
</ul>
<h3 id="å¤šå¤´æ³¨æ„åŠ›">å¤šå¤´æ³¨æ„åŠ›</h3>
<p>å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å¹¶è¡Œè®¡ç®—å¤šä¸ªæ³¨æ„åŠ›ï¼š</p>
<p><strong>å¤šå¤´è®¡ç®—</strong>ï¼š
$$MultiHead(Q, K, V) = Concat(head_1, &hellip;, head_h)W^O$$
$$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$$</p>
<h2 id="-transformeræ¶æ„">ğŸ—ï¸ Transformeræ¶æ„</h2>
<h3 id="ç¼–ç å™¨-encoder">ç¼–ç å™¨ (Encoder)</h3>
<p><strong>ç¼–ç å™¨å±‚ç»“æ„</strong>ï¼š</p>
<ol>
<li><strong>å¤šå¤´è‡ªæ³¨æ„åŠ›</strong>ï¼š$MultiHeadAttention$</li>
<li><strong>å‰é¦ˆç½‘ç»œ</strong>ï¼š$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$</li>
<li><strong>æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–</strong></li>
</ol>
<h3 id="è§£ç å™¨-decoder">è§£ç å™¨ (Decoder)</h3>
<p><strong>è§£ç å™¨å±‚ç»“æ„</strong>ï¼š</p>
<ol>
<li><strong>æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›</strong>ï¼šé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯</li>
<li><strong>å¤šå¤´æ³¨æ„åŠ›</strong>ï¼šå…³æ³¨ç¼–ç å™¨è¾“å‡º</li>
<li><strong>å‰é¦ˆç½‘ç»œ</strong>ï¼šä¸ç¼–ç å™¨ç›¸åŒ</li>
<li><strong>æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–</strong></li>
</ol>
<h2 id="-ä½ç½®ç¼–ç ">ğŸ“ ä½ç½®ç¼–ç </h2>
<h3 id="é—®é¢˜">é—®é¢˜</h3>
<p>Transformeræ²¡æœ‰å¾ªç¯æˆ–å·ç§¯ç»“æ„ï¼Œæ— æ³•æ„ŸçŸ¥åºåˆ—ä½ç½®ã€‚</p>
<h3 id="è§£å†³æ–¹æ¡ˆ">è§£å†³æ–¹æ¡ˆ</h3>
<p><strong>æ­£å¼¦ä½ç½®ç¼–ç </strong>ï¼š
$$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$$
$$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})$$</p>
<p><strong>ç›¸å¯¹ä½ç½®ç¼–ç </strong>ï¼šè€ƒè™‘ç›¸å¯¹ä½ç½®å…³ç³»</p>
<h2 id="-è‡ªæ³¨æ„åŠ›æœºåˆ¶">ğŸ­ è‡ªæ³¨æ„åŠ›æœºåˆ¶</h2>
<h3 id="è‡ªæ³¨æ„åŠ›è®¡ç®—">è‡ªæ³¨æ„åŠ›è®¡ç®—</h3>
<p><strong>æŸ¥è¯¢ã€é”®ã€å€¼</strong>ï¼š</p>
<ul>
<li>$Q = XW^Q$</li>
<li>$K = XW^K$</li>
<li>$V = XW^V$</li>
</ul>
<p><strong>æ³¨æ„åŠ›åˆ†æ•°</strong>ï¼š
$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$</p>
<h3 id="æ©ç æœºåˆ¶">æ©ç æœºåˆ¶</h3>
<p><strong>å¡«å……æ©ç </strong>ï¼šå¿½ç•¥å¡«å……ä½ç½®
<strong>åºåˆ—æ©ç </strong>ï¼šé˜²æ­¢è§£ç å™¨çœ‹åˆ°æœªæ¥ä¿¡æ¯</p>
<h2 id="-ç¼–ç¨‹å®ç°">ğŸš€ ç¼–ç¨‹å®ç°</h2>
<h3 id="pytorchå®ç°">PyTorchå®ç°</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> math
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MultiHeadAttention</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, d_model, num_heads):
</span></span><span style="display:flex;"><span>        super(MultiHeadAttention, self)<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> d_model <span style="color:#f92672">%</span> num_heads <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>d_model <span style="color:#f92672">=</span> d_model
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>num_heads <span style="color:#f92672">=</span> num_heads
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>d_k <span style="color:#f92672">=</span> d_model <span style="color:#f92672">//</span> num_heads
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_q <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_model, d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_k <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_model, d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_v <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_model, d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_o <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_model, d_model)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">scaled_dot_product_attention</span>(self, Q, K, V, mask<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        scores <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(Q, K<span style="color:#f92672">.</span>transpose(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)) <span style="color:#f92672">/</span> math<span style="color:#f92672">.</span>sqrt(self<span style="color:#f92672">.</span>d_k)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> mask <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            scores <span style="color:#f92672">=</span> scores<span style="color:#f92672">.</span>masked_fill(mask <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1e9</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        attention <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>softmax(scores, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>matmul(attention, V)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, Q, K, V, mask<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        batch_size <span style="color:#f92672">=</span> Q<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># çº¿æ€§å˜æ¢</span>
</span></span><span style="display:flex;"><span>        Q <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W_q(Q)<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>num_heads, self<span style="color:#f92672">.</span>d_k)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        K <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W_k(K)<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>num_heads, self<span style="color:#f92672">.</span>d_k)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        V <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W_v(V)<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>num_heads, self<span style="color:#f92672">.</span>d_k)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># æ³¨æ„åŠ›è®¡ç®—</span>
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>scaled_dot_product_attention(Q, K, V, mask)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># æ‹¼æ¥å¤šå¤´è¾“å‡º</span>
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> output<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>d_model)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>W_o(output)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PositionwiseFeedForward</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, d_model, d_ff):
</span></span><span style="display:flex;"><span>        super(PositionwiseFeedForward, self)<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_model, d_ff)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_ff, d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>relu <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>linear2(self<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>linear1(x)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PositionalEncoding</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, d_model, max_len<span style="color:#f92672">=</span><span style="color:#ae81ff">5000</span>):
</span></span><span style="display:flex;"><span>        super(PositionalEncoding, self)<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        pe <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(max_len, d_model)
</span></span><span style="display:flex;"><span>        position <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, max_len, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        div_term <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>exp(torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, d_model, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>float() <span style="color:#f92672">*</span> (<span style="color:#f92672">-</span>math<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">10000.0</span>) <span style="color:#f92672">/</span> d_model))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        pe[:, <span style="color:#ae81ff">0</span>::<span style="color:#ae81ff">2</span>] <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sin(position <span style="color:#f92672">*</span> div_term)
</span></span><span style="display:flex;"><span>        pe[:, <span style="color:#ae81ff">1</span>::<span style="color:#ae81ff">2</span>] <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cos(position <span style="color:#f92672">*</span> div_term)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        pe <span style="color:#f92672">=</span> pe<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>register_buffer(<span style="color:#e6db74">&#39;pe&#39;</span>, pe)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>pe[:x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), :]
</span></span></code></pre></div><h3 id="å®Œæ•´transformerç¼–ç å™¨">å®Œæ•´Transformerç¼–ç å™¨</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TransformerEncoderLayer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, d_model, num_heads, d_ff, dropout<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>        super(TransformerEncoderLayer, self)<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>self_attn <span style="color:#f92672">=</span> MultiHeadAttention(d_model, num_heads)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>feed_forward <span style="color:#f92672">=</span> PositionwiseFeedForward(d_model, d_ff)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>norm1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>norm2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(d_model)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dropout1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(dropout)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dropout2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(dropout)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, mask<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># è‡ªæ³¨æ„åŠ›</span>
</span></span><span style="display:flex;"><span>        attn_output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>self_attn(x, x, x, mask)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>norm1(x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>dropout1(attn_output))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># å‰é¦ˆç½‘ç»œ</span>
</span></span><span style="display:flex;"><span>        ff_output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>feed_forward(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>norm2(x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>dropout2(ff_output))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TransformerEncoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len):
</span></span><span style="display:flex;"><span>        super(TransformerEncoder, self)<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>embedding <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(vocab_size, d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pos_encoding <span style="color:#f92672">=</span> PositionalEncoding(d_model, max_len)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>layers <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([
</span></span><span style="display:flex;"><span>            TransformerEncoderLayer(d_model, num_heads, d_ff)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(num_layers)
</span></span><span style="display:flex;"><span>        ])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>norm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(d_model)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, mask<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># è¯åµŒå…¥ + ä½ç½®ç¼–ç </span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding(x) <span style="color:#f92672">*</span> math<span style="color:#f92672">.</span>sqrt(self<span style="color:#f92672">.</span>embedding<span style="color:#f92672">.</span>embedding_dim)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pos_encoding(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># ç¼–ç å™¨å±‚</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>layers:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> layer(x, mask)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>norm(x)
</span></span></code></pre></div><h2 id="-æ³¨æ„åŠ›å¯è§†åŒ–">ğŸ¨ æ³¨æ„åŠ›å¯è§†åŒ–</h2>
<h3 id="æ³¨æ„åŠ›æƒé‡">æ³¨æ„åŠ›æƒé‡</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_attention</span>(attention_weights, tokens):
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">10</span>))
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>imshow(attention_weights, cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;viridis&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>xticks(range(len(tokens)), tokens, rotation<span style="color:#f92672">=</span><span style="color:#ae81ff">45</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>yticks(range(len(tokens)), tokens)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>colorbar()
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h3 id="å¤šå¤´æ³¨æ„åŠ›-1">å¤šå¤´æ³¨æ„åŠ›</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_multihead_attention</span>(attention_weights, tokens, num_heads):
</span></span><span style="display:flex;"><span>    fig, axes <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>, num_heads, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">15</span>, <span style="color:#ae81ff">5</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_heads):
</span></span><span style="display:flex;"><span>        axes[i]<span style="color:#f92672">.</span>imshow(attention_weights[i], cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;viridis&#39;</span>)
</span></span><span style="display:flex;"><span>        axes[i]<span style="color:#f92672">.</span>set_xticks(range(len(tokens)))
</span></span><span style="display:flex;"><span>        axes[i]<span style="color:#f92672">.</span>set_xticklabels(tokens, rotation<span style="color:#f92672">=</span><span style="color:#ae81ff">45</span>)
</span></span><span style="display:flex;"><span>        axes[i]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Head </span><span style="color:#e6db74">{</span>i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h2 id="-æ€§èƒ½ä¼˜åŒ–">ğŸ“Š æ€§èƒ½ä¼˜åŒ–</h2>
<h3 id="æ¨¡å‹å¹¶è¡Œ">æ¨¡å‹å¹¶è¡Œ</h3>
<ul>
<li><strong>å¼ é‡å¹¶è¡Œ</strong>ï¼šåœ¨å¤šä¸ªGPUä¸Šåˆ†å‰²æ¨¡å‹å‚æ•°</li>
<li><strong>æµæ°´çº¿å¹¶è¡Œ</strong>ï¼šä¸åŒGPUå¤„ç†ä¸åŒå±‚</li>
<li><strong>æ•°æ®å¹¶è¡Œ</strong>ï¼šæ¯ä¸ªGPUå¤„ç†ä¸åŒæ‰¹æ¬¡</li>
</ul>
<h3 id="å†…å­˜ä¼˜åŒ–">å†…å­˜ä¼˜åŒ–</h3>
<ul>
<li><strong>æ¢¯åº¦æ£€æŸ¥ç‚¹</strong>ï¼šå‡å°‘æ¿€æ´»å€¼å­˜å‚¨</li>
<li><strong>æ··åˆç²¾åº¦è®­ç»ƒ</strong>ï¼šä½¿ç”¨float16å‡å°‘å†…å­˜</li>
<li><strong>æ¨¡å‹åˆ†ç‰‡</strong>ï¼šæŒ‰éœ€åŠ è½½æ¨¡å‹å‚æ•°</li>
</ul>
<h2 id="-åº”ç”¨é¢†åŸŸ">ğŸ¯ åº”ç”¨é¢†åŸŸ</h2>
<h3 id="è‡ªç„¶è¯­è¨€å¤„ç†">è‡ªç„¶è¯­è¨€å¤„ç†</h3>
<ul>
<li><strong>æœºå™¨ç¿»è¯‘</strong>ï¼šGoogle Translate, DeepL</li>
<li><strong>æ–‡æœ¬ç”Ÿæˆ</strong>ï¼šGPTç³»åˆ—æ¨¡å‹</li>
<li><strong>æ–‡æœ¬æ‘˜è¦</strong>ï¼šè‡ªåŠ¨æ‘˜è¦ç”Ÿæˆ</li>
<li><strong>é—®ç­”ç³»ç»Ÿ</strong>ï¼šæ™ºèƒ½é—®ç­”æœºå™¨äºº</li>
</ul>
<h3 id="è®¡ç®—æœºè§†è§‰">è®¡ç®—æœºè§†è§‰</h3>
<ul>
<li><strong>å›¾åƒæè¿°</strong>ï¼šä¸ºå›¾åƒç”Ÿæˆæ–‡å­—æè¿°</li>
<li><strong>è§†è§‰é—®ç­”</strong>ï¼šåŸºäºå›¾åƒçš„é—®ç­”</li>
<li><strong>å›¾åƒç”Ÿæˆ</strong>ï¼šDALL-E, Stable Diffusion</li>
</ul>
<h3 id="è¯­éŸ³å¤„ç†">è¯­éŸ³å¤„ç†</h3>
<ul>
<li><strong>è¯­éŸ³è¯†åˆ«</strong>ï¼šç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ«</li>
<li><strong>è¯­éŸ³åˆæˆ</strong>ï¼šTTSç³»ç»Ÿ</li>
<li><strong>è¯­éŸ³ç¿»è¯‘</strong>ï¼šå®æ—¶è¯­éŸ³ç¿»è¯‘</li>
</ul>
<h3 id="å¤šæ¨¡æ€å­¦ä¹ ">å¤šæ¨¡æ€å­¦ä¹ </h3>
<ul>
<li><strong>å›¾åƒ-æ–‡æœ¬</strong>ï¼šCLIPæ¨¡å‹</li>
<li><strong>è§†é¢‘ç†è§£</strong>ï¼šè§†é¢‘é—®ç­”ç³»ç»Ÿ</li>
<li><strong>è·¨æ¨¡æ€ç”Ÿæˆ</strong>ï¼šæ–‡æœ¬ç”Ÿæˆå›¾åƒ</li>
</ul>
<h2 id="-å®ç”¨æŠ€å·§">ğŸ”§ å®ç”¨æŠ€å·§</h2>
<h3 id="å­¦ä¹ ç‡è°ƒåº¦">å­¦ä¹ ç‡è°ƒåº¦</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Warmup + ä½™å¼¦é€€ç«</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_lr_scheduler</span>(optimizer, warmup_steps, total_steps):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">lr_lambda</span>(step):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> step <span style="color:#f92672">&lt;</span> warmup_steps:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> step <span style="color:#f92672">/</span> warmup_steps
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> math<span style="color:#f92672">.</span>cos(math<span style="color:#f92672">.</span>pi <span style="color:#f92672">*</span> (step <span style="color:#f92672">-</span> warmup_steps) <span style="color:#f92672">/</span> (total_steps <span style="color:#f92672">-</span> warmup_steps)))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>lr_scheduler<span style="color:#f92672">.</span>LambdaLR(optimizer, lr_lambda)
</span></span></code></pre></div><h3 id="æ ‡ç­¾å¹³æ»‘">æ ‡ç­¾å¹³æ»‘</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LabelSmoothingLoss</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, size, padding_idx<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, smoothing<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>        super(LabelSmoothingLoss, self)<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>KLDivLoss(reduction<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sum&#39;</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>padding_idx <span style="color:#f92672">=</span> padding_idx
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>confidence <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> smoothing
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>smoothing <span style="color:#f92672">=</span> smoothing
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>size <span style="color:#f92672">=</span> size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>true_dist <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, target):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>) <span style="color:#f92672">==</span> self<span style="color:#f92672">.</span>size
</span></span><span style="display:flex;"><span>        true_dist <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>clone()
</span></span><span style="display:flex;"><span>        true_dist<span style="color:#f92672">.</span>fill_(self<span style="color:#f92672">.</span>smoothing <span style="color:#f92672">/</span> (self<span style="color:#f92672">.</span>size <span style="color:#f92672">-</span> <span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>        true_dist<span style="color:#f92672">.</span>scatter_(<span style="color:#ae81ff">1</span>, target<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>), self<span style="color:#f92672">.</span>confidence)
</span></span><span style="display:flex;"><span>        true_dist[:, self<span style="color:#f92672">.</span>padding_idx] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        mask <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nonzero(target<span style="color:#f92672">.</span>data <span style="color:#f92672">==</span> self<span style="color:#f92672">.</span>padding_idx)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> mask<span style="color:#f92672">.</span>dim() <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            true_dist<span style="color:#f92672">.</span>index_fill_(<span style="color:#ae81ff">0</span>, mask<span style="color:#f92672">.</span>squeeze(), <span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>true_dist <span style="color:#f92672">=</span> true_dist
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>criterion(x, true_dist)
</span></span></code></pre></div><h3 id="beam-search">Beam Search</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">beam_search</span>(model, src, beam_size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, max_len<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>):
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># ç¼–ç æºåºåˆ—</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        encoder_output <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>encode(src)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># åˆå§‹åŒ–</span>
</span></span><span style="display:flex;"><span>    candidates <span style="color:#f92672">=</span> [(<span style="color:#ae81ff">0</span>, [model<span style="color:#f92672">.</span>bos_idx])]
</span></span><span style="display:flex;"><span>    finished <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(max_len):
</span></span><span style="display:flex;"><span>        new_candidates <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> score, sequence <span style="color:#f92672">in</span> candidates:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> sequence[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">==</span> model<span style="color:#f92672">.</span>eos_idx:
</span></span><span style="display:flex;"><span>                finished<span style="color:#f92672">.</span>append((score, sequence))
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">continue</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># è§£ç </span>
</span></span><span style="display:flex;"><span>            tgt <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>LongTensor(sequence)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>                output <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>decode(encoder_output, tgt)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># å–top-ké¢„æµ‹</span>
</span></span><span style="display:flex;"><span>            probs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>softmax(output[:, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            top_probs, top_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>topk(probs, beam_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(beam_size):
</span></span><span style="display:flex;"><span>                new_score <span style="color:#f92672">=</span> score <span style="color:#f92672">+</span> torch<span style="color:#f92672">.</span>log(top_probs[<span style="color:#ae81ff">0</span>][i])
</span></span><span style="display:flex;"><span>                new_sequence <span style="color:#f92672">=</span> sequence <span style="color:#f92672">+</span> [top_indices[<span style="color:#ae81ff">0</span>][i]<span style="color:#f92672">.</span>item()]
</span></span><span style="display:flex;"><span>                new_candidates<span style="color:#f92672">.</span>append((new_score, new_sequence))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># ä¿ç•™top-kå€™é€‰</span>
</span></span><span style="display:flex;"><span>        candidates <span style="color:#f92672">=</span> sorted(new_candidates, key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">0</span>], reverse<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)[:beam_size]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># è¿”å›æœ€ä½³åºåˆ—</span>
</span></span><span style="display:flex;"><span>    best_sequence <span style="color:#f92672">=</span> max(finished <span style="color:#f92672">+</span> candidates, key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">0</span>])[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> best_sequence
</span></span></code></pre></div><h2 id="-å­¦ä¹ èµ„æº">ğŸ“š å­¦ä¹ èµ„æº</h2>
<h3 id="å´æ©è¾¾è¯¾ç¨‹">å´æ©è¾¾è¯¾ç¨‹</h3>
<ul>
<li><a href="https://www.coursera.org/learn/sequence-models">ç¬¬äº”å‘¨ï¼šåºåˆ—æ¨¡å‹</a></li>
</ul>
<h3 id="ç»å…¸è®ºæ–‡">ç»å…¸è®ºæ–‡</h3>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> - Vaswani et al. (2017)</li>
<li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers</a> - Devlin et al. (2018)</li>
<li><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> - GPT-3</li>
</ul>
<h3 id="åœ¨çº¿èµ„æº">åœ¨çº¿èµ„æº</h3>
<ul>
<li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
<li><a href="https://github.com/huggingface/transformers">Transformerä»£ç å®ç°</a></li>
<li><a href="https://transformer-viz.com/">æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–</a></li>
</ul>
<hr>
<p><em>æœ€è¿‘æ›´æ–°: {{ .Lastmod.Format &ldquo;2006-01-02&rdquo; }}</em></p>

      
      <div class="post-separator">------------------------------------------------------------------------</div>
      
    </div>
    <div class="post-footer">
      
      <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
      <div class="busuanzi-container">
        <span id="busuanzi_container_page_pv">--æœ¬æ–‡æ€»é˜…è¯»é‡<span id="busuanzi_value_page_pv"></span>æ¬¡--</span>
      </div>
      
      
      <script src="https://utteranc.es/client.js"
        repo="makkichan947/makkichan947.github.io"
        issue-term="pathname"
        label="Utterances"
        theme="github-dark"
        crossorigin="anonymous"
        async>
      </script>
    </div>
  </article>

    </main>
  </body>
</html>
