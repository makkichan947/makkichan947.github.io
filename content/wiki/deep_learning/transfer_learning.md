+++
date = '2025-10-20T10:15:00+08:00'
draft = false
title = 'è¿ç§»å­¦ä¹ '
comments = true
weight = 8
+++

# è¿ç§»å­¦ä¹ 

è¿ç§»å­¦ä¹ ï¼ˆTransfer Learningï¼‰æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„é‡è¦æŠ€æœ¯ï¼Œé€šè¿‡å°†åœ¨ä¸€ä¸ªä»»åŠ¡ä¸Šå­¦åˆ°çš„çŸ¥è¯†åº”ç”¨åˆ°å¦ä¸€ä¸ªç›¸å…³ä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶é—´å¹¶æé«˜æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«é€‚ç”¨äºæ•°æ®é‡æœ‰é™çš„åœºæ™¯ã€‚

## ğŸ¯ è¿ç§»å­¦ä¹ åŸºç¡€

### è¿ç§»å­¦ä¹ æ¦‚å¿µ

**å®šä¹‰**ï¼š
è¿ç§»å­¦ä¹ æ˜¯æŒ‡å°†ä»ä¸€ä¸ªä»»åŠ¡ï¼ˆæºä»»åŠ¡ï¼‰ä¸­å­¦åˆ°çš„çŸ¥è¯†åº”ç”¨åˆ°å¦ä¸€ä¸ªç›¸å…³ä»»åŠ¡ï¼ˆç›®æ ‡ä»»åŠ¡ï¼‰ä¸­çš„å­¦ä¹ è¿‡ç¨‹ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š
- **ç‰¹å¾å¤ç”¨**ï¼šåº•å±‚ç‰¹å¾ï¼ˆè¾¹ç¼˜ã€çº¹ç†ï¼‰åœ¨ä¸åŒä»»åŠ¡é—´æ˜¯é€šç”¨çš„
- **çŸ¥è¯†è¿ç§»**ï¼šé«˜å±‚è¯­ä¹‰ç‰¹å¾å¯ä»¥ä»æºä»»åŠ¡è¿ç§»åˆ°ç›®æ ‡ä»»åŠ¡
- **å¾®è°ƒ**ï¼šåœ¨ç›®æ ‡ä»»åŠ¡ä¸Šå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°

### è¿ç§»å­¦ä¹ ä¼˜åŠ¿

**ä¼ ç»Ÿæœºå™¨å­¦ä¹  vs è¿ç§»å­¦ä¹ **ï¼š
```python
# ä¼ ç»Ÿæ–¹æ³•ï¼šä»é›¶å¼€å§‹è®­ç»ƒ
model = create_model()
model.compile(optimizer='adam', loss='categorical_crossentropy')
model.fit(x_train, y_train, epochs=100)  # éœ€è¦å¤§é‡æ•°æ®å’Œæ—¶é—´

# è¿ç§»å­¦ä¹ ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False)
base_model.trainable = False  # å†»ç»“é¢„è®­ç»ƒæƒé‡

model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy')
model.fit(x_train, y_train, epochs=10)  # å¿«é€Ÿæ”¶æ•›
```

## ğŸ—ï¸ è¿ç§»å­¦ä¹ ç­–ç•¥

### ç‰¹å¾æå– (Feature Extraction)

**æ–¹æ³•**ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºç‰¹å¾æå–å™¨ï¼Œè®­ç»ƒæ–°çš„åˆ†ç±»å™¨ã€‚

```python
import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras import layers, models

def create_feature_extractor(base_model_name='resnet50', num_classes=10):
    """åˆ›å»ºç‰¹å¾æå–æ¨¡å‹"""

    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä¸åŒ…å«é¡¶éƒ¨åˆ†ç±»å±‚ï¼‰
    if base_model_name == 'resnet50':
        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    elif base_model_name == 'vgg16':
        base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    elif base_model_name == 'inceptionv3':
        base_model = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))

    # å†»ç»“é¢„è®­ç»ƒå±‚
    base_model.trainable = False

    # æ·»åŠ æ–°çš„åˆ†ç±»å±‚
    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation='softmax')
    ])

    return model

# åˆ›å»ºç‰¹å¾æå–æ¨¡å‹
model = create_feature_extractor('resnet50', num_classes=10)
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# è®­ç»ƒæ¨¡å‹
model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val))
```

### å¾®è°ƒ (Fine-tuning)

**æ–¹æ³•**ï¼šè§£å†»éƒ¨åˆ†é¢„è®­ç»ƒå±‚ï¼Œåœ¨ç›®æ ‡ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚

```python
def create_finetune_model(base_model_name='resnet50', num_classes=10, unfreeze_layers=10):
    """åˆ›å»ºå¾®è°ƒæ¨¡å‹"""

    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

    # å†»ç»“æ‰€æœ‰å±‚
    base_model.trainable = False

    # æ„å»ºæ¨¡å‹
    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation='softmax')
    ])

    # ç¼–è¯‘å¹¶è®­ç»ƒç‰¹å¾æå–å™¨
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))

    # è§£å†»é¡¶å±‚
    base_model.trainable = True

    # åªè®­ç»ƒæœ€åå‡ å±‚
    for layer in base_model.layers[:-unfreeze_layers]:
        layer.trainable = False

    # ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡è¿›è¡Œå¾®è°ƒ
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

# åˆ›å»ºå¾®è°ƒæ¨¡å‹
finetune_model = create_finetune_model('resnet50', num_classes=10, unfreeze_layers=20)
finetune_model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val))
```

### é¢†åŸŸè‡ªé€‚åº” (Domain Adaptation)

**æ–¹æ³•**ï¼šå‡å°‘æºé¢†åŸŸå’Œç›®æ ‡é¢†åŸŸä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ã€‚

```python
class DomainAdaptationModel(tf.keras.Model):
    def __init__(self, feature_extractor, task_classifier, domain_classifier):
        super(DomainAdaptationModel, self).__init__()
        self.feature_extractor = feature_extractor
        self.task_classifier = task_classifier
        self.domain_classifier = domain_classifier

    def call(self, inputs, lambda_adapt=1.0):
        # ç‰¹å¾æå–
        features = self.feature_extractor(inputs)

        # ä»»åŠ¡åˆ†ç±»
        task_outputs = self.task_classifier(features)

        # é¢†åŸŸåˆ†ç±»ï¼ˆæ¢¯åº¦åè½¬ï¼‰
        domain_outputs = self.domain_classifier(features)

        return task_outputs, domain_outputs, features

def create_domain_adaptation_model(input_shape, num_classes):
    """åˆ›å»ºé¢†åŸŸè‡ªé€‚åº”æ¨¡å‹"""

    # ç‰¹å¾æå–å™¨
    feature_extractor = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=input_shape),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Conv2D(64, 3, activation='relu'),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Conv2D(128, 3, activation='relu'),
        tf.keras.layers.GlobalAveragePooling2D()
    ])

    # ä»»åŠ¡åˆ†ç±»å™¨
    task_classifier = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])

    # é¢†åŸŸåˆ†ç±»å™¨
    domain_classifier = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])

    return DomainAdaptationModel(feature_extractor, task_classifier, domain_classifier)

# è‡ªå®šä¹‰æŸå¤±å‡½æ•°
def domain_adaptation_loss(y_true_task, y_pred_task, y_true_domain, y_pred_domain, lambda_adapt=1.0):
    """é¢†åŸŸè‡ªé€‚åº”æŸå¤±"""

    # ä»»åŠ¡æŸå¤±
    task_loss = tf.keras.losses.categorical_crossentropy(y_true_task, y_pred_task)

    # é¢†åŸŸæŸå¤±ï¼ˆåè½¬æ ‡ç­¾ï¼‰
    domain_labels = tf.ones_like(y_true_domain) - y_true_domain  # åè½¬æ ‡ç­¾
    domain_loss = tf.keras.losses.binary_crossentropy(domain_labels, y_pred_domain)

    # æ€»æŸå¤±
    total_loss = tf.reduce_mean(task_loss) + lambda_adapt * tf.reduce_mean(domain_loss)

    return total_loss
```

## ğŸ¨ é¢„è®­ç»ƒæ¨¡å‹åº”ç”¨

### ImageNeté¢„è®­ç»ƒæ¨¡å‹

```python
# å¸¸ç”¨ImageNeté¢„è®­ç»ƒæ¨¡å‹
models_dict = {
    'resnet50': tf.keras.applications.ResNet50,
    'resnet101': tf.keras.applications.ResNet101,
    'resnet152': tf.keras.applications.ResNet152,
    'vgg16': tf.keras.applications.VGG16,
    'vgg19': tf.keras.applications.VGG19,
    'inceptionv3': tf.keras.applications.InceptionV3,
    'xception': tf.keras.applications.Xception,
    'mobilenet': tf.keras.applications.MobileNet,
    'mobilenetv2': tf.keras.applications.MobileNetV2,
    'densenet121': tf.keras.applications.DenseNet121,
    'densenet169': tf.keras.applications.DenseNet169,
    'densenet201': tf.keras.applications.DenseNet201,
    'nasnetmobile': tf.keras.applications.NASNetMobile,
    'nasnetlarge': tf.keras.applications.NASNetLarge,
    'efficientnetb0': tf.keras.applications.EfficientNetB0,
    'efficientnetb1': tf.keras.applications.EfficientNetB1,
    'efficientnetb7': tf.keras.applications.EfficientNetB7
}

def load_pretrained_model(model_name, input_shape=(224, 224, 3), include_top=False):
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""

    if model_name not in models_dict:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")

    model_class = models_dict[model_name]

    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    base_model = model_class(
        weights='imagenet',
        include_top=include_top,
        input_shape=input_shape
    )

    return base_model

# ä½¿ç”¨ä¸åŒé¢„è®­ç»ƒæ¨¡å‹
for model_name in ['resnet50', 'vgg16', 'inceptionv3']:
    print(f"\n=== {model_name.upper()} ===")
    base_model = load_pretrained_model(model_name)
    print(f"è¾“å…¥å½¢çŠ¶: {base_model.input_shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {base_model.output_shape}")
    print(f"å‚æ•°æ•°é‡: {base_model.count_params()}")
```

### æ¨¡å‹æ€§èƒ½å¯¹æ¯”

```python
def compare_models(x_train, y_train, x_test, y_test, model_names=['resnet50', 'vgg16', 'mobilenet']):
    """å¯¹æ¯”ä¸åŒé¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½"""

    results = {}

    for model_name in model_names:
        print(f"\nè®­ç»ƒ {model_name}...")

        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        base_model = load_pretrained_model(model_name)
        base_model.trainable = False

        # æ„å»ºå®Œæ•´æ¨¡å‹
        model = tf.keras.Sequential([
            base_model,
            tf.keras.layers.GlobalAveragePooling2D(),
            tf.keras.layers.Dense(256, activation='relu'),
            tf.keras.layers.Dropout(0.5),
            tf.keras.layers.Dense(10, activation='softmax')
        ])

        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )

        # è®­ç»ƒæ¨¡å‹
        history = model.fit(
            x_train, y_train,
            epochs=10,
            validation_data=(x_test, y_test),
            verbose=1
        )

        # è¯„ä¼°æ¨¡å‹
        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)

        results[model_name] = {
            'test_accuracy': test_acc,
            'test_loss': test_loss,
            'parameters': model.count_params(),
            'history': history.history
        }

        print(f"{model_name} æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")

    return results

# å¯è§†åŒ–å¯¹æ¯”ç»“æœ
def plot_comparison(results):
    """å¯è§†åŒ–æ¨¡å‹å¯¹æ¯”ç»“æœ"""

    model_names = list(results.keys())
    accuracies = [results[name]['test_accuracy'] for name in model_names]
    params = [results[name]['parameters'] for name in model_names]

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # å‡†ç¡®ç‡å¯¹æ¯”
    ax1.bar(model_names, accuracies)
    ax1.set_ylabel('Test Accuracy')
    ax1.set_title('Model Accuracy Comparison')
    ax1.tick_params(axis='x', rotation=45)

    # å‚æ•°æ•°é‡å¯¹æ¯”
    ax2.bar(model_names, params)
    ax2.set_ylabel('Number of Parameters')
    ax2.set_title('Model Size Comparison')
    ax2.tick_params(axis='x', rotation=45)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(accuracies):
        ax1.text(i, v + 0.01, f'{v:.3f}', ha='center')

    for i, v in enumerate(params):
        ax2.text(i, v + 1000, f'{v/1e6:.1f}M', ha='center')

    plt.tight_layout()
    plt.show()

# è¿è¡Œå¯¹æ¯”
results = compare_models(x_train, y_train, x_test, y_test)
plot_comparison(results)
```

## ğŸ“ è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è¿ç§»å­¦ä¹ 

### BERTè¿ç§»å­¦ä¹ 

```python
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text

def create_bert_model(num_classes=2, max_len=128):
    """åˆ›å»ºåŸºäºBERTçš„è¿ç§»å­¦ä¹ æ¨¡å‹"""

    # åŠ è½½BERTé¢„å¤„ç†æ¨¡å‹
    bert_preprocess = hub.KerasLayer(
        "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3"
    )

    # åŠ è½½BERTç¼–ç å™¨
    bert_encoder = hub.KerasLayer(
        "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4",
        trainable=True
    )

    # æ„å»ºæ¨¡å‹
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
    preprocessed_text = bert_preprocess(text_input)
    outputs = bert_encoder(preprocessed_text)

    # ä½¿ç”¨BERTçš„CLS tokenè¾“å‡º
    cls_output = outputs['pooled_output']

    # æ·»åŠ åˆ†ç±»å±‚
    dropout = tf.keras.layers.Dropout(0.1)(cls_output)
    output = tf.keras.layers.Dense(num_classes, activation='softmax')(dropout)

    model = tf.keras.Model(inputs=text_input, outputs=output)

    return model

# åˆ›å»ºBERTæ¨¡å‹
bert_model = create_bert_model(num_classes=2)

# ç¼–è¯‘æ¨¡å‹
bert_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# è®­ç»ƒæ¨¡å‹
history = bert_model.fit(
    x_train_text, y_train,
    validation_data=(x_val_text, y_val),
    epochs=3,
    batch_size=16
)
```

### GPTæ¨¡å‹å¾®è°ƒ

```python
class GPTFineTuner:
    def __init__(self, model_name='gpt2', num_classes=None):
        self.model_name = model_name
        self.num_classes = num_classes

        # åŠ è½½é¢„è®­ç»ƒGPTæ¨¡å‹
        self.tokenizer = tf.keras.preprocessing.text.Tokenizer()
        self.model = self._load_gpt_model()

    def _load_gpt_model(self):
        """åŠ è½½GPTæ¨¡å‹"""
        if self.model_name == 'gpt2':
            # ä½¿ç”¨Hugging Faceçš„transformersåº“
            from transformers import TFGPT2Model, GPT2Tokenizer

            model = TFGPT2Model.from_pretrained('gpt2')
            self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

            return model
        else:
            # è‡ªå®šä¹‰GPTæ¨¡å‹
            return self._create_custom_gpt()

    def _create_custom_gpt(self):
        """åˆ›å»ºè‡ªå®šä¹‰GPTæ¨¡å‹"""
        # ç®€åŒ–çš„GPTæ¶æ„
        inputs = tf.keras.Input(shape=(None,), dtype=tf.int32)

        # è¯åµŒå…¥
        embedding = tf.keras.layers.Embedding(30000, 768)(inputs)

        # ä½ç½®ç¼–ç 
        position_embedding = tf.keras.layers.Embedding(1024, 768)(tf.range(1024))
        x = embedding + position_embedding

        # Transformerå—
        for _ in range(12):
            # å¤šå¤´æ³¨æ„åŠ›
            attn_output = tf.keras.layers.MultiHeadAttention(12, 768)(x, x)
            x = tf.keras.layers.LayerNormalization()(x + attn_output)

            # å‰é¦ˆç½‘ç»œ
            ffn_output = tf.keras.layers.Dense(3072, activation='gelu')(x)
            ffn_output = tf.keras.layers.Dense(768)(ffn_output)
            x = tf.keras.layers.LayerNormalization()(x + ffn_output)

        return tf.keras.Model(inputs=inputs, outputs=x)

    def fine_tune(self, texts, labels, epochs=3):
        """å¾®è°ƒGPTæ¨¡å‹"""

        # ç¼–ç æ–‡æœ¬
        encoded_texts = self.tokenizer(texts, padding=True, truncation=True, return_tensors='tf')

        # æ„å»ºå¾®è°ƒæ¨¡å‹
        inputs = tf.keras.Input(shape=(None,), dtype=tf.int32)
        gpt_outputs = self.model(inputs)

        # å–æœ€åä¸€ä¸ªtokençš„è¾“å‡º
        last_token_output = gpt_outputs[:, -1, :]

        # åˆ†ç±»å±‚
        if self.num_classes:
            outputs = tf.keras.layers.Dense(self.num_classes, activation='softmax')(last_token_output)
        else:
            outputs = tf.keras.layers.Dense(768, activation='linear')(last_token_output)

        fine_tuned_model = tf.keras.Model(inputs=inputs, outputs=outputs)

        # ç¼–è¯‘æ¨¡å‹
        fine_tuned_model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
            loss='categorical_crossentropy' if self.num_classes else 'mse',
            metrics=['accuracy'] if self.num_classes else []
        )

        # è®­ç»ƒæ¨¡å‹
        fine_tuned_model.fit(
            encoded_texts['input_ids'], labels,
            epochs=epochs,
            batch_size=8,
            validation_split=0.1
        )

        return fine_tuned_model
```

## ğŸ¯ å®é™…åº”ç”¨æ¡ˆä¾‹

### åŒ»å­¦å›¾åƒåˆ†ç±»

```python
def create_medical_image_model(base_model='resnet50', num_classes=2):
    """åˆ›å»ºåŒ»å­¦å›¾åƒåˆ†ç±»æ¨¡å‹"""

    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    if base_model == 'resnet50':
        base_model = tf.keras.applications.ResNet50(
            weights='imagenet',
            include_top=False,
            input_shape=(224, 224, 3)
        )
    elif base_model == 'densenet121':
        base_model = tf.keras.applications.DenseNet121(
            weights='imagenet',
            include_top=False,
            input_shape=(224, 224, 3)
        )

    # å†»ç»“é¢„è®­ç»ƒå±‚
    base_model.trainable = False

    # æ„å»ºæ¨¡å‹
    model = tf.keras.Sequential([
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])

    return model

def train_medical_model(x_train, y_train, x_val, y_val):
    """è®­ç»ƒåŒ»å­¦å›¾åƒæ¨¡å‹"""

    # æ•°æ®å¢å¼º
    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
        rotation_range=20,
        width_shift_range=0.1,
        height_shift_range=0.1,
        zoom_range=0.1,
        horizontal_flip=True,
        vertical_flip=True,
        brightness_range=[0.8, 1.2]
    )

    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator()

    # åˆ›å»ºæ¨¡å‹
    model = create_medical_image_model('densenet121', num_classes=2)

    # ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒåˆ†ç±»å™¨
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy', tf.keras.metrics.AUC()]
    )

    print("ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒåˆ†ç±»å™¨...")
    model.fit(
        train_datagen.flow(x_train, y_train, batch_size=32),
        epochs=20,
        validation_data=val_datagen.flow(x_val, y_val, batch_size=32),
        callbacks=[
            tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
            tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True)
        ]
    )

    # ç¬¬äºŒé˜¶æ®µï¼šå¾®è°ƒ
    print("ç¬¬äºŒé˜¶æ®µï¼šå¾®è°ƒæ¨¡å‹...")

    # è§£å†»éƒ¨åˆ†å±‚
    for layer in model.layers[0].layers[-20:]:
        layer.trainable = True

    # ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
        loss='categorical_crossentropy',
        metrics=['accuracy', tf.keras.metrics.AUC()]
    )

    model.fit(
        train_datagen.flow(x_train, y_train, batch_size=16),
        epochs=30,
        validation_data=val_datagen.flow(x_val, y_val, batch_size=16),
        callbacks=[
            tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
            tf.keras.callbacks.ModelCheckpoint('finetuned_model.h5', save_best_only=True)
        ]
    )

    return model

# ä½¿ç”¨ç¤ºä¾‹
model = train_medical_model(x_train_medical, y_train_medical, x_val_medical, y_val_medical)
```

### æ–‡æœ¬åˆ†ç±»

```python
def create_text_classification_model(model_type='bert', num_classes=5):
    """åˆ›å»ºæ–‡æœ¬åˆ†ç±»æ¨¡å‹"""

    if model_type == 'bert':
        # ä½¿ç”¨BERT
        bert_preprocess = hub.KerasLayer(
            "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3"
        )
        bert_encoder = hub.KerasLayer(
            "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4",
            trainable=True
        )

        text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
        preprocessed_text = bert_preprocess(text_input)
        outputs = bert_encoder(preprocessed_text)
        cls_output = outputs['pooled_output']

    elif model_type == 'universal_sentence_encoder':
        # ä½¿ç”¨Universal Sentence Encoder
        use_layer = hub.KerasLayer(
            "https://tfhub.dev/google/universal-sentence-encoder/4",
            trainable=False
        )

        text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
        cls_output = use_layer(text_input)

    # åˆ†ç±»å±‚
    dropout = tf.keras.layers.Dropout(0.1)(cls_output)
    output = tf.keras.layers.Dense(num_classes, activation='softmax')(dropout)

    model = tf.keras.Model(inputs=text_input, outputs=output)

    return model

def train_text_model(texts, labels, model_type='bert'):
    """è®­ç»ƒæ–‡æœ¬åˆ†ç±»æ¨¡å‹"""

    # ç¼–ç æ ‡ç­¾
    label_encoder = tf.keras.utils.to_categorical if len(np.unique(labels)) > 2 else lambda x: x
    y_encoded = label_encoder(labels)

    # åˆ’åˆ†æ•°æ®é›†
    x_train, x_val, y_train, y_val = train_test_split(
        texts, y_encoded, test_size=0.2, random_state=42
    )

    # åˆ›å»ºæ¨¡å‹
    model = create_text_classification_model(model_type, num_classes=len(np.unique(labels)))

    # ç¼–è¯‘æ¨¡å‹
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5 if model_type == 'bert' else 0.001),
        loss='categorical_crossentropy' if len(np.unique(labels)) > 2 else 'binary_crossentropy',
        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]
    )

    # è®­ç»ƒæ¨¡å‹
    history = model.fit(
        x_train, y_train,
        validation_data=(x_val, y_val),
        epochs=5 if model_type == 'bert' else 20,
        batch_size=16 if model_type == 'bert' else 32,
        callbacks=[
            tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),
            tf.keras.callbacks.ModelCheckpoint('best_text_model.h5', save_best_only=True)
        ]
    )

    return model, history

# ä½¿ç”¨ç¤ºä¾‹
model, history = train_text_model(texts, labels, model_type='bert')
```

## ğŸ“Š æ¨¡å‹è¯„ä¼°å’Œä¼˜åŒ–

### è¯„ä¼°è¿ç§»å­¦ä¹ æ¨¡å‹

```python
def evaluate_transfer_model(model, x_test, y_test, class_names=None):
    """è¯„ä¼°è¿ç§»å­¦ä¹ æ¨¡å‹"""

    # é¢„æµ‹
    y_pred = model.predict(x_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true = np.argmax(y_test, axis=1)

    # åˆ†ç±»æŠ¥å‘Š
    print("åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_true, y_pred_classes, target_names=class_names))

    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred_classes)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.show()

    # ROCæ›²çº¿ï¼ˆäºŒåˆ†ç±»ï¼‰
    if y_pred.shape[1] == 2:
        from sklearn.metrics import roc_curve, auc

        fpr, tpr, _ = roc_curve(y_true, y_pred[:, 1])
        roc_auc = auc(fpr, tpr)

        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2,
                label=f'ROC curve (AUC = {roc_auc:.2f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver Operating Characteristic')
        plt.legend(loc="lower right")
        plt.show()

    return {
        'accuracy': np.mean(y_pred_classes == y_true),
        'confusion_matrix': cm,
        'classification_report': classification_report(y_true, y_pred_classes, output_dict=True)
    }

# ä½¿ç”¨è¯„ä¼°å‡½æ•°
results = evaluate_transfer_model(model, x_test, y_test, class_names=['class1', 'class2', 'class3'])
```

### è¶…å‚æ•°ä¼˜åŒ–

```python
def optimize_transfer_learning(x_train, y_train, x_val, y_val):
    """è¿ç§»å­¦ä¹ è¶…å‚æ•°ä¼˜åŒ–"""

    def objective(trial):
        # è¶…å‚æ•°æœç´¢ç©ºé—´
        base_model_name = trial.suggest_categorical('base_model', ['resnet50', 'vgg16', 'mobilenetv2'])
        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)
        dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)
        dense_units = trial.suggest_categorical('dense_units', [128, 256, 512])
        unfreeze_layers = trial.suggest_int('unfreeze_layers', 0, 50)

        # åˆ›å»ºæ¨¡å‹
        base_model = load_pretrained_model(base_model_name)
        base_model.trainable = False

        model = tf.keras.Sequential([
            base_model,
            tf.keras.layers.GlobalAveragePooling2D(),
            tf.keras.layers.Dense(dense_units, activation='relu'),
            tf.keras.layers.Dropout(dropout_rate),
            tf.keras.layers.Dense(10, activation='softmax')
        ])

        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )

        # è®­ç»ƒæ¨¡å‹
        model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val), verbose=0)

        # å¾®è°ƒ
        if unfreeze_layers > 0:
            base_model.trainable = True
            for layer in base_model.layers[:-unfreeze_layers]:
                layer.trainable = False

            model.compile(
                optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate * 0.1),
                loss='categorical_crossentropy',
                metrics=['accuracy']
            )

            model.fit(x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=0)

        # è¯„ä¼°
        _, accuracy = model.evaluate(x_val, y_val, verbose=0)
        return accuracy

    # ä½¿ç”¨Optunaè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
    import optuna

    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=20)

    print(f"æœ€ä½³å‚æ•°: {study.best_params}")
    print(f"æœ€ä½³å‡†ç¡®ç‡: {study.best_value}")

    return study.best_params

# è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–
best_params = optimize_transfer_learning(x_train, y_train, x_val, y_val)
```

## ğŸ“š å­¦ä¹ èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [TensorFlowè¿ç§»å­¦ä¹ æŒ‡å—](https://www.tensorflow.org/guide/keras/transfer_learning)
- [Kerasé¢„è®­ç»ƒæ¨¡å‹](https://keras.io/applications/)
- [TensorFlow Hub](https://tfhub.dev/)

### ç»å…¸è®ºæ–‡
- [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) - AlexNet
- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556) - VGG
- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - ResNet
- [Going deeper with convolutions](https://arxiv.org/abs/1409.4842) - GoogLeNet

### å´æ©è¾¾è¯¾ç¨‹
- æ·±åº¦å­¦ä¹ è¯¾ç¨‹ä¸­å…³äºè¿ç§»å­¦ä¹ çš„éƒ¨åˆ†

## ğŸ¯ æœ€ä½³å®è·µ

### æ•°æ®å‡†å¤‡
```python
def prepare_data_for_transfer_learning(images, labels, validation_split=0.2):
    """ä¸ºè¿ç§»å­¦ä¹ å‡†å¤‡æ•°æ®"""

    # è°ƒæ•´å›¾åƒå¤§å°
    target_size = (224, 224)  # å¤§å¤šæ•°é¢„è®­ç»ƒæ¨¡å‹çš„è¾“å…¥å¤§å°
    resized_images = tf.image.resize(images, target_size)

    # å›¾åƒé¢„å¤„ç†ï¼ˆImageNetæ ‡å‡†åŒ–ï¼‰
    preprocessed_images = tf.keras.applications.resnet50.preprocess_input(resized_images)

    # ç¼–ç æ ‡ç­¾
    if len(np.unique(labels)) > 2:
        encoded_labels = tf.keras.utils.to_categorical(labels)
    else:
        encoded_labels = labels

    # åˆ’åˆ†æ•°æ®é›†
    num_val_samples = int(len(images) * validation_split)
    x_train = preprocessed_images[:-num_val_samples]
    y_train = encoded_labels[:-num_val_samples]
    x_val = preprocessed_images[-num_val_samples:]
    y_val = encoded_labels[-num_val_samples:]

    return x_train, y_train, x_val, y_val

# ä½¿ç”¨æ•°æ®å‡†å¤‡å‡½æ•°
x_train, y_train, x_val, y_val = prepare_data_for_transfer_learning(images, labels)
```

### æ¨¡å‹é€‰æ‹©æŒ‡å—
```python
def select_best_model(dataset_size, num_classes, time_budget):
    """æ ¹æ®æ•°æ®é›†å¤§å°å’Œæ—¶é—´é¢„ç®—é€‰æ‹©æœ€ä½³æ¨¡å‹"""

    model_recommendations = {
        'small_dataset': {
            'models': ['mobilenetv2', 'efficientnetb0', 'resnet50'],
            'strategy': 'feature_extraction',
            'epochs': 20
        },
        'medium_dataset': {
            'models': ['resnet50', 'densenet121', 'efficientnetb3'],
            'strategy': 'fine_tuning',
            'epochs': 50
        },
        'large_dataset': {
            'models': ['resnet152', 'densenet201', 'efficientnetb7'],
            'strategy': 'full_training',
            'epochs': 100
        }
    }

    # æ ¹æ®æ•°æ®é›†å¤§å°é€‰æ‹©æ¨è
    if dataset_size < 1000:
        recommendation = model_recommendations['small_dataset']
    elif dataset_size < 10000:
        recommendation = model_recommendations['medium_dataset']
    else:
        recommendation = model_recommendations['large_dataset']

    # æ ¹æ®æ—¶é—´é¢„ç®—è°ƒæ•´
    if time_budget < 60:  # 1å°æ—¶
        recommendation['models'] = recommendation['models'][:1]
        recommendation['epochs'] = min(recommendation['epochs'], 10)

    return recommendation

# ä½¿ç”¨æ¨¡å‹é€‰æ‹©æŒ‡å—
recommendation = select_best_model(dataset_size=5000, num_classes=10, time_budget=120)
print(f"æ¨èæ¨¡å‹: {recommendation['models']}")
print(f"æ¨èç­–ç•¥: {recommendation['strategy']}")
print(f"æ¨èè®­ç»ƒè½®æ•°: {recommendation['epochs']}")
```

---

*æœ€è¿‘æ›´æ–°: {{ .Lastmod.Format "2006-01-02" }}*