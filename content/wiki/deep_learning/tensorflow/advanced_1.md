+++
date = '2025-10-24T21:58:49+08:00'
draft = false
title = 'TensorFlowé«˜çº§ç‰¹æ€§'
comments = true
weight = 2
+++

# TensorFlowé«˜çº§ç‰¹æ€§

æœ¬ç« æ·±å…¥ä»‹ç»TensorFlowçš„é«˜çº§ç‰¹æ€§ï¼ŒåŒ…æ‹¬è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€åˆ†å¸ƒå¼è®­ç»ƒã€æ¨¡å‹ä¼˜åŒ–ã€æ€§èƒ½è°ƒä¼˜ç­‰å†…å®¹ï¼Œå¸®åŠ©ä½ æ„å»ºæ›´å¤æ‚å’Œé«˜æ•ˆçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

## ğŸ¯ è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯

### åŸºç¡€è‡ªå®šä¹‰è®­ç»ƒ
```python
import tensorflow as tf
import numpy as np

# å‡†å¤‡æ•°æ®
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.reshape(-1, 784) / 255.0
x_test = x_test.reshape(-1, 784) / 255.0
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# åˆ›å»ºæ•°æ®é›†
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(1000).batch(32)

# æ„å»ºæ¨¡å‹
class CustomModel(tf.keras.Model):
    def __init__(self):
        super(CustomModel, self).__init__()
        self.dense1 = tf.keras.layers.Dense(128, activation='relu')
        self.dense2 = tf.keras.layers.Dense(64, activation='relu')
        self.dense3 = tf.keras.layers.Dense(10, activation='softmax')

    def call(self, inputs):
        x = self.dense1(inputs)
        x = self.dense2(x)
        return self.dense3(x)

model = CustomModel()

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
loss_fn = tf.keras.losses.CategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam()

# è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯
@tf.function  # ç¼–è¯‘åŠ é€Ÿ
def train_step(x, y):
    with tf.GradientTape() as tape:
        predictions = model(x)
        loss = loss_fn(y, predictions)

    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    return loss

# è®­ç»ƒ
epochs = 5
for epoch in range(epochs):
    total_loss = 0
    num_batches = 0

    for x_batch, y_batch in train_dataset:
        loss = train_step(x_batch, y_batch)
        total_loss += loss
        num_batches += 1

    avg_loss = total_loss / num_batches
    print(f"Epoch {epoch + 1}, Loss: {avg_loss:.4f}")
```

### é«˜çº§è‡ªå®šä¹‰è®­ç»ƒ
```python
class AdvancedTrainer:
    def __init__(self, model, optimizer, loss_fn, metrics=None):
        self.model = model
        self.optimizer = optimizer
        self.loss_fn = loss_fn
        self.metrics = metrics or []

    @tf.function
    def train_step(self, x, y):
        with tf.GradientTape() as tape:
            predictions = self.model(x, training=True)
            loss = self.loss_fn(y, predictions)

            # æ·»åŠ æ­£åˆ™åŒ–æŸå¤±
            for var in self.model.trainable_variables:
                loss += tf.nn.l2_loss(var) * 1e-4

        gradients = tape.gradient(loss, self.model.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))

        # è®¡ç®—æŒ‡æ ‡
        for metric in self.metrics:
            metric.update_state(y, predictions)

        return loss

    def train_epoch(self, dataset):
        total_loss = 0
        num_batches = 0

        for x_batch, y_batch in dataset:
            loss = self.train_step(x_batch, y_batch)
            total_loss += loss
            num_batches += 1

        # é‡ç½®æŒ‡æ ‡çŠ¶æ€
        for metric in self.metrics:
            metric.reset_states()

        return total_loss / num_batches

# ä½¿ç”¨é«˜çº§è®­ç»ƒå™¨
model = CustomModel()
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
loss_fn = tf.keras.losses.CategoricalCrossentropy()

# å®šä¹‰æŒ‡æ ‡
train_acc = tf.keras.metrics.CategoricalAccuracy()
val_acc = tf.keras.metrics.CategoricalAccuracy()

trainer = AdvancedTrainer(model, optimizer, loss_fn, [train_acc])

# è®­ç»ƒå¤šä¸ªepoch
for epoch in range(10):
    loss = trainer.train_epoch(train_dataset)
    print(f"Epoch {epoch + 1}, Loss: {loss:.4f}, Accuracy: {train_acc.result():.4f}")
```

## ğŸ“Š åˆ†å¸ƒå¼è®­ç»ƒ

### å¤šGPUè®­ç»ƒ
```python
# ç­–ç•¥1: MirroredStrategy
strategy = tf.distribute.MirroredStrategy()
print(f"GPUæ•°é‡: {strategy.num_replicas_in_sync}")

with strategy.scope():
    # åœ¨ç­–ç•¥èŒƒå›´å†…æ„å»ºæ¨¡å‹
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])

    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

# è®­ç»ƒ
model.fit(x_train, y_train, epochs=5, batch_size=64)
```

### TPUè®­ç»ƒ
```python
# TPUè®­ç»ƒ
resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)

# åˆ›å»ºTPUç­–ç•¥
strategy = tf.distribute.experimental.TPUStrategy(resolver)

with strategy.scope():
    model = create_model()  # åœ¨TPUç­–ç•¥èŒƒå›´å†…åˆ›å»ºæ¨¡å‹
    model.compile(...)

# è®­ç»ƒ
model.fit(train_dataset, epochs=10)
```

### è‡ªå®šä¹‰åˆ†å¸ƒå¼è®­ç»ƒ
```python
# è‡ªå®šä¹‰åˆ†å¸ƒå¼è®­ç»ƒå¾ªç¯
@tf.function
def distributed_train_step(dataset_inputs):
    def train_step_fn(inputs):
        x, y = inputs
        with tf.GradientTape() as tape:
            predictions = model(x, training=True)
            loss = loss_fn(y, predictions)

        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))
        return loss

    # åœ¨æ‰€æœ‰å‰¯æœ¬ä¸Šè¿è¡Œ
    per_replica_losses = strategy.run(train_step_fn, args=(dataset_inputs,))
    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)

# åˆ†å¸ƒå¼è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    total_loss = 0
    num_batches = 0

    for x_batch, y_batch in distributed_dataset:
        loss = distributed_train_step((x_batch, y_batch))
        total_loss += loss
        num_batches += 1

    avg_loss = total_loss / num_batches
    print(f"Epoch {epoch}, Loss: {avg_loss}")
```

## ğŸš€ æ¨¡å‹ä¼˜åŒ–

### æ··åˆç²¾åº¦è®­ç»ƒ
```python
from tensorflow.keras.mixed_precision import experimental as mixed_precision

# è®¾ç½®æ··åˆç²¾åº¦ç­–ç•¥
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_policy(policy)

# æ„å»ºæ¨¡å‹ï¼ˆè‡ªåŠ¨ä½¿ç”¨æ··åˆç²¾åº¦ï¼‰
with strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
        tf.keras.layers.Dense(10, activation='softmax')
    ])

    # ä½¿ç”¨æ··åˆç²¾åº¦ä¼˜åŒ–å™¨
    optimizer = mixed_precision.LossScaleOptimizer(
        tf.keras.optimizers.Adam(), loss_scale='dynamic'
    )

    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# è®­ç»ƒ
model.fit(x_train, y_train, epochs=5)
```

### æ¨¡å‹é‡åŒ–
```python
import tensorflow_model_optimization as tfmot

# åº”ç”¨é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
quantize_model = tfmot.quantization.keras.quantize_model
q_aware_model = quantize_model(model)

# ç¼–è¯‘é‡åŒ–æ¨¡å‹
q_aware_model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
q_aware_model.fit(x_train, y_train, epochs=5)

# è½¬æ¢ä¸ºå®Œå…¨é‡åŒ–æ¨¡å‹
converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
quantized_tflite_model = converter.convert()
```

### æ¨¡å‹å‰ªæ
```python
# åº”ç”¨æ¨¡å‹å‰ªæ
prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude

# å®šä¹‰å‰ªæå‚æ•°
pruning_params = {
    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(
        initial_sparsity=0.0,
        final_sparsity=0.5,
        begin_step=0,
        end_step=1000
    )
}

# åˆ›å»ºå‰ªææ¨¡å‹
pruned_model = prune_low_magnitude(model, **pruning_params)

# ç¼–è¯‘å‰ªææ¨¡å‹
pruned_model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# å‰ªæè®­ç»ƒ
pruned_model.fit(x_train, y_train, epochs=10)

# å‰¥ç¦»å‰ªæç»“æ„
stripped_pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

## ğŸ¨ é«˜çº§å±‚å’Œæ“ä½œ

### è‡ªå®šä¹‰å±‚
```python
class AttentionLayer(tf.keras.layers.Layer):
    def __init__(self, units, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)
        self.units = units
        self.W = None
        self.b = None
        self.V = None

    def build(self, input_shape):
        self.W = self.add_weight(
            name='W',
            shape=(input_shape[-1], self.units),
            initializer='glorot_uniform',
            trainable=True
        )
        self.b = self.add_weight(
            name='b',
            shape=(self.units,),
            initializer='zeros',
            trainable=True
        )
        self.V = self.add_weight(
            name='V',
            shape=(self.units, 1),
            initializer='glorot_uniform',
            trainable=True
        )

    def call(self, inputs):
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        score = tf.nn.tanh(tf.matmul(inputs, self.W) + self.b)
        attention_weights = tf.nn.softmax(tf.matmul(score, self.V), axis=1)

        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        context_vector = attention_weights * inputs
        context_vector = tf.reduce_sum(context_vector, axis=1)

        return context_vector, attention_weights

# ä½¿ç”¨æ³¨æ„åŠ›å±‚
inputs = tf.keras.Input(shape=(10, 64))  # (batch_size, seq_len, features)
context, attention = AttentionLayer(32)(inputs)
outputs = tf.keras.layers.Dense(10, activation='softmax')(context)

model = tf.keras.Model(inputs=inputs, outputs=outputs)
```

### è‡ªå®šä¹‰æŸå¤±å‡½æ•°
```python
class FocalLoss(tf.keras.losses.Loss):
    def __init__(self, alpha=1.0, gamma=2.0, **kwargs):
        super(FocalLoss, self).__init__(**kwargs)
        self.alpha = alpha
        self.gamma = gamma

    def call(self, y_true, y_pred):
        # å°†æ ‡ç­¾è½¬æ¢ä¸ºone-hotç¼–ç 
        y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=tf.shape(y_pred)[-1])

        # è®¡ç®—äº¤å‰ç†µ
        ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)

        # è®¡ç®—è°ƒåˆ¶å› å­
        pt = tf.exp(-ce)
        focal_modulation = self.alpha * tf.pow((1 - pt), self.gamma)

        return focal_modulation * ce

# ä½¿ç”¨ç„¦ç‚¹æŸå¤±
model.compile(
    optimizer='adam',
    loss=FocalLoss(alpha=1.0, gamma=2.0),
    metrics=['accuracy']
)
```

### è‡ªå®šä¹‰æŒ‡æ ‡
```python
class F1Score(tf.keras.metrics.Metric):
    def __init__(self, name='f1_score', **kwargs):
        super(F1Score, self).__init__(name=name, **kwargs)
        self.true_positives = self.add_weight(name='tp', initializer='zeros')
        self.false_positives = self.add_weight(name='fp', initializer='zeros')
        self.false_negatives = self.add_weight(name='fn', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        # è½¬æ¢ä¸ºäºŒåˆ†ç±»
        y_pred = tf.cast(tf.greater(y_pred, 0.5), tf.float32)
        y_true = tf.cast(y_true, tf.float32)

        # è®¡ç®—TP, FP, FN
        tp = tf.reduce_sum(y_true * y_pred)
        fp = tf.reduce_sum((1 - y_true) * y_pred)
        fn = tf.reduce_sum(y_true * (1 - y_pred))

        self.true_positives.assign_add(tp)
        self.false_positives.assign_add(fp)
        self.false_negatives.assign_add(fn)

    def result(self):
        precision = self.true_positives / (self.true_positives + self.false_positives + 1e-15)
        recall = self.true_positives / (self.true_positives + self.false_negatives + 1e-15)
        return 2 * precision * recall / (precision + recall + 1e-15)

    def reset_states(self):
        self.true_positives.assign(0.0)
        self.false_positives.assign(0.0)
        self.false_negatives.assign(0.0)

# ä½¿ç”¨F1åˆ†æ•°æŒ‡æ ‡
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy', F1Score()]
)
```

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–

### TensorFlow Profiler
```python
import tensorflow as tf

# åˆ›å»ºåˆ†æå™¨
profiler = tf.profiler.experimental.Profiler('/tmp/tf_profile')

# å¯åŠ¨åˆ†æ
tf.profiler.experimental.start('/tmp/tf_profile')

# è¿è¡Œè®­ç»ƒä»£ç 
model.fit(x_train, y_train, epochs=1)

# åœæ­¢åˆ†æ
tf.profiler.experimental.stop()

# æŸ¥çœ‹åˆ†æç»“æœ
# åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ http://localhost:6006 æŸ¥çœ‹TensorBoard
```

### å†…å­˜ä¼˜åŒ–
```python
# é™åˆ¶GPUå†…å­˜å¢é•¿
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# ä½¿ç”¨è™šæ‹ŸGPU
tf.config.experimental.set_virtual_device_configuration(
    gpus[0],
    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)]
)
```

### è®¡ç®—å›¾ä¼˜åŒ–
```python
# ä½¿ç”¨tf.functionç¼–è¯‘å‡½æ•°
@tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        predictions = model(x)
        loss = loss_fn(y, predictions)

    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss

# XLAç¼–è¯‘
@tf.function(jit_compile=True)
def optimized_train_step(x, y):
    return train_step(x, y)
```

## ğŸ“ˆ æ¨¡å‹è§£é‡Šæ€§

### Grad-CAMå¯è§†åŒ–
```python
def grad_cam(model, image, layer_name, class_idx):
    """Grad-CAMå¯è§†åŒ–"""
    # åˆ›å»ºæ¢¯åº¦æ¨¡å‹
    grad_model = tf.keras.models.Model(
        inputs=model.input,
        outputs=[model.get_layer(layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(image)
        loss = predictions[:, class_idx]

    # è®¡ç®—æ¢¯åº¦
    grads = tape.gradient(loss, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    # åº”ç”¨Grad-CAM
    conv_outputs = conv_outputs[0]
    heatmap = tf.reduce_sum(pooled_grads * conv_outputs, axis=-1)
    heatmap = tf.maximum(heatmap, 0)  # ReLU
    heatmap /= tf.reduce_max(heatmap)  # å½’ä¸€åŒ–

    return heatmap.numpy()

# ä½¿ç”¨Grad-CAM
image = tf.expand_dims(x_test[0], 0)
heatmap = grad_cam(model, image, 'dense1', class_idx=5)

# å¯è§†åŒ–çƒ­åŠ›å›¾
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.imshow(x_test[0].reshape(28, 28), cmap='gray')
plt.title('åŸå§‹å›¾åƒ')

plt.subplot(1, 2, 2)
plt.imshow(heatmap, cmap='jet')
plt.title('Grad-CAMçƒ­åŠ›å›¾')
plt.show()
```

### ç‰¹å¾é‡è¦æ€§åˆ†æ
```python
def permutation_importance(model, x, y, feature_names):
    """æ’åˆ—é‡è¦æ€§åˆ†æ"""
    baseline_score = model.evaluate(x, y, verbose=0)[1]
    importances = []

    for i in range(x.shape[1]):
        # æ‰“ä¹±ç¬¬iä¸ªç‰¹å¾
        x_permuted = x.copy()
        np.random.shuffle(x_permuted[:, i])

        # è®¡ç®—æ‰“ä¹±åçš„åˆ†æ•°
        permuted_score = model.evaluate(x_permuted, y, verbose=0)[1]
        importance = baseline_score - permuted_score
        importances.append(importance)

    return dict(zip(feature_names, importances))

# åˆ†æç‰¹å¾é‡è¦æ€§
feature_names = ['feature1', 'feature2', 'feature3', ...]
importances = permutation_importance(model, x_test, y_test, feature_names)

# ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§
plt.figure(figsize=(10, 6))
plt.barh(list(importances.keys()), list(importances.values()))
plt.xlabel('é‡è¦æ€§')
plt.title('ç‰¹å¾é‡è¦æ€§åˆ†æ')
plt.show()
```

## ğŸ¯ å®é™…é¡¹ç›®ï¼šå›¾åƒåˆ†ç±»å™¨

### å®Œæ•´é¡¹ç›®ä»£ç 
```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# æ•°æ®å‡†å¤‡
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

# æ•°æ®å¢å¼º
datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    zoom_range=0.1
)

datagen.fit(x_train)

# æ„å»ºé«˜çº§æ¨¡å‹
def create_advanced_model():
    base_model = tf.keras.applications.ResNet50(
        weights='imagenet',
        include_top=False,
        input_shape=(32, 32, 3)
    )

    # å†»ç»“åŸºç¡€æ¨¡å‹å±‚
    base_model.trainable = False

    model = tf.keras.Sequential([
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(10, activation='softmax')
    ])

    return model

# åˆ›å»ºå’Œç¼–è¯‘æ¨¡å‹
model = create_advanced_model()
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# è‡ªå®šä¹‰å›è°ƒ
class AdvancedCallback(tf.keras.callbacks.Callback):
    def __init__(self):
        self.best_accuracy = 0

    def on_epoch_end(self, epoch, logs=None):
        if logs['val_accuracy'] > self.best_accuracy:
            self.best_accuracy = logs['val_accuracy']
            self.model.save('best_model.h5')
            print(f"\nä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œå‡†ç¡®ç‡: {self.best_accuracy:.4f}")

# è®­ç»ƒæ¨¡å‹
history = model.fit(
    datagen.flow(x_train, y_train, batch_size=64),
    epochs=20,
    validation_data=(x_test, y_test),
    callbacks=[
        AdvancedCallback(),
        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3)
    ]
)

# è¯„ä¼°æ¨¡å‹
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")

# é¢„æµ‹å’Œå¯è§†åŒ–
predictions = model.predict(x_test[:9])
predicted_classes = tf.argmax(predictions, axis=1).numpy()

plt.figure(figsize=(12, 6))
for i in range(9):
    plt.subplot(3, 3, i+1)
    plt.imshow(x_test[i])
    plt.title(f"é¢„æµ‹: {predicted_classes[i]}, çœŸå®: {y_test[i][0]}")
    plt.axis('off')

plt.tight_layout()
plt.show()
```

## ğŸ“š å­¦ä¹ èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [TensorFlowé«˜çº§æ•™ç¨‹](https://www.tensorflow.org/tutorials)
- [TensorFlowæŒ‡å—](https://www.tensorflow.org/guide)
- [TensorFlowæ€§èƒ½æŒ‡å—](https://www.tensorflow.org/guide/performance)

### å´æ©è¾¾è¯¾ç¨‹
- æ·±åº¦å­¦ä¹ è¯¾ç¨‹ä¸­å…³äºTensorFlowé«˜çº§ç‰¹æ€§çš„éƒ¨åˆ†

### ç»å…¸è®ºæ–‡
- [EfficientNet: Rethinking Model Scaling](https://arxiv.org/abs/1905.11946)
- [Batch Normalization](https://arxiv.org/abs/1502.03167)
- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)

## ğŸ”§ æœ€ä½³å®è·µ

### ä»£ç ç»„ç»‡
```python
# é«˜çº§é¡¹ç›®ç»“æ„
advanced_project/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.yaml
â”‚   â””â”€â”€ hyperparameters.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”‚   â””â”€â”€ augmentation.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ custom_layers.py
â”‚   â”‚   â”œâ”€â”€ custom_losses.py
â”‚   â”‚   â””â”€â”€ model_builder.py
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â””â”€â”€ callbacks.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ visualization.py
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ experiments.ipynb
â””â”€â”€ scripts/
    â”œâ”€â”€ train.py
    â””â”€â”€ evaluate.py
```

### è°ƒè¯•æŠ€å·§
```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
tf.debugging.set_log_device_placement(True)

# æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
with tf.GradientTape() as tape:
    predictions = model(x)
    loss = loss_fn(y, predictions)

gradients = tape.gradient(loss, model.trainable_variables)

# æ£€æŸ¥æ¢¯åº¦
for i, grad in enumerate(gradients):
    if grad is not None:
        print(f"æ¢¯åº¦{i}çš„èŒƒæ•°: {tf.norm(grad).numpy()}")

# ä½¿ç”¨tf.printè°ƒè¯•
x = tf.print(x, [x], "è°ƒè¯•ä¿¡æ¯:")
```

---

*æœ€è¿‘æ›´æ–°: {{ .Lastmod.Format "2006-01-02" }}*