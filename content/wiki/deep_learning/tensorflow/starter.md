+++
date = '2025-10-24T21:58:35+08:00'
draft = false
title = 'TensorFlowåŸºç¡€æ•™ç¨‹'
comments = true
weight = 1
+++

# TensorFlowåŸºç¡€æ•™ç¨‹

æœ¬æ•™ç¨‹å°†ä»é›¶å¼€å§‹ä»‹ç»TensorFlowçš„åŸºç¡€çŸ¥è¯†ï¼ŒåŒ…æ‹¬ç¯å¢ƒæ­å»ºã€åŸºæœ¬æ¦‚å¿µã€ç®€å•æ¨¡å‹æ„å»ºå’Œè®­ç»ƒã€‚é€šè¿‡æœ¬æ•™ç¨‹ï¼Œä½ å°†èƒ½å¤Ÿä½¿ç”¨TensorFlowæ„å»ºå’Œè®­ç»ƒåŸºæœ¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚

## ğŸ› ï¸ ç¯å¢ƒæ­å»º

### å®‰è£…TensorFlow
```bash
# CPUç‰ˆæœ¬
pip install tensorflow

# GPUç‰ˆæœ¬ï¼ˆéœ€è¦CUDAæ”¯æŒï¼‰
pip install tensorflow[and-cuda]

# éªŒè¯å®‰è£…
python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU' if tf.test.is_gpu_available() else 'CPU'))"
```

### æ¨èå¼€å‘ç¯å¢ƒ
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n tf_env python=3.9
conda activate tf_env

# å®‰è£…TensorFlowå’Œç›¸å…³ä¾èµ–
pip install tensorflow numpy matplotlib jupyter pandas scikit-learn

# å¯åŠ¨Jupyter Notebook
jupyter notebook
```

## ğŸ¯ åŸºç¡€æ¦‚å¿µ

### Hello TensorFlow
```python
import tensorflow as tf

# æ‰“å°TensorFlowç‰ˆæœ¬
print(f"TensorFlowç‰ˆæœ¬: {tf.__version__}")

# åˆ›å»ºå¸¸é‡å¼ é‡
hello = tf.constant("Hello, TensorFlow!")
print(f"å¸¸é‡: {hello.numpy()}")

# åŸºæœ¬è¿ç®—
a = tf.constant(5)
b = tf.constant(3)
print(f"5 + 3 = {tf.add(a, b).numpy()}")
print(f"5 * 3 = {tf.multiply(a, b).numpy()}")
```

### å¼ é‡æ“ä½œ
```python
import tensorflow as tf
import numpy as np

# åˆ›å»ºä¸åŒç±»å‹çš„å¼ é‡
scalar = tf.constant(42)  # æ ‡é‡
vector = tf.constant([1, 2, 3, 4, 5])  # å‘é‡
matrix = tf.constant([[1, 2], [3, 4], [5, 6]])  # çŸ©é˜µ
tensor_3d = tf.constant(np.random.rand(2, 3, 4))  # ä¸‰ç»´å¼ é‡

print(f"æ ‡é‡å½¢çŠ¶: {scalar.shape}")
print(f"å‘é‡å½¢çŠ¶: {vector.shape}")
print(f"çŸ©é˜µå½¢çŠ¶: {matrix.shape}")
print(f"ä¸‰ç»´å¼ é‡å½¢çŠ¶: {tensor_3d.shape}")

# å¼ é‡è¿ç®—
x = tf.constant([1, 2, 3, 4, 5])
y = tf.constant([6, 7, 8, 9, 10])

# å…ƒç´ çº§è¿ç®—
print(f"x + y = {tf.add(x, y).numpy()}")
print(f"x * y = {tf.multiply(x, y).numpy()}")

# å¹¿æ’­æœºåˆ¶
matrix = tf.constant([[1, 2], [3, 4]])
vector = tf.constant([5, 6])
print(f"çŸ©é˜µ + å‘é‡å¹¿æ’­ç»“æœ:\n{tf.add(matrix, vector).numpy()}")
```

## ğŸ—ï¸ ç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œ

### çº¿æ€§å›å½’æ¨¡å‹
```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
np.random.seed(42)
X = np.random.rand(100, 1) * 10
y = 2 * X + 1 + np.random.randn(100, 1) * 0.5

# æ„å»ºæ¨¡å‹
model = tf.keras.Sequential([
    tf.keras.layers.Dense(1, input_shape=(1,))
])

# ç¼–è¯‘æ¨¡å‹
model.compile(
    optimizer='sgd',  # éšæœºæ¢¯åº¦ä¸‹é™
    loss='mse',       # å‡æ–¹è¯¯å·®
    metrics=['mae']   # å¹³å‡ç»å¯¹è¯¯å·®
)

# æ¨¡å‹æ‘˜è¦
model.summary()

# è®­ç»ƒæ¨¡å‹
history = model.fit(X, y, epochs=100, verbose=0)

# é¢„æµ‹
y_pred = model.predict(X)

# å¯è§†åŒ–ç»“æœ
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.scatter(X, y, alpha=0.5, label='çœŸå®æ•°æ®')
plt.plot(X, y_pred, color='red', label='é¢„æµ‹ç»“æœ')
plt.xlabel('X')
plt.ylabel('y')
plt.title('çº¿æ€§å›å½’ç»“æœ')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('è®­ç»ƒæŸå¤±æ›²çº¿')

plt.tight_layout()
plt.show()

print(f"æ¨¡å‹æƒé‡: {model.weights[0].numpy().flatten()}")
print(f"æ¨¡å‹åç½®: {model.weights[1].numpy()}")
```

### åˆ†ç±»æ¨¡å‹ - MNISTæ‰‹å†™æ•°å­—è¯†åˆ«
```python
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

# åŠ è½½MNISTæ•°æ®é›†
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# æ•°æ®é¢„å¤„ç†
x_train = x_train.reshape(-1, 784) / 255.0  # å±•å¹³å¹¶å½’ä¸€åŒ–
x_test = x_test.reshape(-1, 784) / 255.0
y_train = to_categorical(y_train, 10)  # one-hotç¼–ç 
y_test = to_categorical(y_test, 10)

# æ„å»ºæ¨¡å‹
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dropout(0.2),  # Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')  # è¾“å‡ºå±‚
])

# ç¼–è¯‘æ¨¡å‹
model.compile(
    optimizer='adam',  # Adamä¼˜åŒ–å™¨
    loss='categorical_crossentropy',  # äº¤å‰ç†µæŸå¤±
    metrics=['accuracy']  # å‡†ç¡®ç‡
)

# è®­ç»ƒæ¨¡å‹
history = model.fit(
    x_train, y_train,
    batch_size=128,
    epochs=10,
    validation_split=0.1,  # éªŒè¯é›†
    verbose=1
)

# è¯„ä¼°æ¨¡å‹
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")

# é¢„æµ‹
predictions = model.predict(x_test[:5])
predicted_labels = tf.argmax(predictions, axis=1).numpy()
true_labels = tf.argmax(y_test[:5], axis=1).numpy()

print("é¢„æµ‹ç»“æœ:", predicted_labels)
print("çœŸå®æ ‡ç­¾:", true_labels)
```

## ğŸ“Š æ•°æ®å¤„ç†

### ä½¿ç”¨tf.data API
```python
import tensorflow as tf
import numpy as np

# åˆ›å»ºæ•°æ®é›†
def create_dataset():
    # ç”Ÿæˆéšæœºæ•°æ®
    X = np.random.rand(1000, 10)
    y = np.random.randint(0, 2, 1000)

    # åˆ›å»ºtf.data.Dataset
    dataset = tf.data.Dataset.from_tensor_slices((X, y))

    # æ•°æ®é¢„å¤„ç†
    def preprocess(x, y):
        # å½’ä¸€åŒ–
        x = (x - tf.reduce_mean(x)) / tf.math.reduce_std(x)
        return x, y

    # åº”ç”¨é¢„å¤„ç†
    dataset = dataset.map(preprocess)

    # æ‰“ä¹±å’Œæ‰¹å¤„ç†
    dataset = dataset.shuffle(buffer_size=100)
    dataset = dataset.batch(32)

    # é‡å¤å’Œé¢„å–
    dataset = dataset.repeat()
    dataset = dataset.prefetch(tf.data.AUTOTUNE)

    return dataset

# ä½¿ç”¨æ•°æ®é›†è®­ç»ƒ
dataset = create_dataset()

model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# è®­ç»ƒ
model.fit(dataset, epochs=5, steps_per_epoch=10)
```

### å›¾åƒæ•°æ®å¢å¼º
```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# åˆ›å»ºå›¾åƒæ•°æ®ç”Ÿæˆå™¨
datagen = ImageDataGenerator(
    rotation_range=20,      # éšæœºæ—‹è½¬è§’åº¦
    width_shift_range=0.1,  # æ°´å¹³å¹³ç§»
    height_shift_range=0.1, # å‚ç›´å¹³ç§»
    shear_range=0.1,        # å‰ªåˆ‡å˜æ¢
    zoom_range=0.1,         # ç¼©æ”¾
    horizontal_flip=True,   # æ°´å¹³ç¿»è½¬
    fill_mode='nearest'     # å¡«å……æ¨¡å¼
)

# åº”ç”¨æ•°æ®å¢å¼º
# datagen.fit(x_train)  # å¦‚æœéœ€è¦çš„è¯
# enhanced_data = datagen.flow(x_train, y_train, batch_size=32)
```

## ğŸ¨ è‡ªå®šä¹‰æ¨¡å‹å’Œå±‚

### è‡ªå®šä¹‰å±‚
```python
import tensorflow as tf

class CustomDense(tf.keras.layers.Layer):
    def __init__(self, units, activation=None, **kwargs):
        super(CustomDense, self).__init__(**kwargs)
        self.units = units
        self.activation = tf.keras.activations.get(activation)

    def build(self, input_shape):
        # åˆ›å»ºæƒé‡
        self.w = self.add_weight(
            name='kernel',
            shape=(input_shape[-1], self.units),
            initializer='glorot_uniform',
            trainable=True
        )
        self.b = self.add_weight(
            name='bias',
            shape=(self.units,),
            initializer='zeros',
            trainable=True
        )

    def call(self, inputs):
        # å‰å‘ä¼ æ’­
        output = tf.matmul(inputs, self.w) + self.b
        if self.activation is not None:
            output = self.activation(output)
        return output

    def get_config(self):
        # åºåˆ—åŒ–é…ç½®
        config = super(CustomDense, self).get_config()
        config.update({
            'units': self.units,
            'activation': tf.keras.activations.serialize(self.activation)
        })
        return config

# ä½¿ç”¨è‡ªå®šä¹‰å±‚
model = tf.keras.Sequential([
    CustomDense(128, activation='relu', input_shape=(784,)),
    CustomDense(64, activation='relu'),
    CustomDense(10, activation='softmax')
])
```

### è‡ªå®šä¹‰æ¨¡å‹
```python
class CustomModel(tf.keras.Model):
    def __init__(self, num_classes):
        super(CustomModel, self).__init__()
        self.dense1 = tf.keras.layers.Dense(128, activation='relu')
        self.dense2 = tf.keras.layers.Dense(64, activation='relu')
        self.dense3 = tf.keras.layers.Dense(num_classes, activation='softmax')
        self.dropout = tf.keras.layers.Dropout(0.2)

    def call(self, inputs, training=False):
        x = self.dense1(inputs)
        x = self.dropout(x, training=training)
        x = self.dense2(x)
        x = self.dropout(x, training=training)
        return self.dense3(x)

# ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹
model = CustomModel(num_classes=10)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
```

## ğŸ”§ æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

### ä¿å­˜å’ŒåŠ è½½æ¨¡å‹
```python
import tensorflow as tf

# æ„å»ºå’Œè®­ç»ƒæ¨¡å‹
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# è®­ç»ƒæ¨¡å‹ï¼ˆè¿™é‡Œä½¿ç”¨éšæœºæ•°æ®ä½œä¸ºç¤ºä¾‹ï¼‰
import numpy as np
X_train = np.random.rand(100, 10)
y_train = np.random.randint(0, 2, 100)

model.fit(X_train, y_train, epochs=5, verbose=0)

# ä¿å­˜å®Œæ•´æ¨¡å‹
model.save('my_model.h5')  # HDF5æ ¼å¼
model.save('my_model')     # TensorFlow SavedModelæ ¼å¼

# åŠ è½½æ¨¡å‹
loaded_model_h5 = tf.keras.models.load_model('my_model.h5')
loaded_model = tf.keras.models.load_model('my_model')

# æ¯”è¾ƒé¢„æµ‹ç»“æœ
test_data = np.random.rand(5, 10)
original_pred = model.predict(test_data)
loaded_pred = loaded_model.predict(test_data)

print(f"é¢„æµ‹ç»“æœä¸€è‡´: {np.allclose(original_pred, loaded_pred)}")
```

### ä¿å­˜å’ŒåŠ è½½æƒé‡
```python
# ä¿å­˜æ¨¡å‹æƒé‡
model.save_weights('model_weights.h5')

# åˆ›å»ºç›¸åŒæ¶æ„çš„æ–°æ¨¡å‹
new_model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# åŠ è½½æƒé‡
new_model.load_weights('model_weights.h5')

# éªŒè¯æƒé‡æ˜¯å¦ç›¸åŒ
original_weights = model.get_weights()
loaded_weights = new_model.get_weights()

for i, (orig, loaded) in enumerate(zip(original_weights, loaded_weights)):
    print(f"æƒé‡{i}ä¸€è‡´: {np.allclose(orig, loaded)}")
```

## ğŸ“ˆ æ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–

### è¯„ä¼°æŒ‡æ ‡
```python
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# é¢„æµ‹æµ‹è¯•é›†
y_pred = model.predict(x_test)
y_pred_classes = tf.argmax(y_pred, axis=1).numpy()
y_true = tf.argmax(y_test, axis=1).numpy()

# åˆ†ç±»æŠ¥å‘Š
print("åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_true, y_pred_classes))

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('é¢„æµ‹æ ‡ç­¾')
plt.ylabel('çœŸå®æ ‡ç­¾')
plt.title('æ··æ·†çŸ©é˜µ')
plt.show()
```

### è®­ç»ƒå†å²å¯è§†åŒ–
```python
import matplotlib.pyplot as plt

# è®­ç»ƒå†å²
history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=20)

# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
plt.figure(figsize=(12, 4))

# æŸå¤±æ›²çº¿
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='è®­ç»ƒæŸå¤±')
plt.plot(history.history['val_loss'], label='éªŒè¯æŸå¤±')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
plt.legend()

# å‡†ç¡®ç‡æ›²çº¿
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡')
plt.plot(history.history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')
plt.legend()

plt.tight_layout()
plt.show()
```

## ğŸš€ å®é™…é¡¹ç›®ï¼šæˆ¿ä»·é¢„æµ‹

### å®Œæ•´é¡¹ç›®ä»£ç 
```python
import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ç”Ÿæˆæ¨¡æ‹Ÿæˆ¿ä»·æ•°æ®
np.random.seed(42)
n_samples = 1000

# ç‰¹å¾ï¼šé¢ç§¯ã€æˆ¿é—´æ•°ã€å¹´é¾„ã€è·ç¦»å¸‚ä¸­å¿ƒè·ç¦»
areas = np.random.normal(100, 20, n_samples)  # å¹³æ–¹ç±³
rooms = np.random.randint(1, 6, n_samples)     # æˆ¿é—´æ•°
ages = np.random.randint(0, 50, n_samples)     # æˆ¿å±‹å¹´é¾„
distances = np.random.normal(10, 3, n_samples) # è·ç¦»å¸‚ä¸­å¿ƒï¼ˆå…¬é‡Œï¼‰

# ç›®æ ‡ï¼šæˆ¿ä»·ï¼ˆä¸‡å…ƒï¼‰
prices = 50 + 0.8 * areas + 10 * rooms - 0.5 * ages - 2 * distances + np.random.normal(0, 10, n_samples)

# åˆ›å»ºDataFrame
data = pd.DataFrame({
    'area': areas,
    'rooms': rooms,
    'age': ages,
    'distance': distances,
    'price': prices
})

# æ•°æ®é¢„å¤„ç†
features = ['area', 'rooms', 'age', 'distance']
X = data[features].values
y = data['price'].values

# æ ‡å‡†åŒ–ç‰¹å¾
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# åˆ†å‰²æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# æ„å»ºæ¨¡å‹
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(4,)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1)  # å›å½’é—®é¢˜ï¼Œè¾“å‡ºå±‚æ— æ¿€æ´»å‡½æ•°
])

# ç¼–è¯‘æ¨¡å‹
model.compile(
    optimizer='adam',
    loss='mse',
    metrics=['mae', 'mse']
)

# è®­ç»ƒæ¨¡å‹
history = model.fit(
    X_train, y_train,
    batch_size=32,
    epochs=100,
    validation_split=0.2,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
    ],
    verbose=1
)

# è¯„ä¼°æ¨¡å‹
test_loss, test_mae, test_mse = model.evaluate(X_test, y_test, verbose=0)
print(f"æµ‹è¯•é›†MAE: {test_mae:.2f}ä¸‡å…ƒ")
print(f"æµ‹è¯•é›†MSE: {test_mse:.2f}ä¸‡å…ƒÂ²")

# é¢„æµ‹
predictions = model.predict(X_test[:5])
print("\né¢„æµ‹ç»“æœ:")
for i in range(5):
    print(f"çœŸå®ä»·æ ¼: {y_test[i]:.2f}ä¸‡å…ƒ, é¢„æµ‹ä»·æ ¼: {predictions[i][0]:.2f}ä¸‡å…ƒ")

# ä¿å­˜æ¨¡å‹
model.save('house_price_model.h5')
print("\næ¨¡å‹å·²ä¿å­˜ä¸º 'house_price_model.h5'")
```

## ğŸ¯ æœ€ä½³å®è·µ

### ä»£ç ç»„ç»‡
```python
# æ¨èçš„é¡¹ç›®ç»“æ„
house_price_prediction/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ saved_models/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_preprocessing.py
â”‚   â”œâ”€â”€ model.py
â”‚   â””â”€â”€ train.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml
â””â”€â”€ requirements.txt
```

### è°ƒè¯•æŠ€å·§
```python
# å¯ç”¨TensorFlowè°ƒè¯•
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # å‡å°‘è­¦å‘Šä¿¡æ¯

# æ£€æŸ¥æ¨¡å‹ç»“æ„
model.summary()

# æ£€æŸ¥å¼ é‡å½¢çŠ¶
print(f"è¾“å…¥å½¢çŠ¶: {x_train.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {model.predict(x_train[:1]).shape}")

# ä½¿ç”¨tf.debugging
x = tf.constant([1, 2, 3])
tf.debugging.assert_shapes([
    (x, ('N',)),  # æ–­è¨€xçš„å½¢çŠ¶
])
```

## ğŸ“š å­¦ä¹ èµ„æº

### å®˜æ–¹æ•™ç¨‹
- [TensorFlowå®˜æ–¹æ•™ç¨‹](https://www.tensorflow.org/tutorials)
- [Kerasæ–‡æ¡£](https://keras.io/getting_started/)
- [TensorFlow 2.0å¿«é€Ÿå…¥é—¨](https://www.tensorflow.org/tutorials/quickstart/beginner)

### å´æ©è¾¾è¯¾ç¨‹
- æ·±åº¦å­¦ä¹ è¯¾ç¨‹ä¸­å…³äºTensorFlowçš„éƒ¨åˆ†

### å®è·µé¡¹ç›®
- [TensorFlow Examples](https://github.com/tensorflow/examples)
- [Kaggleç«èµ›](https://www.kaggle.com/competitions)
- [Google Colab](https://colab.research.google.com/)

## ğŸ”§ å¸¸è§é—®é¢˜

### GPUå†…å­˜ä¸è¶³
```python
# é™åˆ¶GPUå†…å­˜ä½¿ç”¨
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)
```

### æ¨¡å‹ä¸æ”¶æ•›
```python
# è°ƒæ•´å­¦ä¹ ç‡
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

# ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=0.001,
    decay_steps=1000,
    decay_rate=0.9
)
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
```

### æ•°æ®é¢„å¤„ç†
```python
# æ­£ç¡®çš„æ•°æ®é¢„å¤„ç†æµç¨‹
def preprocess_data(X, y):
    # 1. å¤„ç†ç¼ºå¤±å€¼
    # 2. ç‰¹å¾ç¼©æ”¾
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # 3. åˆ†å‰²æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)

    return X_train, X_test, y_train, y_test, scaler
```

---

*æœ€è¿‘æ›´æ–°: {{ .Lastmod.Format "2006-01-02" }}*