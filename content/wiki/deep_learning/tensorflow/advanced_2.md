+++
date = '2025-10-24T21:58:51+08:00'
draft = false
title = 'TensorFlowé«˜çº§åº”ç”¨'
comments = true
weight = 3
+++

# TensorFlowé«˜çº§åº”ç”¨

æœ¬ç« ä»‹ç»TensorFlowåœ¨å®é™…é¡¹ç›®ä¸­çš„é«˜çº§åº”ç”¨ï¼ŒåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€å¼ºåŒ–å­¦ä¹ ã€æ¨¡å‹éƒ¨ç½²ç­‰é¢†åŸŸçš„å®Œæ•´è§£å†³æ–¹æ¡ˆå’Œæœ€ä½³å®è·µã€‚

## ğŸ¨ è®¡ç®—æœºè§†è§‰åº”ç”¨

### ç›®æ ‡æ£€æµ‹ - YOLOå®ç°
```python
import tensorflow as tf
import numpy as np
import cv2

def create_yolo_model(num_classes, input_shape=(416, 416, 3)):
    """åˆ›å»ºYOLOæ¨¡å‹"""
    inputs = tf.keras.Input(shape=input_shape)

    # ç‰¹å¾æå–ç½‘ç»œ
    x = tf.keras.layers.Conv2D(32, 3, strides=1, padding='same', activation='relu')(inputs)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.MaxPooling2D(pool_size=2)(x)

    x = tf.keras.layers.Conv2D(64, 3, strides=1, padding='same', activation='relu')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.MaxPooling2D(pool_size=2)(x)

    x = tf.keras.layers.Conv2D(128, 3, strides=1, padding='same', activation='relu')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.MaxPooling2D(pool_size=2)(x)

    # æ£€æµ‹å¤´
    x = tf.keras.layers.Conv2D(256, 3, strides=1, padding='same', activation='relu')(x)
    x = tf.keras.layers.Conv2D(512, 3, strides=1, padding='same', activation='relu')(x)

    # è¾“å‡ºå±‚ï¼šè¾¹ç•Œæ¡†ã€ç½®ä¿¡åº¦å’Œç±»åˆ«é¢„æµ‹
    output = tf.keras.layers.Conv2D(
        num_classes + 5, 1, strides=1, padding='same', activation='linear'
    )(x)

    return tf.keras.Model(inputs=inputs, outputs=output)

# YOLOæŸå¤±å‡½æ•°
class YOLOLoss(tf.keras.losses.Loss):
    def __init__(self, num_classes, grid_size=13, **kwargs):
        super(YOLOLoss, self).__init__(**kwargs)
        self.num_classes = num_classes
        self.grid_size = grid_size

    def call(self, y_true, y_pred):
        # å®ç°YOLOæŸå¤±å‡½æ•°
        # åŒ…æ‹¬è¾¹ç•Œæ¡†å›å½’æŸå¤±ã€ç½®ä¿¡åº¦æŸå¤±å’Œåˆ†ç±»æŸå¤±
        return total_loss

# åˆ›å»ºå’Œè®­ç»ƒYOLOæ¨¡å‹
model = create_yolo_model(num_classes=20)
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss=YOLOLoss(num_classes=20),
    metrics=['accuracy']
)
```

### å›¾åƒåˆ†å‰² - U-Netå®ç°
```python
def create_unet_model(input_shape=(256, 256, 3), num_classes=1):
    """åˆ›å»ºU-Netæ¨¡å‹"""
    inputs = tf.keras.Input(shape=input_shape)

    # ç¼–ç å™¨
    c1 = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu')(inputs)
    c1 = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu')(c1)
    p1 = tf.keras.layers.MaxPooling2D(pool_size=2)(c1)

    c2 = tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu')(p1)
    c2 = tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu')(c2)
    p2 = tf.keras.layers.MaxPooling2D(pool_size=2)(c2)

    c3 = tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu')(p2)
    c3 = tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu')(c3)
    p3 = tf.keras.layers.MaxPooling2D(pool_size=2)(c3)

    # ç“¶é¢ˆå±‚
    c4 = tf.keras.layers.Conv2D(512, 3, padding='same', activation='relu')(p3)
    c4 = tf.keras.layers.Conv2D(512, 3, padding='same', activation='relu')(c4)

    # è§£ç å™¨
    u5 = tf.keras.layers.Conv2DTranspose(256, 2, strides=2, padding='same')(c4)
    u5 = tf.keras.layers.concatenate([u5, c3])
    c5 = tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu')(u5)
    c5 = tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu')(c5)

    u6 = tf.keras.layers.Conv2DTranspose(128, 2, strides=2, padding='same')(c5)
    u6 = tf.keras.layers.concatenate([u6, c2])
    c6 = tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu')(u6)
    c6 = tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu')(c6)

    u7 = tf.keras.layers.Conv2DTranspose(64, 2, strides=2, padding='same')(c6)
    u7 = tf.keras.layers.concatenate([u7, c1])
    c7 = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu')(u7)
    c7 = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu')(c7)

    # è¾“å‡ºå±‚
    outputs = tf.keras.layers.Conv2D(num_classes, 1, activation='sigmoid')(c7)

    return tf.keras.Model(inputs=inputs, outputs=outputs)

# DiceæŸå¤±å‡½æ•°
def dice_loss(y_true, y_pred):
    smooth = 1e-15
    intersection = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)
    dice = (2.0 * intersection + smooth) / (union + smooth)
    return 1.0 - dice

# åˆ›å»ºå’Œè®­ç»ƒU-Net
model = create_unet_model()
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss=dice_loss,
    metrics=[tf.keras.metrics.MeanIoU(num_classes=2)]
)
```

## ğŸ“ è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨

### Transformeræ¨¡å‹å®ç°
```python
class TransformerModel(tf.keras.Model):
    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len):
        super(TransformerModel, self).__init__()

        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)
        self.pos_encoding = self.positional_encoding(max_len, d_model)

        self.encoder_layers = [
            self.encoder_layer(d_model, num_heads, d_ff) for _ in range(num_layers)
        ]
        self.decoder_layers = [
            self.decoder_layer(d_model, num_heads, d_ff) for _ in range(num_layers)
        ]

        self.final_layer = tf.keras.layers.Dense(vocab_size)

    def positional_encoding(self, max_len, d_model):
        pos = tf.range(max_len, dtype=tf.float32)[:, tf.newaxis]
        i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]
        angle_rates = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))
        angle_rads = pos * angle_rates

        angle_rads = tf.where(i % 2 == 0, tf.sin(angle_rads), tf.cos(angle_rads))
        return angle_rads[tf.newaxis, ...]

    def encoder_layer(self, d_model, num_heads, d_ff):
        inputs = tf.keras.Input(shape=(None, d_model))

        # å¤šå¤´æ³¨æ„åŠ›
        attn_output = tf.keras.layers.MultiHeadAttention(num_heads, d_model)(inputs, inputs)
        attn_output = tf.keras.layers.Dropout(0.1)(attn_output)
        out1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)

        # å‰é¦ˆç½‘ç»œ
        ffn_output = tf.keras.layers.Dense(d_ff, activation='relu')(out1)
        ffn_output = tf.keras.layers.Dense(d_model)(ffn_output)
        ffn_output = tf.keras.layers.Dropout(0.1)(ffn_output)
        out2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)

        return tf.keras.Model(inputs=inputs, outputs=out2)

    def decoder_layer(self, d_model, num_heads, d_ff):
        inputs = tf.keras.Input(shape=(None, d_model))
        enc_output = tf.keras.Input(shape=(None, d_model))

        # æ©ç å¤šå¤´æ³¨æ„åŠ›
        attn1 = tf.keras.layers.MultiHeadAttention(num_heads, d_model)(inputs, inputs)
        attn1 = tf.keras.layers.Dropout(0.1)(attn1)
        out1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attn1)

        # ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›
        attn2 = tf.keras.layers.MultiHeadAttention(num_heads, d_model)(out1, enc_output)
        attn2 = tf.keras.layers.Dropout(0.1)(attn2)
        out2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(out1 + attn2)

        # å‰é¦ˆç½‘ç»œ
        ffn_output = tf.keras.layers.Dense(d_ff, activation='relu')(out2)
        ffn_output = tf.keras.layers.Dense(d_model)(ffn_output)
        ffn_output = tf.keras.layers.Dropout(0.1)(ffn_output)
        out3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(out2 + ffn_output)

        return tf.keras.Model(inputs=[inputs, enc_output], outputs=out3)

    def call(self, inputs, targets=None, training=False):
        # è¯åµŒå…¥ + ä½ç½®ç¼–ç 
        x = self.embedding(inputs)
        x *= tf.math.sqrt(tf.cast(tf.shape(x)[-1], tf.float32))
        x += self.pos_encoding[:, :tf.shape(x)[1], :]

        # ç¼–ç å™¨
        enc_output = x
        for layer in self.encoder_layers:
            enc_output = layer(enc_output)

        # è§£ç å™¨
        if targets is not None:
            y = self.embedding(targets)
            y *= tf.math.sqrt(tf.cast(tf.shape(y)[-1], tf.float32))
            y += self.pos_encoding[:, :tf.shape(y)[1], :]

            for layer in self.decoder_layers:
                y = layer([y, enc_output])

            outputs = self.final_layer(y)
        else:
            outputs = None

        return outputs, enc_output

# åˆ›å»ºTransformeræ¨¡å‹
model = TransformerModel(
    vocab_size=10000,
    d_model=512,
    num_heads=8,
    num_layers=6,
    d_ff=2048,
    max_len=100
)
```

### BERTé¢„è®­ç»ƒæ¨¡å‹
```python
class BERTModel(tf.keras.Model):
    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len):
        super(BERTModel, self).__init__()

        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)
        self.pos_encoding = self.positional_encoding(max_len, d_model)
        self.segment_embedding = tf.keras.layers.Embedding(2, d_model)

        self.transformer_layers = [
            self.transformer_layer(d_model, num_heads, d_ff) for _ in range(num_layers)
        ]

        self.mlm_head = tf.keras.layers.Dense(vocab_size, activation='softmax')
        self.nsp_head = tf.keras.layers.Dense(2, activation='softmax')

    def transformer_layer(self, d_model, num_heads, d_ff):
        inputs = tf.keras.Input(shape=(None, d_model))

        # å¤šå¤´æ³¨æ„åŠ›
        attn_output = tf.keras.layers.MultiHeadAttention(num_heads, d_model)(inputs, inputs)
        attn_output = tf.keras.layers.Dropout(0.1)(attn_output)
        out1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)

        # å‰é¦ˆç½‘ç»œ
        ffn_output = tf.keras.layers.Dense(d_ff, activation='relu')(out1)
        ffn_output = tf.keras.layers.Dense(d_model)(ffn_output)
        ffn_output = tf.keras.layers.Dropout(0.1)(ffn_output)
        out2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)

        return tf.keras.Model(inputs=inputs, outputs=out2)

    def call(self, inputs, segment_ids=None, masked_positions=None):
        # è¯åµŒå…¥ + ä½ç½®ç¼–ç  + æ®µç¼–ç 
        x = self.embedding(inputs)
        x += self.pos_encoding[:, :tf.shape(x)[1], :]

        if segment_ids is not None:
            x += self.segment_embedding(segment_ids)

        # Transformerå±‚
        for layer in self.transformer_layers:
            x = layer(x)

        # æ©ç è¯­è¨€æ¨¡å‹é¢„æµ‹
        if masked_positions is not None:
            masked_outputs = tf.gather(x, masked_positions, axis=1, batch_dims=1)
            mlm_logits = self.mlm_head(masked_outputs)
        else:
            mlm_logits = None

        # ä¸‹ä¸€å¥é¢„æµ‹
        cls_token = x[:, 0, :]  # [CLS] token
        nsp_logits = self.nsp_head(cls_token)

        return mlm_logits, nsp_logits

# åˆ›å»ºBERTæ¨¡å‹
bert_model = BERTModel(
    vocab_size=30000,
    d_model=768,
    num_heads=12,
    num_layers=12,
    d_ff=3072,
    max_len=512
)
```

## ğŸ® å¼ºåŒ–å­¦ä¹ åº”ç”¨

### DQNå®ç°
```python
import tensorflow as tf
import numpy as np
import random
from collections import deque

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        self.gamma = 0.95  # æŠ˜æ‰£å› å­
        self.epsilon = 1.0  # æ¢ç´¢ç‡
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001

        self.model = self._build_model()
        self.target_model = self._build_model()
        self.update_target_model()

    def _build_model(self):
        """æ„å»ºDQNç½‘ç»œ"""
        model = tf.keras.Sequential([
            tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'),
            tf.keras.layers.Dense(24, activation='relu'),
            tf.keras.layers.Dense(self.action_size, activation='linear')
        ])

        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),
            loss='mse'
        )

        return model

    def update_target_model(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_model.set_weights(self.model.get_weights())

    def remember(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        """é€‰æ‹©åŠ¨ä½œ"""
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)  # éšæœºæ¢ç´¢

        act_values = self.model.predict(state, verbose=0)
        return np.argmax(act_values[0])  # åˆ©ç”¨

    def replay(self, batch_size):
        """ç»éªŒå›æ”¾"""
        minibatch = random.sample(self.memory, batch_size)

        for state, action, reward, next_state, done in minibatch:
            target = self.model.predict(state, verbose=0)

            if done:
                target[0][action] = reward
            else:
                t = self.target_model.predict(next_state, verbose=0)
                target[0][action] = reward + self.gamma * np.amax(t[0])

            self.model.fit(state, target, epochs=1, verbose=0)

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# ä½¿ç”¨DQNä»£ç†
env = gym.make('CartPole-v1')
state_size = env.observation_space.shape[0]
action_size = env.action_space.n

agent = DQNAgent(state_size, action_size)

# è®­ç»ƒDQN
episodes = 1000
batch_size = 32

for e in range(episodes):
    state = env.reset()
    state = np.reshape(state, [1, state_size])

    for time in range(500):
        action = agent.act(state)
        next_state, reward, done, _ = env.step(action)
        next_state = np.reshape(next_state, [1, state_size])

        agent.remember(state, action, reward, next_state, done)
        state = next_state

        if done:
            agent.update_target_model()
            print(f"Episode: {e}/{episodes}, Score: {time}")
            break

        if len(agent.memory) > batch_size:
            agent.replay(batch_size)
```

### PPOå®ç°
```python
class PPOAgent:
    def __init__(self, state_size, action_size, clip_ratio=0.2):
        self.state_size = state_size
        self.action_size = action_size
        self.clip_ratio = clip_ratio

        # Actorç½‘ç»œ
        self.actor = self._build_actor()
        # Criticç½‘ç»œ
        self.critic = self._build_critic()

        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)
        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

    def _build_actor(self):
        """æ„å»ºç­–ç•¥ç½‘ç»œ"""
        inputs = tf.keras.Input(shape=(self.state_size,))
        x = tf.keras.layers.Dense(64, activation='relu')(inputs)
        x = tf.keras.layers.Dense(64, activation='relu')(x)
        outputs = tf.keras.layers.Dense(self.action_size, activation='softmax')(x)
        return tf.keras.Model(inputs=inputs, outputs=outputs)

    def _build_critic(self):
        """æ„å»ºä»·å€¼ç½‘ç»œ"""
        inputs = tf.keras.Input(shape=(self.state_size,))
        x = tf.keras.layers.Dense(64, activation='relu')(inputs)
        x = tf.keras.layers.Dense(64, activation='relu')(x)
        outputs = tf.keras.layers.Dense(1)(x)
        return tf.keras.Model(inputs=inputs, outputs=outputs)

    def get_action(self, state):
        """è·å–åŠ¨ä½œ"""
        state = np.reshape(state, [1, self.state_size])
        probs = self.actor.predict(state, verbose=0)[0]
        action = np.random.choice(self.action_size, p=probs)
        return action, probs[action]

    def compute_advantages(self, rewards, values, next_values, dones):
        """è®¡ç®—ä¼˜åŠ¿å‡½æ•°"""
        advantages = np.zeros_like(rewards)
        last_gae_lam = 0

        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_non_terminal = 1.0 - dones[t]
                next_values = next_values[t]
            else:
                next_non_terminal = 1.0 - dones[t]
                next_values = values[t + 1]

            delta = rewards[t] + 0.99 * next_values * next_non_terminal - values[t]
            advantages[t] = last_gae_lam = delta + 0.99 * 0.95 * next_non_terminal * last_gae_lam

        return advantages

    def train(self, states, actions, old_probs, advantages, returns):
        """PPOè®­ç»ƒ"""
        with tf.GradientTape() as tape:
            probs = self.actor(states)
            values = self.critic(states)

            # è®¡ç®—ç­–ç•¥æŸå¤±
            new_probs = tf.gather(probs, actions, axis=1, batch_dims=1)
            ratio = new_probs / old_probs
            clipped_ratio = tf.clip_by_value(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio)

            policy_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, clipped_ratio * advantages))

            # è®¡ç®—ä»·å€¼æŸå¤±
            value_loss = tf.reduce_mean(tf.square(returns - values))

            # æ€»æŸå¤±
            loss = policy_loss + 0.5 * value_loss

        # æ›´æ–°Actor
        actor_grads = tape.gradient(policy_loss, self.actor.trainable_variables)
        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))

        # æ›´æ–°Critic
        critic_grads = tape.gradient(value_loss, self.critic.trainable_variables)
        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))

        return loss
```

## ğŸš€ æ¨¡å‹éƒ¨ç½²

### TensorFlow Serving
```python
# ä¿å­˜æ¨¡å‹ç”¨äºServing
import tensorflow as tf

# åˆ›å»ºæ¨¡å‹
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# è®­ç»ƒæ¨¡å‹ï¼ˆç¤ºä¾‹ï¼‰
# model.fit(...)

# ä¿å­˜æ¨¡å‹
model.save('model/1')  # TensorFlow Servingæ ¼å¼

# å¯åŠ¨TensorFlow Serving
"""
tensorflow_model_server \
    --rest_api_port=8501 \
    --model_name=my_model \
    --model_base_path=/path/to/model
"""

# ä½¿ç”¨REST APIè¿›è¡Œé¢„æµ‹
import requests
import json

data = json.dumps({
    "signature_name": "serving_default",
    "inputs": {
        "dense_input": [[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]]
    }
})

headers = {"content-type": "application/json"}
response = requests.post('http://localhost:8501/v1/models/my_model:predict', data=data, headers=headers)
print(response.json())
```

### TensorFlow Liteè½¬æ¢å’Œéƒ¨ç½²
```python
# è½¬æ¢ä¸ºTensorFlow Lite
def convert_to_tflite(model, quantization='float32'):
    """è½¬æ¢ä¸ºTensorFlow Liteæ ¼å¼"""

    # åˆ›å»ºè½¬æ¢å™¨
    converter = tf.lite.TFLiteConverter.from_keras_model(model)

    if quantization == 'float16':
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_types = [tf.float16]
    elif quantization == 'int8':
        converter.optimizations = [tf.lite.Optimize.DEFAULT]

        # æä¾›ä»£è¡¨æ€§æ•°æ®é›†ç”¨äºé‡åŒ–
        def representative_data_gen():
            for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):
                yield [input_value]

        converter.representative_dataset = representative_data_gen
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
        converter.inference_input_type = tf.int8
        converter.inference_output_type = tf.int8

    # è½¬æ¢æ¨¡å‹
    tflite_model = converter.convert()

    # ä¿å­˜æ¨¡å‹
    with open(f'model_{quantization}.tflite', 'wb') as f:
        f.write(tflite_model)

    return tflite_model

# è½¬æ¢ä¸åŒç²¾åº¦ç‰ˆæœ¬
model = create_model()  # å‡è®¾çš„æ¨¡å‹åˆ›å»ºå‡½æ•°

# è½¬æ¢ä¸ºä¸åŒç²¾åº¦
convert_to_tflite(model, 'float32')
convert_to_tflite(model, 'float16')
convert_to_tflite(model, 'int8')

# è¯„ä¼°æ¨¡å‹å¤§å°å’Œæ€§èƒ½
import os

for precision in ['float32', 'float16', 'int8']:
    model_path = f'model_{precision}.tflite'
    if os.path.exists(model_path):
        size = os.path.getsize(model_path) / 1024 / 1024  # MB
        print(f"{precision}æ¨¡å‹å¤§å°: {size:.2f} MB")
```

### TensorFlow.jsè½¬æ¢
```python
# è½¬æ¢ä¸ºTensorFlow.jsæ ¼å¼
import tensorflowjs as tfjs

# ä¿å­˜ä¸ºTensorFlow.jsæ ¼å¼
tfjs.converters.save_keras_model(model, 'tfjs_model/')

# æˆ–è€…è½¬æ¢ä¸ºåˆ†å±‚æ ¼å¼
tfjs.converters.save_keras_model(model, 'tfjs_model/', quantization_dtype=tfjs.quantization_config.INT8)

# HTMLä¸­ä½¿ç”¨æ¨¡å‹
"""
<html>
<head>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
</head>
<body>
    <script>
        async function loadModel() {
            // åŠ è½½æ¨¡å‹
            const model = await tf.loadLayersModel('tfjs_model/model.json');

            // å‡†å¤‡è¾“å…¥æ•°æ®
            const input = tf.tensor2d([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]);

            // è¿›è¡Œé¢„æµ‹
            const prediction = model.predict(input);
            prediction.print();
        }

        loadModel();
    </script>
</body>
</html>
"""
```

## ğŸ“Š ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

### æ¨¡å‹ç›‘æ§
```python
class ModelMonitor:
    def __init__(self, model, metrics=['accuracy', 'latency', 'throughput']):
        self.model = model
        self.metrics = metrics
        self.predictions = []
        self.true_labels = []
        self.latencies = []

    def predict_with_monitoring(self, inputs, true_labels=None):
        """å¸¦ç›‘æ§çš„é¢„æµ‹"""
        import time

        start_time = time.time()

        # è¿›è¡Œé¢„æµ‹
        predictions = self.model.predict(inputs)

        end_time = time.time()
        latency = end_time - start_time

        # è®°å½•æŒ‡æ ‡
        self.latencies.append(latency)
        self.predictions.extend(predictions)

        if true_labels is not None:
            self.true_labels.extend(true_labels)

        return predictions

    def generate_report(self):
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        report = {}

        if self.latencies:
            report['avg_latency'] = np.mean(self.latencies)
            report['p95_latency'] = np.percentile(self.latencies, 95)
            report['throughput'] = len(self.latencies) / sum(self.latencies)

        if self.true_labels and self.predictions:
            predictions = np.array(self.predictions)
            true_labels = np.array(self.true_labels)

            if len(predictions.shape) > 1:
                pred_classes = np.argmax(predictions, axis=1)
                true_classes = np.argmax(true_labels, axis=1)
            else:
                pred_classes = (predictions > 0.5).astype(int)
                true_classes = true_labels

            report['accuracy'] = np.mean(pred_classes == true_classes)

        return report

# ä½¿ç”¨æ¨¡å‹ç›‘æ§
monitor = ModelMonitor(model)

# æ¨¡æ‹Ÿç”Ÿäº§ç¯å¢ƒé¢„æµ‹
for i in range(100):
    test_input = np.random.rand(1, 10)
    true_label = np.random.randint(0, 2, 1)

    prediction = monitor.predict_with_monitoring(test_input, true_label)

# ç”ŸæˆæŠ¥å‘Š
report = monitor.generate_report()
print("æ¨¡å‹ç›‘æ§æŠ¥å‘Š:", report)
```

### A/Bæµ‹è¯•æ¡†æ¶
```python
class ABTestFramework:
    def __init__(self, model_a, model_b, traffic_split=0.5):
        self.model_a = model_a
        self.model_b = model_b
        self.traffic_split = traffic_split

        self.model_a_metrics = []
        self.model_b_metrics = []

    def predict(self, inputs, true_labels=None):
        """A/Bæµ‹è¯•é¢„æµ‹"""
        results = []

        for i, (input_data, true_label) in enumerate(zip(inputs, true_labels or [])):
            # éšæœºåˆ†é…æµé‡
            if np.random.random() < self.traffic_split:
                model = self.model_a
                model_name = 'A'
            else:
                model = self.model_b
                model_name = 'B'

            # è¿›è¡Œé¢„æµ‹
            prediction = model.predict(np.expand_dims(input_data, 0))[0]

            results.append({
                'model': model_name,
                'prediction': prediction,
                'true_label': true_label
            })

        return results

    def evaluate_models(self, results):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        model_a_results = [r for r in results if r['model'] == 'A']
        model_b_results = [r for r in results if r['model'] == 'B']

        def calculate_metrics(results):
            if not results:
                return {}

            predictions = np.array([r['prediction'] for r in results])
            true_labels = np.array([r['true_label'] for r in results])

            pred_classes = np.argmax(predictions, axis=1)
            true_classes = np.argmax(true_labels, axis=1)

            accuracy = np.mean(pred_classes == true_classes)

            return {'accuracy': accuracy, 'sample_size': len(results)}

        metrics_a = calculate_metrics(model_a_results)
        metrics_b = calculate_metrics(model_b_results)

        return {'model_a': metrics_a, 'model_b': metrics_b}

# ä½¿ç”¨A/Bæµ‹è¯•æ¡†æ¶
ab_test = ABTestFramework(model_v1, model_v2, traffic_split=0.5)

# æ¨¡æ‹ŸA/Bæµ‹è¯•
test_inputs = [np.random.rand(10) for _ in range(1000)]
test_labels = [np.random.randint(0, 2, 10) for _ in range(1000)]

results = ab_test.predict(test_inputs, test_labels)
metrics = ab_test.evaluate_models(results)

print("A/Bæµ‹è¯•ç»“æœ:", metrics)
```

## ğŸ“š å­¦ä¹ èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [TensorFlow Servingæ–‡æ¡£](https://www.tensorflow.org/tfx/guide/serving)
- [TensorFlow Liteæ–‡æ¡£](https://www.tensorflow.org/lite)
- [TensorFlow.jsæ–‡æ¡£](https://www.tensorflow.org/js)

### å´æ©è¾¾è¯¾ç¨‹
- æ·±åº¦å­¦ä¹ è¯¾ç¨‹ä¸­å…³äºå®é™…åº”ç”¨çš„éƒ¨åˆ†

### ç»å…¸é¡¹ç›®
- [TensorFlow Models](https://github.com/tensorflow/models) - å®˜æ–¹æ¨¡å‹åº“
- [TensorFlow Hub](https://tfhub.dev/) - é¢„è®­ç»ƒæ¨¡å‹ä»“åº“
- [Kaggleç«èµ›](https://www.kaggle.com/competitions) - å®è·µé¡¹ç›®

## ğŸ”§ éƒ¨ç½²æœ€ä½³å®è·µ

### å®¹å™¨åŒ–éƒ¨ç½²
```dockerfile
# Dockerfile
FROM tensorflow/serving:latest

COPY model/ /models/my_model
ENV MODEL_NAME=my_model

# å¯åŠ¨å‘½ä»¤
CMD ["tensorflow_model_server", "--rest_api_port=8501", "--model_name=my_model", "--model_base_path=/models/my_model"]
```

### æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
```python
# æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
import tensorflow as tf

class ModelVersionManager:
    def __init__(self, model_base_path):
        self.model_base_path = model_base_path
        self.versions = {}

    def save_model_version(self, model, version, metadata=None):
        """ä¿å­˜æ¨¡å‹ç‰ˆæœ¬"""
        version_path = f"{self.model_base_path}/{version}"

        # ä¿å­˜æ¨¡å‹
        model.save(version_path)

        # ä¿å­˜å…ƒæ•°æ®
        if metadata:
            with open(f"{version_path}/metadata.json", 'w') as f:
                json.dump(metadata, f)

        self.versions[version] = {
            'path': version_path,
            'metadata': metadata,
            'timestamp': time.time()
        }

    def load_model_version(self, version):
        """åŠ è½½æ¨¡å‹ç‰ˆæœ¬"""
        if version not in self.versions:
            raise ValueError(f"ç‰ˆæœ¬ {version} ä¸å­˜åœ¨")

        version_path = self.versions[version]['path']
        return tf.keras.models.load_model(version_path)

    def list_versions(self):
        """åˆ—å‡ºç‰ˆæœ¬"""
        return list(self.versions.keys())

    def rollback(self, version):
        """å›æ»šåˆ°æŒ‡å®šç‰ˆæœ¬"""
        model = self.load_model_version(version)
        self.save_model_version(model, 'current', {'rollback_from': version})
        return model

# ä½¿ç”¨ç‰ˆæœ¬ç®¡ç†å™¨
version_manager = ModelVersionManager('./models')

# ä¿å­˜ä¸åŒç‰ˆæœ¬
for i, model in enumerate([model_v1, model_v2, model_v3]):
    version_manager.save_model_version(
        model,
        version=f"v{i+1}",
        metadata={'description': f'æ¨¡å‹ç‰ˆæœ¬{i+1}', 'accuracy': 0.95 + i*0.01}
    )
```

---

*æœ€è¿‘æ›´æ–°: {{ .Lastmod.Format "2006-01-02" }}*