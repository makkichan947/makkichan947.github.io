+++
date = '2025-10-19T20:10:09+08:00'
draft = false
title = 'ç¥ç»ç½‘ç»œåŸºç¡€'
comments = true
weight = 1
+++

# ç¥ç»ç½‘ç»œåŸºç¡€

ç¥ç»ç½‘ç»œï¼ˆNeural Networksï¼‰æ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼Œæ¨¡æ‹Ÿäº†äººè„‘ç¥ç»å…ƒçš„å·¥ä½œæ–¹å¼ã€‚æœ¬ç« è¯¦ç»†ä»‹ç»ç¥ç»ç½‘ç»œçš„åŸºæœ¬æ¦‚å¿µã€ç»“æ„å’Œè®­ç»ƒæ–¹æ³•ã€‚

## ğŸ§  ç¥ç»å…ƒæ¨¡å‹

### ç”Ÿç‰©ç¥ç»å…ƒ
äººè„‘ä¸­çš„ç¥ç»å…ƒé€šè¿‡çªè§¦æ¥æ”¶å’Œå¤„ç†ä¿¡å·ï¼Œç¥ç»ç½‘ç»œä¸­çš„äººå·¥ç¥ç»å…ƒæ¨¡ä»¿äº†è¿™ä¸€è¿‡ç¨‹ã€‚

### äººå·¥ç¥ç»å…ƒ
äººå·¥ç¥ç»å…ƒï¼ˆArtificial Neuronï¼‰æ˜¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬å•å…ƒï¼š

**æ•°å­¦æ¨¡å‹**ï¼š
$$z = \sum_{i=1}^{n} w_i x_i + b$$
$$a = g(z)$$

å…¶ä¸­ï¼š
- $x_i$ï¼šè¾“å…¥ç‰¹å¾
- $w_i$ï¼šæƒé‡å‚æ•°
- $b$ï¼šåç½®é¡¹
- $g(\cdot)$ï¼šæ¿€æ´»å‡½æ•°
- $a$ï¼šç¥ç»å…ƒè¾“å‡º

### ç¥ç»ç½‘ç»œç»“æ„

**å•å±‚ç¥ç»ç½‘ç»œ**ï¼š
```
è¾“å…¥å±‚ â†’ è¾“å‡ºå±‚
```

**å¤šå±‚ç¥ç»ç½‘ç»œ**ï¼š
```
è¾“å…¥å±‚ â†’ éšè—å±‚1 â†’ éšè—å±‚2 â†’ ... â†’ è¾“å‡ºå±‚
```

## ğŸ”„ å‰å‘ä¼ æ’­

### è®¡ç®—è¿‡ç¨‹

**ç¬¬ä¸€å±‚**ï¼š
$$z^{(1)} = W^{(1)} x + b^{(1)}$$
$$a^{(1)} = g^{(1)}(z^{(1)})$$

**ç¬¬lå±‚**ï¼š
$$z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$$
$$a^{(l)} = g^{(l)}(z^{(l)})$$

### å‘é‡è¡¨ç¤º

ä½¿ç”¨çŸ©é˜µè¿ç®—æé«˜æ•ˆç‡ï¼š
$$Z = W \cdot A + b$$
$$A = g(Z)$$

## ğŸ“‰ åå‘ä¼ æ’­

### æŸå¤±å‡½æ•°

**å‡æ–¹è¯¯å·® (MSE)**ï¼š
$$J = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2$$

**äº¤å‰ç†µæŸå¤±**ï¼š
$$J = -\frac{1}{m} \sum_{i=1}^{m} [y_i \log \hat{y}_i + (1-y_i) \log (1-\hat{y}_i)]$$

### æ¢¯åº¦è®¡ç®—

**è¾“å‡ºå±‚æ¢¯åº¦**ï¼š
$$\frac{\partial J}{\partial z^{(L)}} = a^{(L)} - y$$

**éšè—å±‚æ¢¯åº¦**ï¼š
$$\frac{\partial J}{\partial z^{(l)}} = \frac{\partial J}{\partial z^{(l+1)}} \cdot (W^{(l+1)})^T \odot g'(z^{(l)})$$

## âš¡ æ¿€æ´»å‡½æ•°

### Sigmoidå‡½æ•°
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**å¯¼æ•°**ï¼š
$$\sigma'(z) = \sigma(z) (1 - \sigma(z))$$

### ReLUå‡½æ•°
$$ReLU(z) = \max(0, z)$$

**å¯¼æ•°**ï¼š
$$ReLU'(z) = \begin{cases}
1 & z > 0 \\
0 & z \leq 0
\end{cases}$$

### Tanhå‡½æ•°
$$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$

**å¯¼æ•°**ï¼š
$$\tanh'(z) = 1 - \tanh^2(z)$$

## ğŸ¯ æ¢¯åº¦ä¸‹é™

### æ‰¹é‡æ¢¯åº¦ä¸‹é™ (BGD)

**å‚æ•°æ›´æ–°**ï¼š
$$W = W - \alpha \frac{\partial J}{\partial W}$$
$$b = b - \alpha \frac{\partial J}{\partial b}$$

### éšæœºæ¢¯åº¦ä¸‹é™ (SGD)

**å‚æ•°æ›´æ–°**ï¼š
$$W = W - \alpha \frac{\partial J^{(i)}}{\partial W}$$
$$b = b - \alpha \frac{\partial J^{(i)}}{\partial b}$$

### å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ (Mini-batch SGD)

ç»“åˆBGDå’ŒSGDçš„ä¼˜ç‚¹ï¼š
- æ¯æ¬¡ä½¿ç”¨ä¸€å°æ‰¹æ ·æœ¬è®¡ç®—æ¢¯åº¦
- æ”¶æ•›é€Ÿåº¦å¿«ä¸”æ›´ç¨³å®š

## ğŸ—ï¸ ç½‘ç»œæ¶æ„

### å‰é¦ˆç¥ç»ç½‘ç»œ (FNN)

**ç‰¹ç‚¹**ï¼š
- ä¿¡æ¯å•å‘æµåŠ¨
- æ²¡æœ‰å¾ªç¯è¿æ¥
- é€‚åˆåˆ†ç±»å’Œå›å½’ä»»åŠ¡

### æ·±åº¦ç¥ç»ç½‘ç»œ (DNN)

**æ·±åº¦**ï¼šé€šå¸¸æŒ‡å…·æœ‰å¤šä¸ªéšè—å±‚çš„ç½‘ç»œ
**ä¼˜åŠ¿**ï¼š
- å­¦ä¹ æ›´å¤æ‚çš„ç‰¹å¾è¡¨ç¤º
- è§£å†³æ›´å¤æ‚çš„å®é™…é—®é¢˜

## ğŸ“Š è¿‡æ‹Ÿåˆä¸æ­£åˆ™åŒ–

### è¿‡æ‹Ÿåˆé—®é¢˜

**è¡¨ç°**ï¼š
- è®­ç»ƒè¯¯å·®ä½ï¼Œæµ‹è¯•è¯¯å·®é«˜
- æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾ˆå¥½
- åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°å¾ˆå·®

### L2æ­£åˆ™åŒ–

**æŸå¤±å‡½æ•°**ï¼š
$$J_{regularized} = J + \frac{\lambda}{2m} \sum_{l=1}^{L} \|W^{(l)}\|_F^2$$

**æ¢¯åº¦**ï¼š
$$\frac{\partial J_{regularized}}{\partial W} = \frac{\partial J}{\partial W} + \frac{\lambda}{m} W$$

### Dropout

**è®­ç»ƒæ—¶**ï¼šéšæœºä¸¢å¼ƒéƒ¨åˆ†ç¥ç»å…ƒ
**æµ‹è¯•æ—¶**ï¼šä½¿ç”¨æ‰€æœ‰ç¥ç»å…ƒä½†æƒé‡ç¼©æ”¾

## ğŸš€ ç¼–ç¨‹å®ç°

### PyTorchå®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim

class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNetwork, self).__init__()
        self.layer1 = nn.Linear(input_size, hidden_size)
        self.layer2 = nn.Linear(hidden_size, output_size)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.layer2(x)
        return x

# æ¨¡å‹åˆå§‹åŒ–
model = NeuralNetwork(784, 128, 10)

# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒè¿‡ç¨‹
for epoch in range(num_epochs):
    for data, target in dataloader:
        # å‰å‘ä¼ æ’­
        output = model(data)
        loss = criterion(output, target)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­

```python
def forward_propagation(X, parameters):
    """å‰å‘ä¼ æ’­"""
    W1, b1, W2, b2 = parameters

    # ç¬¬ä¸€å±‚
    Z1 = np.dot(W1, X) + b1
    A1 = relu(Z1)

    # ç¬¬äºŒå±‚
    Z2 = np.dot(W2, A1) + b2
    A2 = softmax(Z2)

    cache = {"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2}
    return A2, cache

def backward_propagation(parameters, cache, X, Y):
    """åå‘ä¼ æ’­"""
    W1, b1, W2, b2 = parameters
    Z1, A1, Z2, A2 = cache.values()

    m = X.shape[1]

    # è¾“å‡ºå±‚æ¢¯åº¦
    dZ2 = A2 - Y
    dW2 = (1/m) * np.dot(dZ2, A1.T)
    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)

    # éšè—å±‚æ¢¯åº¦
    dZ1 = np.dot(W2.T, dZ2) * relu_derivative(Z1)
    dW1 = (1/m) * np.dot(dZ1, X.T)
    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)

    gradients = {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2}
    return gradients
```

## ğŸ¨ æ¿€æ´»å‡½æ•°å¯è§†åŒ–

### Sigmoidå‡½æ•°
```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def sigmoid_derivative(z):
    return sigmoid(z) * (1 - sigmoid(z))
```

### ReLUå‡½æ•°
```python
def relu(z):
    return np.maximum(0, z)

def relu_derivative(z):
    return np.where(z > 0, 1, 0)
```

## ğŸ“ˆ è®­ç»ƒæŠ€å·§

### æƒé‡åˆå§‹åŒ–

**Xavieråˆå§‹åŒ–**ï¼š
```python
def xavier_init(fan_in, fan_out):
    limit = np.sqrt(6 / (fan_in + fan_out))
    return np.random.uniform(-limit, limit, (fan_out, fan_in))
```

### å­¦ä¹ ç‡è¡°å‡

**æŒ‡æ•°è¡°å‡**ï¼š
```python
learning_rate = initial_lr * decay_rate ** epoch
```

### æ¢¯åº¦æ£€æŸ¥

éªŒè¯åå‘ä¼ æ’­çš„æ­£ç¡®æ€§ï¼š
```python
def gradient_check(parameters, gradients, X, Y, epsilon=1e-7):
    for param_name in parameters:
        param = parameters[param_name]
        grad = gradients["d" + param_name]

        # è®¡ç®—æ•°å€¼æ¢¯åº¦
        numerical_grad = np.zeros_like(param)
        for i in range(param.shape[0]):
            for j in range(param.shape[1]):
                param_plus = param.copy()
                param_minus = param.copy()
                param_plus[i,j] += epsilon
                param_minus[i,j] -= epsilon

                loss_plus = compute_loss(param_plus)
                loss_minus = compute_loss(param_minus)
                numerical_grad[i,j] = (loss_plus - loss_minus) / (2 * epsilon)

        # æ¯”è¾ƒæ¢¯åº¦
        diff = np.linalg.norm(grad - numerical_grad) / np.linalg.norm(grad + numerical_grad)
        print(f"{param_name} gradient check: {diff}")
```

## ğŸ”§ è¶…å‚æ•°è°ƒä¼˜

### ç½‘æ ¼æœç´¢

```python
learning_rates = [0.001, 0.01, 0.1]
hidden_sizes = [64, 128, 256]
batch_sizes = [32, 64, 128]

best_accuracy = 0
best_params = {}

for lr in learning_rates:
    for hidden_size in hidden_sizes:
        for batch_size in batch_sizes:
            # è®­ç»ƒæ¨¡å‹
            accuracy = train_model(lr, hidden_size, batch_size)
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_params = {'lr': lr, 'hidden_size': hidden_size, 'batch_size': batch_size}
```

### éšæœºæœç´¢

```python
def random_search(num_trials=100):
    best_accuracy = 0
    best_params = {}

    for _ in range(num_trials):
        # éšæœºé‡‡æ ·è¶…å‚æ•°
        lr = 10 ** np.random.uniform(-5, -1)
        hidden_size = 2 ** np.random.randint(5, 9)
        batch_size = 2 ** np.random.randint(4, 8)

        # è®­ç»ƒå¹¶è¯„ä¼°
        accuracy = train_model(lr, hidden_size, batch_size)
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_params = {'lr': lr, 'hidden_size': hidden_size, 'batch_size': batch_size}

    return best_params
```

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡

### åˆ†ç±»ä»»åŠ¡

**å‡†ç¡®ç‡ (Accuracy)**ï¼š
$$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$$

**ç²¾ç¡®ç‡ (Precision)**ï¼š
$$Precision = \frac{TP}{TP + FP}$$

**å¬å›ç‡ (Recall)**ï¼š
$$Recall = \frac{TP}{TP + FN}$$

**F1åˆ†æ•°**ï¼š
$$F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$$

### å›å½’ä»»åŠ¡

**å‡æ–¹è¯¯å·® (MSE)**ï¼š
$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

**å¹³å‡ç»å¯¹è¯¯å·® (MAE)**ï¼š
$$MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$$

## ğŸ¯ åº”ç”¨å®ä¾‹

### æ‰‹å†™æ•°å­—è¯†åˆ«

```python
# åŠ è½½MNISTæ•°æ®é›†
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

# æ¨¡å‹å®šä¹‰
class MNISTNet(nn.Module):
    def __init__(self):
        super(MNISTNet, self).__init__()
        self.fc1 = nn.Linear(28*28, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)

    def forward(self, x):
        x = x.view(-1, 28*28)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# è®­ç»ƒæ¨¡å‹
model = MNISTNet()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
```

## ğŸ“š å­¦ä¹ èµ„æº

### å´æ©è¾¾è¯¾ç¨‹
- [ç¬¬ä¸€å‘¨ï¼šç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ åŸºç¡€](https://www.coursera.org/learn/neural-networks-deep-learning)

### ç»å…¸è®ºæ–‡
- [A Logical Calculus of the Ideas Immanent in Nervous Activity](https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf) - McCulloch & Pitts (1943)
- [Learning Internal Representations by Error Propagation](https://www.iro.umontreal.ca/~pift6266/A06/refs/backprop.pdf) - Rumelhart et al. (1986)

### åœ¨çº¿èµ„æº
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
- [CS231n: Convolutional Neural Networks](https://cs231n.github.io/)