+++
date = '2025-10-25T00:10:19+08:00'
draft = false
title = 'Transformeræ¶æ„'
comments = true
weight = 4
+++

# Transformeræ¶æ„

Transformeræ˜¯2017å¹´ç”±Googleæå‡ºçš„é©å‘½æ€§æ¶æ„ï¼Œå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œå½»åº•æ”¹å˜äº†æ·±åº¦å­¦ä¹ é¢†åŸŸã€‚æœ¬ç« è¯¦ç»†ä»‹ç»Transformerçš„æ ¸å¿ƒæ¦‚å¿µå’Œå®ç°ç»†èŠ‚ã€‚

## ğŸ¯ æ³¨æ„åŠ›æœºåˆ¶

### åŸºæœ¬æ³¨æ„åŠ›
æ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹åœ¨å¤„ç†åºåˆ—æ—¶åŠ¨æ€åœ°èšç„¦äºç›¸å…³éƒ¨åˆ†ï¼š

**Scaled Dot-Product Attention**ï¼š
$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$

å…¶ä¸­ï¼š
- $Q$ï¼šæŸ¥è¯¢çŸ©é˜µ
- $K$ï¼šé”®çŸ©é˜µ
- $V$ï¼šå€¼çŸ©é˜µ
- $d_k$ï¼šé”®å‘é‡çš„ç»´åº¦

### å¤šå¤´æ³¨æ„åŠ›
å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å¹¶è¡Œè®¡ç®—å¤šä¸ªæ³¨æ„åŠ›ï¼š

**å¤šå¤´è®¡ç®—**ï¼š
$$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$$
$$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$$

## ğŸ—ï¸ Transformeræ¶æ„

### ç¼–ç å™¨ (Encoder)

**ç¼–ç å™¨å±‚ç»“æ„**ï¼š
1. **å¤šå¤´è‡ªæ³¨æ„åŠ›**ï¼š$MultiHeadAttention$
2. **å‰é¦ˆç½‘ç»œ**ï¼š$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$
3. **æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–**

### è§£ç å™¨ (Decoder)

**è§£ç å™¨å±‚ç»“æ„**ï¼š
1. **æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›**ï¼šé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯
2. **å¤šå¤´æ³¨æ„åŠ›**ï¼šå…³æ³¨ç¼–ç å™¨è¾“å‡º
3. **å‰é¦ˆç½‘ç»œ**ï¼šä¸ç¼–ç å™¨ç›¸åŒ
4. **æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–**

## ğŸ“ ä½ç½®ç¼–ç 

### é—®é¢˜
Transformeræ²¡æœ‰å¾ªç¯æˆ–å·ç§¯ç»“æ„ï¼Œæ— æ³•æ„ŸçŸ¥åºåˆ—ä½ç½®ã€‚

### è§£å†³æ–¹æ¡ˆ
**æ­£å¼¦ä½ç½®ç¼–ç **ï¼š
$$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$$
$$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})$$

**ç›¸å¯¹ä½ç½®ç¼–ç **ï¼šè€ƒè™‘ç›¸å¯¹ä½ç½®å…³ç³»

## ğŸ­ è‡ªæ³¨æ„åŠ›æœºåˆ¶

### è‡ªæ³¨æ„åŠ›è®¡ç®—

**æŸ¥è¯¢ã€é”®ã€å€¼**ï¼š
- $Q = XW^Q$
- $K = XW^K$
- $V = XW^V$

**æ³¨æ„åŠ›åˆ†æ•°**ï¼š
$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$

### æ©ç æœºåˆ¶

**å¡«å……æ©ç **ï¼šå¿½ç•¥å¡«å……ä½ç½®
**åºåˆ—æ©ç **ï¼šé˜²æ­¢è§£ç å™¨çœ‹åˆ°æœªæ¥ä¿¡æ¯

## ğŸš€ ç¼–ç¨‹å®ç°

### PyTorchå®ç°

```python
import torch
import torch.nn as nn
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention = torch.softmax(scores, dim=-1)
        return torch.matmul(attention, V)

    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)

        # çº¿æ€§å˜æ¢
        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

        # æ³¨æ„åŠ›è®¡ç®—
        output = self.scaled_dot_product_attention(Q, K, V, mask)

        # æ‹¼æ¥å¤šå¤´è¾“å‡º
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)

        return self.W_o(output)

class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff):
        super(PositionwiseFeedForward, self).__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()

    def forward(self, x):
        return self.linear2(self.relu(self.linear1(x)))

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:x.size(0), :]
```

### å®Œæ•´Transformerç¼–ç å™¨

```python
class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(TransformerEncoderLayer, self).__init__()

        self.self_attn = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # è‡ªæ³¨æ„åŠ›
        attn_output = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout1(attn_output))

        # å‰é¦ˆç½‘ç»œ
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout2(ff_output))

        return x

class TransformerEncoder(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len):
        super(TransformerEncoder, self).__init__()

        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_len)

        self.layers = nn.ModuleList([
            TransformerEncoderLayer(d_model, num_heads, d_ff)
            for _ in range(num_layers)
        ])

        self.norm = nn.LayerNorm(d_model)

    def forward(self, x, mask=None):
        # è¯åµŒå…¥ + ä½ç½®ç¼–ç 
        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)
        x = self.pos_encoding(x)

        # ç¼–ç å™¨å±‚
        for layer in self.layers:
            x = layer(x, mask)

        return self.norm(x)
```

## ğŸ¨ æ³¨æ„åŠ›å¯è§†åŒ–

### æ³¨æ„åŠ›æƒé‡
```python
def plot_attention(attention_weights, tokens):
    plt.figure(figsize=(10, 10))
    plt.imshow(attention_weights, cmap='viridis')
    plt.xticks(range(len(tokens)), tokens, rotation=45)
    plt.yticks(range(len(tokens)), tokens)
    plt.colorbar()
    plt.show()
```

### å¤šå¤´æ³¨æ„åŠ›
```python
def plot_multihead_attention(attention_weights, tokens, num_heads):
    fig, axes = plt.subplots(1, num_heads, figsize=(15, 5))

    for i in range(num_heads):
        axes[i].imshow(attention_weights[i], cmap='viridis')
        axes[i].set_xticks(range(len(tokens)))
        axes[i].set_xticklabels(tokens, rotation=45)
        axes[i].set_title(f'Head {i+1}')

    plt.tight_layout()
    plt.show()
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### æ¨¡å‹å¹¶è¡Œ
- **å¼ é‡å¹¶è¡Œ**ï¼šåœ¨å¤šä¸ªGPUä¸Šåˆ†å‰²æ¨¡å‹å‚æ•°
- **æµæ°´çº¿å¹¶è¡Œ**ï¼šä¸åŒGPUå¤„ç†ä¸åŒå±‚
- **æ•°æ®å¹¶è¡Œ**ï¼šæ¯ä¸ªGPUå¤„ç†ä¸åŒæ‰¹æ¬¡

### å†…å­˜ä¼˜åŒ–
- **æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼šå‡å°‘æ¿€æ´»å€¼å­˜å‚¨
- **æ··åˆç²¾åº¦è®­ç»ƒ**ï¼šä½¿ç”¨float16å‡å°‘å†…å­˜
- **æ¨¡å‹åˆ†ç‰‡**ï¼šæŒ‰éœ€åŠ è½½æ¨¡å‹å‚æ•°

## ğŸ¯ åº”ç”¨é¢†åŸŸ

### è‡ªç„¶è¯­è¨€å¤„ç†
- **æœºå™¨ç¿»è¯‘**ï¼šGoogle Translate, DeepL
- **æ–‡æœ¬ç”Ÿæˆ**ï¼šGPTç³»åˆ—æ¨¡å‹
- **æ–‡æœ¬æ‘˜è¦**ï¼šè‡ªåŠ¨æ‘˜è¦ç”Ÿæˆ
- **é—®ç­”ç³»ç»Ÿ**ï¼šæ™ºèƒ½é—®ç­”æœºå™¨äºº

### è®¡ç®—æœºè§†è§‰
- **å›¾åƒæè¿°**ï¼šä¸ºå›¾åƒç”Ÿæˆæ–‡å­—æè¿°
- **è§†è§‰é—®ç­”**ï¼šåŸºäºå›¾åƒçš„é—®ç­”
- **å›¾åƒç”Ÿæˆ**ï¼šDALL-E, Stable Diffusion

### è¯­éŸ³å¤„ç†
- **è¯­éŸ³è¯†åˆ«**ï¼šç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ«
- **è¯­éŸ³åˆæˆ**ï¼šTTSç³»ç»Ÿ
- **è¯­éŸ³ç¿»è¯‘**ï¼šå®æ—¶è¯­éŸ³ç¿»è¯‘

### å¤šæ¨¡æ€å­¦ä¹ 
- **å›¾åƒ-æ–‡æœ¬**ï¼šCLIPæ¨¡å‹
- **è§†é¢‘ç†è§£**ï¼šè§†é¢‘é—®ç­”ç³»ç»Ÿ
- **è·¨æ¨¡æ€ç”Ÿæˆ**ï¼šæ–‡æœ¬ç”Ÿæˆå›¾åƒ

## ğŸ”§ å®ç”¨æŠ€å·§

### å­¦ä¹ ç‡è°ƒåº¦
```python
# Warmup + ä½™å¼¦é€€ç«
def get_lr_scheduler(optimizer, warmup_steps, total_steps):
    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        else:
            return 0.5 * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps)))
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
```

### æ ‡ç­¾å¹³æ»‘
```python
class LabelSmoothingLoss(nn.Module):
    def __init__(self, size, padding_idx=0, smoothing=0.1):
        super(LabelSmoothingLoss, self).__init__()
        self.criterion = nn.KLDivLoss(reduction='sum')
        self.padding_idx = padding_idx
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.size = size
        self.true_dist = None

    def forward(self, x, target):
        assert x.size(1) == self.size
        true_dist = x.data.clone()
        true_dist.fill_(self.smoothing / (self.size - 2))
        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        true_dist[:, self.padding_idx] = 0
        mask = torch.nonzero(target.data == self.padding_idx)
        if mask.dim() > 0:
            true_dist.index_fill_(0, mask.squeeze(), 0.0)
        self.true_dist = true_dist
        return self.criterion(x, true_dist)
```

### Beam Search
```python
def beam_search(model, src, beam_size=5, max_len=50):
    model.eval()

    # ç¼–ç æºåºåˆ—
    with torch.no_grad():
        encoder_output = model.encode(src)

    # åˆå§‹åŒ–
    candidates = [(0, [model.bos_idx])]
    finished = []

    for step in range(max_len):
        new_candidates = []

        for score, sequence in candidates:
            if sequence[-1] == model.eos_idx:
                finished.append((score, sequence))
                continue

            # è§£ç 
            tgt = torch.LongTensor(sequence).unsqueeze(0).to(device)
            with torch.no_grad():
                output = model.decode(encoder_output, tgt)

            # å–top-ké¢„æµ‹
            probs = torch.softmax(output[:, -1], dim=-1)
            top_probs, top_indices = torch.topk(probs, beam_size)

            for i in range(beam_size):
                new_score = score + torch.log(top_probs[0][i])
                new_sequence = sequence + [top_indices[0][i].item()]
                new_candidates.append((new_score, new_sequence))

        # ä¿ç•™top-kå€™é€‰
        candidates = sorted(new_candidates, key=lambda x: x[0], reverse=True)[:beam_size]

    # è¿”å›æœ€ä½³åºåˆ—
    best_sequence = max(finished + candidates, key=lambda x: x[0])[1]
    return best_sequence
```

## ğŸ“š å­¦ä¹ èµ„æº

### å´æ©è¾¾è¯¾ç¨‹
- [ç¬¬äº”å‘¨ï¼šåºåˆ—æ¨¡å‹](https://www.coursera.org/learn/sequence-models)

### ç»å…¸è®ºæ–‡
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al. (2017)
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) - Devlin et al. (2018)
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) - GPT-3

### åœ¨çº¿èµ„æº
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Transformerä»£ç å®ç°](https://github.com/huggingface/transformers)
- [æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–](https://transformer-viz.com/)

---
*æœ€è¿‘æ›´æ–°: {{ .Lastmod.Format "2006-01-02" }}*