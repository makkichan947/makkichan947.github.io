+++
date = '2025-10-25T10:00:00+08:00'
draft = false
title = 'å¼ºåŒ–å­¦ä¹ '
comments = true
weight = 7
+++

# å¼ºåŒ–å­¦ä¹ 

å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œé€šè¿‡æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œåœ¨æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆå°±ã€‚

## ğŸ¯ å¼ºåŒ–å­¦ä¹ åŸºç¡€

### é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)

å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ¡†æ¶æ˜¯é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼š

**MDPåŒ…å«çš„å…ƒç´ **ï¼š
- **çŠ¶æ€ç©ºé—´ S**ï¼šç¯å¢ƒå¯èƒ½çš„æ‰€æœ‰çŠ¶æ€
- **åŠ¨ä½œç©ºé—´ A**ï¼šæ™ºèƒ½ä½“å¯ä»¥æ‰§è¡Œçš„æ‰€æœ‰åŠ¨ä½œ
- **å¥–åŠ±å‡½æ•° R(s,a)**ï¼šä»çŠ¶æ€sæ‰§è¡ŒåŠ¨ä½œaåè·å¾—çš„å¥–åŠ±
- **çŠ¶æ€è½¬ç§»æ¦‚ç‡ P(s'|s,a)**ï¼šä»çŠ¶æ€sæ‰§è¡ŒåŠ¨ä½œaåè½¬ç§»åˆ°çŠ¶æ€s'çš„æ¦‚ç‡
- **æŠ˜æ‰£å› å­ Î³**ï¼šæœªæ¥å¥–åŠ±çš„è¡°å‡å› å­

**MDPçš„ç›®æ ‡**ï¼š
æ‰¾åˆ°ä¸€ä¸ªç­–ç•¥Ï€(a|s)ï¼Œä½¿å¾—ä»åˆå§‹çŠ¶æ€å¼€å§‹çš„ç´¯ç§¯æŠ˜æ‰£å¥–åŠ±æœ€å¤§åŒ–ï¼š
$$\pi^* = \arg\max_\pi \mathbb{E}[\sum_{t=0}^\infty \gamma^t R(s_t, a_t)]$$

### å¼ºåŒ–å­¦ä¹ ç»„æˆ

**æ™ºèƒ½ä½“ (Agent)**ï¼š
- è§‚å¯Ÿç¯å¢ƒçŠ¶æ€
- é€‰æ‹©å’Œæ‰§è¡ŒåŠ¨ä½œ
- æ¥æ”¶å¥–åŠ±ä¿¡å·
- å­¦ä¹ æœ€ä¼˜ç­–ç•¥

**ç¯å¢ƒ (Environment)**ï¼š
- æ¥æ”¶æ™ºèƒ½ä½“çš„åŠ¨ä½œ
- è½¬ç§»åˆ°æ–°çŠ¶æ€
- æä¾›å¥–åŠ±ä¿¡å·
- æä¾›çŠ¶æ€è§‚å¯Ÿ

## ğŸ—ï¸ å¼ºåŒ–å­¦ä¹ ç®—æ³•

### Q-Learningç®—æ³•

**Q-Learning**æ˜¯æœ€ç»å…¸çš„åŸºäºå€¼å‡½æ•°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼š

```python
import numpy as np
import random

class QLearningAgent:
    def __init__(self, state_size, action_size, learning_rate=0.1, gamma=0.95, epsilon=1.0):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.gamma = gamma  # æŠ˜æ‰£å› å­
        self.epsilon = epsilon  # æ¢ç´¢ç‡
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995

        # Qè¡¨ï¼šçŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•°
        self.q_table = np.zeros((state_size, action_size))

    def choose_action(self, state):
        """Îµ-è´ªå¿ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        if random.uniform(0, 1) < self.epsilon:
            return random.randint(0, self.action_size - 1)  # æ¢ç´¢
        else:
            return np.argmax(self.q_table[state])  # åˆ©ç”¨

    def learn(self, state, action, reward, next_state, done):
        """Q-Learningæ›´æ–°"""
        # å½“å‰Qå€¼
        current_q = self.q_table[state, action]

        # ç›®æ ‡Qå€¼
        if done:
            target_q = reward
        else:
            target_q = reward + self.gamma * np.max(self.q_table[next_state])

        # Qå€¼æ›´æ–°
        self.q_table[state, action] += self.learning_rate * (target_q - current_q)

        # è¡°å‡æ¢ç´¢ç‡
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# ä½¿ç”¨Q-Learning
env = gym.make('FrozenLake-v1')
state_size = env.observation_space.n
action_size = env.action_space.n

agent = QLearningAgent(state_size, action_size)

# è®­ç»ƒ
episodes = 1000
for episode in range(episodes):
    state = env.reset()
    done = False
    total_reward = 0

    while not done:
        action = agent.choose_action(state)
        next_state, reward, done, _ = env.step(action)

        agent.learn(state, action, reward, next_state, done)

        state = next_state
        total_reward += reward

    if episode % 100 == 0:
        print(f"Episode: {episode}, Total Reward: {total_reward}")
```

### æ·±åº¦Qç½‘ç»œ (DQN)

**DQN**ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼Qå‡½æ•°ï¼š

```python
import tensorflow as tf
import numpy as np
import random
from collections import deque

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.batch_size = 32

        self.model = self._build_model()
        self.target_model = self._build_model()
        self.update_target_model()

    def _build_model(self):
        """æ„å»ºDQNç½‘ç»œ"""
        model = tf.keras.Sequential([
            tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'),
            tf.keras.layers.Dense(24, activation='relu'),
            tf.keras.layers.Dense(self.action_size, activation='linear')
        ])

        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),
            loss='mse'
        )

        return model

    def update_target_model(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_model.set_weights(self.model.get_weights())

    def remember(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.memory.append((state, action, reward, next_state, done))

    def choose_action(self, state):
        """Îµ-è´ªå¿ƒç­–ç•¥"""
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)

        act_values = self.model.predict(state, verbose=0)
        return np.argmax(act_values[0])

    def replay(self):
        """ç»éªŒå›æ”¾"""
        if len(self.memory) < self.batch_size:
            return

        minibatch = random.sample(self.memory, self.batch_size)

        for state, action, reward, next_state, done in minibatch:
            target = self.model.predict(state, verbose=0)

            if done:
                target[0][action] = reward
            else:
                t = self.target_model.predict(next_state, verbose=0)
                target[0][action] = reward + self.gamma * np.amax(t[0])

            self.model.fit(state, target, epochs=1, verbose=0)

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# ä½¿ç”¨DQN
import gym

env = gym.make('CartPole-v1')
state_size = env.observation_space.shape[0]
action_size = env.action_space.n

agent = DQNAgent(state_size, action_size)

# è®­ç»ƒ
episodes = 1000
for e in range(episodes):
    state = env.reset()
    state = np.reshape(state, [1, state_size])
    total_reward = 0

    for time in range(500):
        action = agent.choose_action(state)
        next_state, reward, done, _ = env.step(action)
        next_state = np.reshape(next_state, [1, state_size])

        agent.remember(state, action, reward, next_state, done)
        agent.replay()

        state = next_state
        total_reward += reward

        if done:
            agent.update_target_model()
            print(f"Episode: {e}, Score: {total_reward}")
            break
```

### ç­–ç•¥æ¢¯åº¦æ–¹æ³•

**REINFORCEç®—æ³•**ï¼š

```python
class REINFORCEAgent:
    def __init__(self, state_size, action_size, learning_rate=0.01, gamma=0.99):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.gamma = gamma

        self.model = self._build_model()
        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)

    def _build_model(self):
        """æ„å»ºç­–ç•¥ç½‘ç»œ"""
        model = tf.keras.Sequential([
            tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'),
            tf.keras.layers.Dense(24, activation='relu'),
            tf.keras.layers.Dense(self.action_size, activation='softmax')
        ])
        return model

    def choose_action(self, state):
        """æ ¹æ®ç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        state = np.reshape(state, [1, self.state_size])
        probs = self.model.predict(state, verbose=0)[0]
        action = np.random.choice(self.action_size, p=probs)
        return action, probs

    def learn(self, states, actions, rewards):
        """REINFORCEå­¦ä¹ """
        discounted_rewards = self._discount_rewards(rewards)

        with tf.GradientTape() as tape:
            # è®¡ç®—ç­–ç•¥æ¦‚ç‡
            states = np.array(states)
            probs = self.model(states)

            # é€‰æ‹©å¯¹åº”åŠ¨ä½œçš„æ¦‚ç‡
            action_probs = tf.gather(probs, actions, axis=1, batch_dims=1)

            # è®¡ç®—æŸå¤±ï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰
            loss = -tf.reduce_mean(tf.math.log(action_probs) * discounted_rewards)

        # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°
        gradients = tape.gradient(loss, self.model.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))

        return loss.numpy()

    def _discount_rewards(self, rewards):
        """è®¡ç®—æŠ˜æ‰£å¥–åŠ±"""
        discounted_rewards = np.zeros_like(rewards, dtype=np.float32)
        running_sum = 0

        for i in reversed(range(len(rewards))):
            running_sum = running_sum * self.gamma + rewards[i]
            discounted_rewards[i] = running_sum

        # æ ‡å‡†åŒ–
        discounted_rewards -= np.mean(discounted_rewards)
        discounted_rewards /= np.std(discounted_rewards) + 1e-8

        return discounted_rewards

# ä½¿ç”¨REINFORCE
agent = REINFORCEAgent(state_size, action_size)

# è®­ç»ƒ
episodes = 1000
for episode in range(episodes):
    states = []
    actions = []
    rewards = []

    state = env.reset()
    done = False

    while not done:
        action, prob = agent.choose_action(state)
        next_state, reward, done, _ = env.step(action)

        states.append(state)
        actions.append(action)
        rewards.append(reward)

        state = next_state

    loss = agent.learn(states, actions, rewards)

    if episode % 100 == 0:
        print(f"Episode: {episode}, Loss: {loss:.4f}")
```

## ğŸ® é«˜çº§å¼ºåŒ–å­¦ä¹ ç®—æ³•

### Actor-Criticæ–¹æ³•

```python
class ActorCriticAgent:
    def __init__(self, state_size, action_size, learning_rate=0.001):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.gamma = 0.99

        # Actorç½‘ç»œï¼ˆç­–ç•¥ï¼‰
        self.actor = self._build_actor()
        # Criticç½‘ç»œï¼ˆä»·å€¼ï¼‰
        self.critic = self._build_critic()

        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)
        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)

    def _build_actor(self):
        """æ„å»ºActorç½‘ç»œ"""
        inputs = tf.keras.Input(shape=(self.state_size,))
        x = tf.keras.layers.Dense(24, activation='relu')(inputs)
        x = tf.keras.layers.Dense(24, activation='relu')(x)
        outputs = tf.keras.layers.Dense(self.action_size, activation='softmax')(x)
        return tf.keras.Model(inputs=inputs, outputs=outputs)

    def _build_critic(self):
        """æ„å»ºCriticç½‘ç»œ"""
        inputs = tf.keras.Input(shape=(self.state_size,))
        x = tf.keras.layers.Dense(24, activation='relu')(inputs)
        x = tf.keras.layers.Dense(24, activation='relu')(x)
        outputs = tf.keras.layers.Dense(1)(x)
        return tf.keras.Model(inputs=inputs, outputs=outputs)

    def choose_action(self, state):
        """é€‰æ‹©åŠ¨ä½œ"""
        state = np.reshape(state, [1, self.state_size])
        probs = self.actor.predict(state, verbose=0)[0]
        action = np.random.choice(self.action_size, p=probs)
        return action, probs

    def learn(self, state, action, reward, next_state, done):
        """Actor-Criticå­¦ä¹ """
        state = np.reshape(state, [1, self.state_size])
        next_state = np.reshape(next_state, [1, self.state_size])

        # Criticæ›´æ–°
        with tf.GradientTape() as tape:
            value = self.critic(state)
            next_value = self.critic(next_state) if not done else 0

            target = reward + self.gamma * next_value
            critic_loss = tf.reduce_mean(tf.square(target - value))

        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)
        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))

        # Actoræ›´æ–°
        with tf.GradientTape() as tape:
            probs = self.actor(state)
            action_prob = probs[0][action]

            advantage = target - value
            actor_loss = -tf.math.log(action_prob) * advantage

        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)
        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))

        return critic_loss.numpy(), actor_loss.numpy()
```

### è¿‘ç«¯ç­–ç•¥ä¼˜åŒ– (PPO)

```python
class PPOAgent:
    def __init__(self, state_size, action_size, clip_ratio=0.2):
        self.state_size = state_size
        self.action_size = action_size
        self.clip_ratio = clip_ratio
        self.gamma = 0.99
        self.learning_rate = 0.0003

        self.actor = self._build_actor()
        self.critic = self._build_critic()

        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)
        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)

    def _build_actor(self):
        """æ„å»ºActorç½‘ç»œ"""
        inputs = tf.keras.Input(shape=(self.state_size,))
        x = tf.keras.layers.Dense(64, activation='relu')(inputs)
        x = tf.keras.layers.Dense(64, activation='relu')(x)
        outputs = tf.keras.layers.Dense(self.action_size, activation='softmax')(x)
        return tf.keras.Model(inputs=inputs, outputs=outputs)

    def _build_critic(self):
        """æ„å»ºCriticç½‘ç»œ"""
        inputs = tf.keras.Input(shape=(self.state_size,))
        x = tf.keras.layers.Dense(64, activation='relu')(inputs)
        x = tf.keras.layers.Dense(64, activation='relu')(x)
        outputs = tf.keras.layers.Dense(1)(x)
        return tf.keras.Model(inputs=inputs, outputs=outputs)

    def get_action(self, state):
        """è·å–åŠ¨ä½œå’Œæ¦‚ç‡"""
        state = np.reshape(state, [1, self.state_size])
        probs = self.actor.predict(state, verbose=0)[0]
        action = np.random.choice(self.action_size, p=probs)
        return action, probs[action]

    def compute_advantages(self, rewards, values, next_values, dones):
        """è®¡ç®—ä¼˜åŠ¿å‡½æ•°"""
        advantages = np.zeros_like(rewards)
        last_gae_lam = 0

        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_non_terminal = 1.0 - dones[t]
                next_values = next_values[t]
            else:
                next_non_terminal = 1.0 - dones[t]
                next_values = values[t + 1]

            delta = rewards[t] + self.gamma * next_values * next_non_terminal - values[t]
            advantages[t] = last_gae_lam = delta + self.gamma * 0.95 * next_non_terminal * last_gae_lam

        return advantages

    def train(self, states, actions, old_probs, advantages, returns):
        """PPOè®­ç»ƒ"""
        with tf.GradientTape() as tape:
            probs = self.actor(states)
            values = self.critic(states)

            # è®¡ç®—ç­–ç•¥æŸå¤±
            new_probs = tf.gather(probs, actions, axis=1, batch_dims=1)
            ratio = new_probs / old_probs
            clipped_ratio = tf.clip_by_value(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio)

            policy_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, clipped_ratio * advantages))

            # è®¡ç®—ä»·å€¼æŸå¤±
            value_loss = tf.reduce_mean(tf.square(returns - values))

            # æ€»æŸå¤±
            loss = policy_loss + 0.5 * value_loss

        # æ›´æ–°Actor
        actor_grads = tape.gradient(policy_loss, self.actor.trainable_variables)
        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))

        # æ›´æ–°Critic
        critic_grads = tape.gradient(value_loss, self.critic.trainable_variables)
        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))

        return loss.numpy()
```

## ğŸ¯ åº”ç”¨é¢†åŸŸ

### æ¸¸æˆAI

**AlphaGoçš„å¯å‘**ï¼š
- **è’™ç‰¹å¡æ´›æ ‘æœç´¢ (MCTS)**ï¼šç»“åˆæ·±åº¦å­¦ä¹ å’Œæœç´¢
- **è‡ªæˆ‘å¯¹å¼ˆ**ï¼šé€šè¿‡è‡ªæˆ‘å¯¹å¼ˆæå‡ç­–ç•¥
- **ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œ**ï¼šåˆ†åˆ«é¢„æµ‹èµ°æ³•å’Œå±€é¢ä»·å€¼

```python
# ç®€åŒ–ç‰ˆAlphaGoç­–ç•¥
class SimpleAlphaGo:
    def __init__(self, board_size=9):
        self.board_size = board_size
        self.policy_network = self._build_policy_network()
        self.value_network = self._build_value_network()

    def _build_policy_network(self):
        """ç­–ç•¥ç½‘ç»œ"""
        inputs = tf.keras.Input(shape=(self.board_size, self.board_size, 1))
        x = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)
        x = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu')(x)
        x = tf.keras.layers.Flatten()(x)
        outputs = tf.keras.layers.Dense(self.board_size * self.board_size, activation='softmax')(x)
        return tf.keras.Model(inputs=inputs, outputs=outputs)

    def _build_value_network(self):
        """ä»·å€¼ç½‘ç»œ"""
        inputs = tf.keras.Input(shape=(self.board_size, self.board_size, 1))
        x = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)
        x = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu')(x)
        x = tf.keras.layers.Flatten()(x)
        x = tf.keras.layers.Dense(128, activation='relu')(x)
        outputs = tf.keras.layers.Dense(1, activation='tanh')(x)
        return tf.keras.Model(inputs=inputs, outputs=outputs)
```

### æœºå™¨äººæ§åˆ¶

**è¿ç»­æ§åˆ¶ä»»åŠ¡**ï¼š
- **DDPG (Deep Deterministic Policy Gradient)**ï¼šå¤„ç†è¿ç»­åŠ¨ä½œç©ºé—´
- **TD3 (Twin Delayed DDPG)**ï¼šæ”¹è¿›çš„DDPGç®—æ³•
- **SAC (Soft Actor-Critic)**ï¼šæœ€å¤§åŒ–ç†µçš„å¼ºåŒ–å­¦ä¹ 

```python
class DDPGAgent:
    def __init__(self, state_size, action_size, action_low, action_high):
        self.state_size = state_size
        self.action_size = action_size
        self.action_low = action_low
        self.action_high = action_high

        # Actorç½‘ç»œï¼ˆç¡®å®šæ€§ç­–ç•¥ï¼‰
        self.actor = self._build_actor()
        # Criticç½‘ç»œï¼ˆQå‡½æ•°ï¼‰
        self.critic = self._build_critic()
        # ç›®æ ‡ç½‘ç»œ
        self.target_actor = self._build_actor()
        self.target_critic = self._build_critic()

        # å¤åˆ¶æƒé‡åˆ°ç›®æ ‡ç½‘ç»œ
        self.update_target_networks(1.0)

    def _build_actor(self):
        """æ„å»ºActorç½‘ç»œ"""
        inputs = tf.keras.Input(shape=(self.state_size,))
        x = tf.keras.layers.Dense(400, activation='relu')(inputs)
        x = tf.keras.layers.Dense(300, activation='relu')(x)
        outputs = tf.keras.layers.Dense(self.action_size, activation='tanh')(x)

        # ç¼©æ”¾åˆ°åŠ¨ä½œèŒƒå›´
        model = tf.keras.Model(inputs=inputs, outputs=outputs)
        return model

    def _build_critic(self):
        """æ„å»ºCriticç½‘ç»œ"""
        state_inputs = tf.keras.Input(shape=(self.state_size,))
        action_inputs = tf.keras.Input(shape=(self.action_size,))

        # çŠ¶æ€è·¯å¾„
        state_out = tf.keras.layers.Dense(400, activation='relu')(state_inputs)
        state_out = tf.keras.layers.Dense(300, activation='relu')(state_out)

        # åŠ¨ä½œè·¯å¾„
        action_out = tf.keras.layers.Dense(300, activation='relu')(action_inputs)

        # åˆå¹¶
        merged = tf.keras.layers.Add()([state_out, action_out])
        outputs = tf.keras.layers.Dense(1)(merged)

        return tf.keras.Model(inputs=[state_inputs, action_inputs], outputs=outputs)
```

### è‡ªåŠ¨é©¾é©¶

**è‡ªåŠ¨é©¾é©¶ä¸­çš„å¼ºåŒ–å­¦ä¹ **ï¼š
- **è·¯å¾„è§„åˆ’**ï¼šå­¦ä¹ æœ€ä¼˜é©¾é©¶è·¯å¾„
- **è¡Œä¸ºå†³ç­–**ï¼šåœ¨å¤æ‚äº¤é€šç¯å¢ƒä¸­å†³ç­–
- **æ§åˆ¶ä¼˜åŒ–**ï¼šä¼˜åŒ–è½¦è¾†æ§åˆ¶å‚æ•°

```python
class AutonomousDrivingAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size

        # æ„ŸçŸ¥æ¨¡å—
        self.perception_model = self._build_perception_model()
        # å†³ç­–æ¨¡å—
        self.decision_model = self._build_decision_model()
        # æ§åˆ¶æ¨¡å—
        self.control_model = self._build_control_model()

    def _build_perception_model(self):
        """æ„ŸçŸ¥æ¨¡å—ï¼šå¤„ç†ä¼ æ„Ÿå™¨æ•°æ®"""
        inputs = tf.keras.Input(shape=(self.state_size,))
        x = tf.keras.layers.Dense(256, activation='relu')(inputs)
        x = tf.keras.layers.Dense(128, activation='relu')(x)
        # æ£€æµ‹å…¶ä»–è½¦è¾†ã€è¡Œäººã€äº¤é€šæ ‡å¿—ç­‰
        outputs = {
            'vehicles': tf.keras.layers.Dense(10, activation='sigmoid')(x),
            'pedestrians': tf.keras.layers.Dense(5, activation='sigmoid')(x),
            'traffic_lights': tf.keras.layers.Dense(3, activation='softmax')(x)
        }
        return tf.keras.Model(inputs=inputs, outputs=outputs)

    def _build_decision_model(self):
        """å†³ç­–æ¨¡å—ï¼šåšå‡ºé©¾é©¶å†³ç­–"""
        inputs = tf.keras.Input(shape=(128,))
        x = tf.keras.layers.Dense(64, activation='relu')(inputs)
        x = tf.keras.layers.Dense(32, activation='relu')(x)
        outputs = tf.keras.layers.Dense(self.action_size, activation='softmax')(x)
        return tf.keras.Model(inputs=inputs, outputs=outputs)
```

## ğŸ“Š è¯„ä¼°å’Œè°ƒè¯•

### è¯„ä¼°æŒ‡æ ‡

```python
def evaluate_agent(agent, env, episodes=100):
    """è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½"""
    total_rewards = []
    episode_lengths = []

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        steps = 0

        while True:
            action = agent.choose_action(state)
            next_state, reward, done, _ = env.step(action)

            total_reward += reward
            steps += 1

            if done:
                break

            state = next_state

        total_rewards.append(total_reward)
        episode_lengths.append(steps)

    return {
        'mean_reward': np.mean(total_rewards),
        'std_reward': np.std(total_rewards),
        'mean_length': np.mean(episode_lengths),
        'success_rate': np.mean([r > threshold for r in total_rewards])
    }

# ä½¿ç”¨è¯„ä¼°å‡½æ•°
results = evaluate_agent(agent, env, episodes=100)
print(f"å¹³å‡å¥–åŠ±: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}")
print(f"æˆåŠŸç‡: {results['success_rate']:.2%}")
```

### è°ƒè¯•æŠ€å·§

```python
class DebuggingAgent:
    def __init__(self, agent):
        self.agent = agent
        self.episode_rewards = []
        self.q_values = []
        self.gradients = []

    def debug_episode(self, env):
        """è°ƒè¯•å•ä¸ªepisode"""
        state = env.reset()
        episode_reward = 0
        episode_q_values = []

        while True:
            # è®°å½•Qå€¼
            if hasattr(self.agent, 'model'):
                q_values = self.agent.model.predict(np.reshape(state, [1, -1]), verbose=0)[0]
                episode_q_values.append(np.max(q_values))

            action = self.agent.choose_action(state)
            next_state, reward, done, _ = env.step(action)

            episode_reward += reward

            if done:
                break

            state = next_state

        self.episode_rewards.append(episode_reward)
        self.q_values.append(episode_q_values)

        return episode_reward

    def plot_debugging_info(self):
        """ç»˜åˆ¶è°ƒè¯•ä¿¡æ¯"""
        plt.figure(figsize=(12, 4))

        # å¥–åŠ±æ›²çº¿
        plt.subplot(1, 3, 1)
        plt.plot(self.episode_rewards)
        plt.xlabel('Episode')
        plt.ylabel('Reward')
        plt.title('Episode Rewards')

        # Qå€¼æ›²çº¿
        plt.subplot(1, 3, 2)
        for i, q_values in enumerate(self.q_values[-10:]):  # æœ€è¿‘10ä¸ªepisode
            plt.plot(q_values, alpha=0.3, label=f'Episode {len(self.episode_rewards)-10+i}')
        plt.xlabel('Step')
        plt.ylabel('Max Q-value')
        plt.title('Q-values')
        plt.legend()

        # å¥–åŠ±åˆ†å¸ƒ
        plt.subplot(1, 3, 3)
        plt.hist(self.episode_rewards, bins=20)
        plt.xlabel('Reward')
        plt.ylabel('Frequency')
        plt.title('Reward Distribution')

        plt.tight_layout()
        plt.show()

# ä½¿ç”¨è°ƒè¯•å·¥å…·
debug_agent = DebuggingAgent(agent)

for episode in range(100):
    reward = debug_agent.debug_episode(env)

debug_agent.plot_debugging_info()
```

## ğŸ“š å­¦ä¹ èµ„æº

### ç»å…¸è®ºæ–‡
- [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602) - DQN
- [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236) - DQN Nature
- [Mastering the game of Go with deep neural networks](https://www.nature.com/articles/nature16961) - AlphaGo
- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) - PPO

### åœ¨çº¿èµ„æº
- [OpenAI Gym](https://gym.openai.com/) - å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
- [Stable Baselines](https://stable-baselines.readthedocs.io/) - å¼ºåŒ–å­¦ä¹ ç®—æ³•åº“
- [RLlib](https://ray.readthedocs.io/en/latest/rllib.html) - Rayä¸­çš„å¼ºåŒ–å­¦ä¹ åº“

### å´æ©è¾¾è¯¾ç¨‹
- æ·±åº¦å­¦ä¹ è¯¾ç¨‹ä¸­å…³äºå¼ºåŒ–å­¦ä¹ çš„éƒ¨åˆ†

## ğŸ¯ å®é™…é¡¹ç›®

### æ™ºèƒ½ä½“è®­ç»ƒæ¡†æ¶

```python
class RLTrainingFramework:
    def __init__(self, env_name, agent_class, config):
        self.env_name = env_name
        self.agent_class = agent_class
        self.config = config

        self.env = gym.make(env_name)
        self.agent = agent_class(self.env.observation_space.shape[0],
                                self.env.action_space.n, **config)

    def train(self, num_episodes, eval_interval=100):
        """è®­ç»ƒæ™ºèƒ½ä½“"""
        best_reward = -np.inf
        rewards_history = []

        for episode in range(num_episodes):
            # è®­ç»ƒepisode
            episode_reward = self._train_episode()

            rewards_history.append(episode_reward)

            # å®šæœŸè¯„ä¼°
            if episode % eval_interval == 0:
                eval_reward = self._evaluate_agent()
                print(f"Episode {episode}: Train Reward = {episode_reward:.2f}, "
                      f"Eval Reward = {eval_reward:.2f}")

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if eval_reward > best_reward:
                    best_reward = eval_reward
                    self._save_model(f"best_model_{episode}")

        return rewards_history

    def _train_episode(self):
        """è®­ç»ƒå•ä¸ªepisode"""
        state = self.env.reset()
        total_reward = 0
        done = False

        while not done:
            action = self.agent.choose_action(state)
            next_state, reward, done, _ = self.env.step(action)

            self.agent.learn(state, action, reward, next_state, done)

            state = next_state
            total_reward += reward

        return total_reward

    def _evaluate_agent(self, episodes=10):
        """è¯„ä¼°æ™ºèƒ½ä½“"""
        total_reward = 0

        for _ in range(episodes):
            state = self.env.reset()
            episode_reward = 0
            done = False

            while not done:
                action = self.agent.choose_action(state)
                next_state, reward, done, _ = self.env.step(action)

                state = next_state
                episode_reward += reward

            total_reward += episode_reward

        return total_reward / episodes

    def _save_model(self, filename):
        """ä¿å­˜æ¨¡å‹"""
        # å®ç°æ¨¡å‹ä¿å­˜é€»è¾‘
        pass

# ä½¿ç”¨è®­ç»ƒæ¡†æ¶
config = {
    'learning_rate': 0.001,
    'gamma': 0.99,
    'epsilon': 1.0,
    'epsilon_decay': 0.995
}

framework = RLTrainingFramework('CartPole-v1', DQNAgent, config)
rewards = framework.train(num_episodes=1000)
```

---

*æœ€è¿‘æ›´æ–°: {{ .Lastmod.Format "2006-01-02" }}*