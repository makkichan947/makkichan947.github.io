+++
date = '2025-10-20T21:38:39+08:00'
draft = false
title = 'å¾ªç¯ç¥ç»ç½‘ç»œ'
comments = true
weight = 3
+++

# å¾ªç¯ç¥ç»ç½‘ç»œ

å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Networks, RNNï¼‰æ˜¯ä¸“é—¨ç”¨äºå¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œèƒ½å¤Ÿè®°å¿†å†å²ä¿¡æ¯å¹¶ç”¨äºå½“å‰é¢„æµ‹ã€‚

## â° åºåˆ—æ•°æ®å¤„ç†

### åºåˆ—æ•°æ®ç‰¹ç‚¹
- **æ—¶é—´ä¾èµ–**ï¼šå½“å‰çŠ¶æ€ä¾èµ–äºä¹‹å‰çš„çŠ¶æ€
- **å˜é•¿è¾“å…¥**ï¼šåºåˆ—é•¿åº¦å¯èƒ½ä¸åŒ
- **ä¸Šä¸‹æ–‡ä¿¡æ¯**ï¼šéœ€è¦ç†è§£å‰åæ–‡å…³ç³»

### å¸¸è§åºåˆ—æ•°æ®
- **æ–‡æœ¬åºåˆ—**ï¼šå¥å­ã€æ–‡ç« ã€ä»£ç 
- **æ—¶é—´åºåˆ—**ï¼šè‚¡ç¥¨ä»·æ ¼ã€å¤©æ°”æ•°æ®
- **éŸ³é¢‘åºåˆ—**ï¼šè¯­éŸ³ä¿¡å·ã€éŸ³ä¹
- **è§†é¢‘åºåˆ—**ï¼šåŠ¨ä½œåºåˆ—ã€è¡Œä¸ºè¯†åˆ«

## ğŸ”„ RNNåŸºæœ¬ç»“æ„

### ç®€å•RNNå•å…ƒ

**éšè—çŠ¶æ€æ›´æ–°**ï¼š
$$h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b)$$

**è¾“å‡ºè®¡ç®—**ï¼š
$$y_t = W_{hy} h_t + c$$

### å¾ªç¯æœºåˆ¶
- **çŠ¶æ€ä¼ é€’**ï¼š$h_t$ ä¾èµ–äº $h_{t-1}$
- **å‚æ•°å…±äº«**ï¼šæ‰€æœ‰æ—¶é—´æ­¥ä½¿ç”¨ç›¸åŒçš„æƒé‡
- **è®°å¿†èƒ½åŠ›**ï¼šé€šè¿‡çŠ¶æ€ä¼ é€’ä¿ç•™å†å²ä¿¡æ¯

## ğŸš¨ æ¢¯åº¦æ¶ˆå¤±ä¸çˆ†ç‚¸

### æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- **åŸå› **ï¼šé“¾å¼æ±‚å¯¼å¯¼è‡´æ¢¯åº¦æŒ‡æ•°çº§è¡°å‡
- **å½±å“**ï¼šé•¿æœŸä¾èµ–æ— æ³•å­¦ä¹ 
- **è§£å†³æ–¹æ¡ˆ**ï¼šLSTMã€GRUç­‰æ”¹è¿›æ¶æ„

### æ¢¯åº¦çˆ†ç‚¸é—®é¢˜
- **åŸå› **ï¼šæ¢¯åº¦æŒ‡æ•°çº§å¢é•¿
- **å½±å“**ï¼šè®­ç»ƒä¸ç¨³å®šï¼Œå‚æ•°æ›´æ–°è¿‡å¤§
- **è§£å†³æ–¹æ¡ˆ**ï¼šæ¢¯åº¦è£å‰ªã€æƒé‡æ­£åˆ™åŒ–

## ğŸ§  é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ (LSTM)

### LSTMå•å…ƒç»“æ„

**é—å¿˜é—¨**ï¼š
$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$

**è¾“å…¥é—¨**ï¼š
$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$
$$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

**ç»†èƒçŠ¶æ€æ›´æ–°**ï¼š
$$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$$

**è¾“å‡ºé—¨**ï¼š
$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$
$$h_t = o_t \odot \tanh(C_t)$$

### LSTMä¼˜åŠ¿
- **é•¿æœŸè®°å¿†**ï¼šç»†èƒçŠ¶æ€ä¼ é€’å†å²ä¿¡æ¯
- **é€‰æ‹©æ€§é—å¿˜**ï¼šé—å¿˜é—¨æ§åˆ¶ä¿¡æ¯ä¿ç•™
- **æ¢¯åº¦ç¨³å®š**ï¼šé¿å…æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

## ğŸ”„ é—¨æ§å¾ªç¯å•å…ƒ (GRU)

### GRUç®€åŒ–ç»“æ„

**æ›´æ–°é—¨**ï¼š
$$z_t = \sigma(W_z \cdot [h_{t-1}, x_t])$$

**é‡ç½®é—¨**ï¼š
$$r_t = \sigma(W_r \cdot [h_{t-1}, x_t])$$

**å€™é€‰éšè—çŠ¶æ€**ï¼š
$$\tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t])$$

**éšè—çŠ¶æ€æ›´æ–°**ï¼š
$$h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$$

### GRU vs LSTM
- **å‚æ•°æ›´å°‘**ï¼šGRUæ¯”LSTMå‚æ•°é‡å°‘25%
- **è®­ç»ƒé€Ÿåº¦**ï¼šGRUé€šå¸¸è®­ç»ƒæ›´å¿«
- **æ€§èƒ½ç›¸å½“**ï¼šåœ¨è®¸å¤šä»»åŠ¡ä¸Šæ€§èƒ½ç›¸ä¼¼

## ğŸ“ è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨

### æ–‡æœ¬ç”Ÿæˆ
```python
def generate_text(model, start_text, length=100):
    generated = start_text

    for _ in range(length):
        # ç¼–ç è¾“å…¥
        x = encode_text(generated[-seq_length:])

        # é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦
        pred = model.predict(x)
        next_char = decode_prediction(pred)

        # æ·»åŠ åˆ°ç”Ÿæˆæ–‡æœ¬
        generated += next_char

    return generated
```

### æœºå™¨ç¿»è¯‘
- **ç¼–ç å™¨**ï¼šå°†æºè¯­è¨€ç¼–ç ä¸ºå‘é‡
- **è§£ç å™¨**ï¼šç”Ÿæˆç›®æ ‡è¯­è¨€åºåˆ—
- **æ³¨æ„åŠ›æœºåˆ¶**ï¼šå…³æ³¨æºè¯­è¨€çš„ç›¸å…³éƒ¨åˆ†

### æƒ…æ„Ÿåˆ†æ
- **è¾“å…¥**ï¼šæ–‡æœ¬åºåˆ—
- **è¾“å‡º**ï¼šæƒ…æ„Ÿææ€§ï¼ˆæ­£é¢/è´Ÿé¢ï¼‰
- **åº”ç”¨**ï¼šäº§å“è¯„è®ºã€ç¤¾äº¤åª’ä½“åˆ†æ

## ğŸ“ˆ æ—¶é—´åºåˆ—é¢„æµ‹

### è‚¡ç¥¨ä»·æ ¼é¢„æµ‹
```python
# æ•°æ®é¢„å¤„ç†
def create_sequences(data, seq_length):
    sequences = []
    targets = []

    for i in range(len(data) - seq_length):
        sequences.append(data[i:i+seq_length])
        targets.append(data[i+seq_length])

    return np.array(sequences), np.array(targets)

# æ¨¡å‹æ„å»º
model = Sequential([
    LSTM(50, input_shape=(seq_length, 1)),
    Dense(1)
])
```

### å¤©æ°”é¢„æµ‹
- **è¾“å…¥ç‰¹å¾**ï¼šæ¸©åº¦ã€æ¹¿åº¦ã€æ°”å‹ç­‰å†å²æ•°æ®
- **é¢„æµ‹ç›®æ ‡**ï¼šæœªæ¥å‡ å¤©çš„å¤©æ°”çŠ¶å†µ
- **æŒ‘æˆ˜**ï¼šå¤„ç†å¤šå˜é‡æ—¶é—´åºåˆ—

## ğŸµ éŸ³é¢‘å¤„ç†åº”ç”¨

### è¯­éŸ³è¯†åˆ«
- **å£°è°±å›¾**ï¼šå°†éŸ³é¢‘è½¬æ¢ä¸ºå›¾åƒ
- **CTCæŸå¤±**ï¼šå¤„ç†åºåˆ—é•¿åº¦ä¸åŒ¹é…
- **è¯­è¨€æ¨¡å‹**ï¼šæé«˜è¯†åˆ«å‡†ç¡®ç‡

### éŸ³ä¹ç”Ÿæˆ
- **MIDIåºåˆ—**ï¼šéŸ³ç¬¦åºåˆ—ç”Ÿæˆ
- **é£æ ¼è¿ç§»**ï¼šå­¦ä¹ éŸ³ä¹é£æ ¼
- **å’Œå£°ç”Ÿæˆ**ï¼šè‡ªåŠ¨ä½œæ›²ç³»ç»Ÿ

## ğŸ—ï¸ é«˜çº§RNNæ¶æ„

### åŒå‘RNN (BiRNN)
- **å‰å‘RNN**ï¼šæ­£å‘å¤„ç†åºåˆ—
- **åå‘RNN**ï¼šåå‘å¤„ç†åºåˆ—
- **æ‹¼æ¥è¾“å‡º**ï¼šç»“åˆå‰åæ–‡ä¿¡æ¯

### å¤šå±‚RNN
- **å †å ç»“æ„**ï¼šå¤šä¸ªRNNå±‚å †å 
- **ç‰¹å¾å±‚æ¬¡**ï¼šå­¦ä¹ ä¸åŒæŠ½è±¡å±‚æ¬¡çš„ç‰¹å¾
- **æ¢¯åº¦é—®é¢˜**ï¼šæ›´ä¸¥é‡çš„æ¢¯åº¦æ¶ˆå¤±

### æ³¨æ„åŠ›å¢å¼ºRNN
- **æ³¨æ„åŠ›æœºåˆ¶**ï¼šå…³æ³¨åºåˆ—ä¸­çš„é‡è¦éƒ¨åˆ†
- **é•¿è·ç¦»ä¾èµ–**ï¼šç›´æ¥è¿æ¥è¿œè·ç¦»ä¿¡æ¯
- **å¯è§£é‡Šæ€§**ï¼šå¯è§†åŒ–æ³¨æ„åŠ›æƒé‡

## ğŸš€ ç¼–ç¨‹å®ç°

### PyTorch LSTMå®ç°

```python
import torch
import torch.nn as nn

class LSTMNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(LSTMNet, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTMå±‚
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)

        # è¾“å‡ºå±‚
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # LSTMè¾“å‡º
        lstm_out, _ = self.lstm(x)

        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        output = self.fc(lstm_out[:, -1, :])

        return output
```

### Keras GRUå®ç°

```python
from keras.models import Sequential
from keras.layers import GRU, Dense

model = Sequential([
    GRU(64, input_shape=(seq_length, num_features)),
    Dense(32, activation='relu'),
    Dense(1, activation='linear')
])

model.compile(optimizer='adam', loss='mse')
```

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡

### åºåˆ—ç”Ÿæˆä»»åŠ¡
- **å›°æƒ‘åº¦ (Perplexity)**ï¼šè¡¡é‡è¯­è¨€æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›
- **BLEUåˆ†æ•°**ï¼šæœºå™¨ç¿»è¯‘è´¨é‡è¯„ä¼°
- **ROUGEåˆ†æ•°**ï¼šæ–‡æœ¬æ‘˜è¦è´¨é‡è¯„ä¼°

### æ—¶é—´åºåˆ—é¢„æµ‹
- **å‡æ–¹è¯¯å·® (MSE)**ï¼šé¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å¹³å‡è¯¯å·®
- **å¹³å‡ç»å¯¹è¯¯å·® (MAE)**ï¼šç»å¯¹è¯¯å·®çš„å¹³å‡å€¼
- **å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® (MAPE)**ï¼šç›¸å¯¹è¯¯å·®çš„å¹³å‡å€¼

## ğŸ”§ å®ç”¨æŠ€å·§

### åºåˆ—å¡«å……
```python
from keras.preprocessing.sequence import pad_sequences

# å¡«å……åºåˆ—åˆ°ç›¸åŒé•¿åº¦
padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')
```

### æ©ç æœºåˆ¶
```python
# å¤„ç†å˜é•¿åºåˆ—
mask = tf.sequence_mask(sequence_lengths, maxlen=max_length)
```

### æ³¨æ„åŠ›å¯è§†åŒ–
```python
def plot_attention(attention_weights, input_tokens, output_tokens):
    plt.figure(figsize=(10, 10))
    plt.imshow(attention_weights, cmap='viridis')
    plt.xticks(range(len(input_tokens)), input_tokens, rotation=45)
    plt.yticks(range(len(output_tokens)), output_tokens)
    plt.colorbar()
    plt.show()
```

## ğŸ¯ åº”ç”¨å®ä¾‹

### æ–‡æœ¬ç”Ÿæˆ
```python
# è®­ç»ƒæ–‡æœ¬ç”Ÿæˆæ¨¡å‹
model = Sequential([
    LSTM(128, input_shape=(seq_length, vocab_size)),
    Dense(vocab_size, activation='softmax')
])

# ç”Ÿæˆæ–‡æœ¬
def generate_text(seed_text, num_words=50):
    for _ in range(num_words):
        # ç¼–ç ç§å­æ–‡æœ¬
        encoded = encode_sequence(seed_text[-seq_length:])

        # é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
        prediction = model.predict(encoded)
        next_word = decode_prediction(prediction)

        # æ·»åŠ åˆ°æ–‡æœ¬
        seed_text += ' ' + next_word

    return seed_text
```

### æƒ…æ„Ÿåˆ†æ
```python
# åŠ è½½é¢„è®­ç»ƒè¯å‘é‡
embedding_layer = Embedding(
    input_dim=vocab_size,
    output_dim=embedding_dim,
    weights=[embedding_matrix],
    trainable=False
)

# æ„å»ºæ¨¡å‹
model = Sequential([
    embedding_layer,
    LSTM(128),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])
```

## ğŸ“š å­¦ä¹ èµ„æº

### å´æ©è¾¾è¯¾ç¨‹
- [ç¬¬äº”å‘¨ï¼šåºåˆ—æ¨¡å‹](https://www.coursera.org/learn/sequence-models)

### ç»å…¸è®ºæ–‡
- [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf) - Hochreiter & Schmidhuber (1997)
- [Learning to forget: Continual prediction with LSTM](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/hochreiter97_lstm.pdf) - Gers et al. (2000)
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078) - Cho et al. (2014)

### åœ¨çº¿èµ„æº
- [Colah's Blog: Understanding LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Illustrated Guide to RNN](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb803935)
- [PyTorch RNN Documentation](https://pytorch.org/docs/stable/nn.html#recurrent-layers)

---
*æœ€è¿‘æ›´æ–°: {{ .Lastmod.Format "2006-01-02" }}*